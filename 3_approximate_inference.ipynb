{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many interesting models the evidence  \n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\mathcal{M}_i) = \\int p(\\mathcal{D}|\\mathcal{M}_i, \\theta) p(\\theta| \\mathcal{M}_i) d\\theta\n",
    "$$\n",
    "\n",
    "and hence the posterior are intractable:\n",
    "\n",
    "- The integral has no closed-form\n",
    "- The dimensionality is so big that numerical integration is not feasible\n",
    "\n",
    "We resort to stochastic or deterministic approximations\n",
    "\n",
    "- MCMC is computationally demanding but can be exact\n",
    "- VI scales better but is not exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Approximation\n",
    "\n",
    "\n",
    "Propose a function of $\\theta \\in \\mathbb{R}^K$\n",
    "\n",
    "$$\n",
    "g(\\theta) = \\log p(\\mathcal{D}| \\theta) p(\\theta)\n",
    "$$\n",
    "\n",
    "Do a second order Taylor expansion around $\\theta= \\hat \\theta_{\\text{map}}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(\\theta) \\approx g(\\hat \\theta_{\\text{map}}) &+ (\\theta - \\hat \\theta_{\\text{map}})^T \\frac{dg}{d\\theta}\\bigg \\rvert_{\\theta=\\hat \\theta_{\\text{map}}} \\nonumber \\\\\n",
    "&+ \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\frac{d^2 g}{d\\theta^2} \\bigg \\rvert_{\\theta=\\hat \\theta_{\\text{map}}} (\\theta - \\hat \\theta_{\\text{map}})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- By definition the first derivative evaluated at $\\hat \\theta_{\\text{map}}$ is zero \n",
    "- We call the negative Hessian evaluated at $\\hat \\theta_{\\text{map}}$: $\\Sigma^{-1} = -\\frac{d^2 g}{d\\theta^2} (\\hat \\theta_{\\text{map}})$ \n",
    "\n",
    "If we plug the approximation in the evidence we can solve the integral\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}) \\approx  e^{g(\\hat \\theta_{\\text{map}})} \\int e^{-  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Sigma^{-1} (\\theta - \\hat \\theta_{\\text{map}})} d\\theta = e^{g(\\hat \\theta_{\\text{map}})} (2\\pi)^{K/2} |\\Sigma|^{1/2}\n",
    "$$\n",
    "\n",
    "And the posterior\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta| \\mathcal{D}) &= \\frac{p(\\mathcal{D}|\\theta) p(\\theta) }{p(\\mathcal{D})} \\nonumber \\\\\n",
    "&\\approx \\frac{1}{(2\\pi)^{K/2} |\\Sigma|^{1/2}} e^{-  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Sigma^{-1} (\\theta - \\hat \\theta_{\\text{map}})} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Laplace method approximates the posterior by a **Multivariate Gaussian** centered in the MAP\n",
    "\n",
    "Two steps\n",
    "1. Find the mode (MAP)\n",
    "1. Evaluate the Hessian at the mode\n",
    "\n",
    "Note that\n",
    "- We didn't assume any distribution for the prior or likelihood\n",
    "- We require that $g$ is continuous and differentiable on $\\theta$ \n",
    "- We also require that the negative Hessian of $g$ on the MAP is a proper covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evidence decomposition \n",
    "\n",
    "Using Laplace approximation the log evidence can be decomposed as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}|\\mathcal{M}_i) &\\approx g(\\hat \\theta_{\\text{map}}) + \\log (2\\pi)^{K/2} |\\Sigma|^{1/2} \\nonumber \\\\\n",
    "&=\\log p(\\mathcal{D}|\\mathcal{M}_i, \\hat \\theta_{\\text{map}}) + \\log p(\\hat \\theta_{\\text{map}}| \\mathcal{M}_i) + \\frac{K}{2} \\log(2\\pi) + \\frac{1}{2} \\log | \\Sigma | \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> The log evidence is approximated by the best likelihood fit plus the Occam's factor\n",
    "\n",
    "The Occam's factor depends on the\n",
    "- log pdf of $\\theta$\n",
    "- number of parameters $K$\n",
    "- second derivative of the posterior (model uncertainty)\n",
    "\n",
    "\n",
    "If the prior is very broad and $N$ is very large we recover the **Bayesian Information Criterion** (BIC) Proof?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference (VI)\n",
    "\n",
    "We want the posterior\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "but it may be intractable\n",
    "\n",
    "In VI a simpler (tractable) posterior distribution is proposed\n",
    "\n",
    "$$\n",
    "q_\\eta(\\theta)\n",
    "$$\n",
    "\n",
    "> We approximate $p(\\theta|\\mathcal{D})$ with $q_\\eta(\\theta)$\n",
    "\n",
    "$q_\\eta(\\theta)$ represents a family of distributions parametrized by $\\eta$\n",
    "\n",
    "> **Optimization problem:** Find $\\eta$ that makes $q$ most similar to $p$\n",
    "\n",
    "We can write this as a KL divergence\n",
    "\n",
    "$$\n",
    "\\min_\\eta D_{\\text{KL}}[q_\\eta(\\theta) || p(\\theta|\\mathcal{D})] = \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\theta|\\mathcal{D})} d\\theta\n",
    "$$\n",
    "\n",
    "This is still intractable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue we use Bayes Theorem on the posterior and move the evidence out from the integral\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}[q_\\eta(\\theta) || p(\\theta|\\mathcal{D})] = \\log p(\\mathcal{D}) + \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta\n",
    "$$\n",
    "\n",
    "> If we are minimizing with respect to $\\eta$ we can ignore the evidence\n",
    "\n",
    "The KL divergence is \n",
    "- non-negative\n",
    "- zero only if $q_\\eta(\\theta) \\equiv p(\\theta|\\mathcal{D})$\n",
    "\n",
    "Using the non-negativity we find a lower bound for the evidence\n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}) \\geq  \\mathcal{L}(\\eta) = - \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta\n",
    "$$\n",
    "\n",
    "> $\\mathcal{L}(\\eta)$ is called the **Evidence Lower BOund** (ELBO)\n",
    "\n",
    "- Minimizing the KL between $q$ and $p$ is equivalent to maximizing the ELBO wrt $q$\n",
    "$$\n",
    "\\hat \\eta = \\text{arg}\\max_\\eta \\mathcal{L}(\\eta)\n",
    "$$\n",
    "- We can use $q_{\\hat \\eta}(\\theta)$ as a drop-in replacement for $p(\\theta|\\mathcal{D})$\n",
    "- The ELBO is tractable for simple, parametric $q$\n",
    "- The ELBO can only be tight if $p$ is within the family of $q$\n",
    "\n",
    "\n",
    "\n",
    "Â¿Why variational?\n",
    "\n",
    "- Functional: Function of functions. [Calculus of variations](https://en.wikipedia.org/wiki/Calculus_of_variations): Derivatives of functionals\n",
    "- [Variational Free Energy](https://en.wikipedia.org/wiki/Thermodynamic_free_energy): $-\\mathcal{L}(\\eta)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to \"obtain\" the ELBO\n",
    "\n",
    "Using Jensen's inequality on the log evidence\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}) &=  \\log \\mathbb{E}_{\\theta\\sim p(\\theta)} \\left[p(\\mathcal{D}|\\theta)\\right]\\nonumber \\\\\n",
    "&=  \\log \\mathbb{E}_{\\theta\\sim q_\\eta(\\theta)} \\left[p(\\mathcal{D}|\\theta)\\frac{p(\\theta)}{q_\\eta(\\theta)}\\right]\\nonumber \\\\\n",
    "&\\geq  \\mathbb{E}_{\\theta\\sim q_\\eta(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\frac{p(\\theta)}{q_\\eta(\\theta)}\\right] =- \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta \\nonumber \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More attention on the ELBO \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\eta) &= - \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta \\nonumber \\\\\n",
    "&= - \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{ p (\\theta)} d\\theta + \\int q_\\eta(\\theta) \\log p(\\mathcal{D}|\\theta) d\\theta \\nonumber \\\\\n",
    "&= - D_{KL}[q_\\eta(\\theta) || p(\\theta)] + \\mathbb{E}_{\\theta \\sim q_\\eta(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\right]\\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Maximizing the ELBO is equivalent to:\n",
    "- Maximize the log likelihood when sampling from the approximate posterior\n",
    "- Minimize the KL between the approximate posterior and prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-factorized posterior\n",
    "\n",
    "A simple (tractable) posterior\n",
    "\n",
    "$$\n",
    "q_\\eta(\\theta) = \\prod_{i=1}^K q_{\\eta_i}(\\theta_i)\n",
    "$$\n",
    "\n",
    "- no correlation between factors\n",
    "- this is known as the Mean-field VI or Mean-field Theory (physics)\n",
    "\n",
    "Using this factorized posterior the ELBO\n",
    "$$\n",
    "\\mathcal{L}(\\eta) =  \\int q_{\\eta_i}(\\theta_i) \\left [ p(\\mathcal{D}|\\theta)p(\\theta) \\prod_{j\\neq i} q_{\\eta_j}(\\theta_j) d\\theta_j \\right ] d\\theta_i - \\sum_i \\int q_{\\eta_i}(\\theta_i) \\log q_{\\eta_i}(\\theta_i)  d\\theta_i\n",
    "$$\n",
    "\n",
    "We can iteratively keep all $\\theta$ but $i$ fixed and update $i$\n",
    "\n",
    "- Guaranteed convergence (convex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.5.1-BayesLogistic.pdf\n",
    "- https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.5.2-VarBayesLogistic.pdf\n",
    "- https://cedar.buffalo.edu/~srihari/CSE574/Chap10/10.2VariationalInference.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
