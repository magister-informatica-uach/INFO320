{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is information? Can we measure it?\n",
    "\n",
    "Information Theory is the mathematical study of the quantification and transmission of information proposed by **Claude Shannon** on this seminal work: *A Mathematical Theory of Communication*, 1948\n",
    "\n",
    "Shannon considered the output of a noisy source as a random variable $X$ taking $M$ possible values $\\mathcal{A} = \\{x_1, x_2, x_3, \\ldots, x_M\\}$\n",
    "\n",
    "Each value $x_i$ have an associated probability $P(X=x_i) = p_i$\n",
    "\n",
    "> What is the amount of information carried by $x_i$?\n",
    "\n",
    "Shannon defined the amount of information as\n",
    "\n",
    "$$\n",
    "I(x_i) = \\log_2 \\frac{1}{p_i},\n",
    "$$\n",
    "\n",
    "which is measured in **bits**\n",
    "\n",
    "> One bit is the amount of information needed to choose between two **equiprobable** states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: A meteorological station that sends tomorrow's weather prediction\n",
    "\n",
    "The dictionary of messages: (1) Rainy, (2) Cloudy, (3) Partially cloudy, (4) Sunny\n",
    "\n",
    "Their probabilities are: $p_1=1/2$, $p_2=1/4$, $p_3=1/8$, $p_4=1/8$\n",
    "\n",
    "The minimum number of yes/no questions (equiprobable) needed to guess tomorrow's weather:\n",
    "\n",
    "- Is it going to rain? \n",
    "- No: Is it going to be cloudy?\n",
    "- No: Is it going to be sunny?\n",
    "\n",
    "Amount of information:\n",
    "- Rainy: $\\log_2 \\frac{1}{p_1} = \\log_2 2 = 1$ bits\n",
    "- Cloudy: $2$ bits \n",
    "- Partially cloudy and Sunny: $3$ bits\n",
    "\n",
    "> The larger the probability the smallest information it carries\n",
    "\n",
    "> Amount of information is also called surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon's entropy\n",
    "\n",
    "After defining the amount of information for a state Shannon's defined the average information of the source $X$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= \\mathbb{E}_{x\\sim X}\\left [\\log_2 \\frac{1}{P(x)} \\right] \\nonumber \\\\\n",
    "&= - \\sum_{x\\in \\mathcal{A}} P(x) \\log_2 P(X)  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^M p_i \\log_2 p_i  ~ \\text{[bits]} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and called it the **entropy** of the source\n",
    "\n",
    "> Entropy is the \"average information of the source\"\n",
    "\n",
    "#### Properties:\n",
    "- Entropy is nonnegative: $H(X)>0$\n",
    "- Entropy is equal to zero when $p_j = 1 \\wedge p_i = 0, i \\neq j$\n",
    "- Entropy is maximum when $X$ is uniformly distributed $p_i = \\frac{1}{M}$, $H(X) = \\log_2(M)$\n",
    "\n",
    "> The more random the source is the larger its entropy\n",
    "\n",
    "Differential entropy for continuous variables as \n",
    "\n",
    "$$\n",
    "H(p) = - \\int p(x) \\log p(x) \\,dx ~ \\text{[nats]}\n",
    "$$\n",
    "\n",
    "where $p(x)$ is the probability density function (pdf) of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Entropy: [Kullback](https://en.wikipedia.org/wiki/Solomon_Kullback)-[Leibler](https://en.wikipedia.org/wiki/Richard_Leibler) divergence\n",
    "\n",
    "Consider a continuous random variable $X$ and two distributions $q(x)$ and $p(x)$ defined on its probability space\n",
    "\n",
    "The relative entropy between these distributions is \n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] &= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log \\frac{p(x)}{q(x)} \\right ] \\nonumber \\\\\n",
    "&= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log p(x) \\right ]  - \\mathbb{E}_{x \\sim p(x)} \\left [ \\log q(x) \\right ],  \\nonumber \\\\\n",
    "&= \\int p(x) \\log p(x) \\,dx  - \\int p(x) \\log q(x) \\,dx  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "which is also known as the **Kullback-Leibler divergence**\n",
    "\n",
    "- The left hand side term is the negative entropy of p(x)\n",
    "- The right hand side term is called the **cross-entropy of q(x) relative to p(x)** \n",
    "\n",
    "#### Intepretations of KL\n",
    "- Coding: Expected number of \"extra bits\" needed to code p(x) using a code optimal for q(x)\n",
    "- Bayesian modeling: Amount of information lost when q(x) is used as a model for p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negativity\n",
    "\n",
    "The KL divergence is non-negative\n",
    "$$\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] \\geq 0\n",
    "$$\n",
    "with the equality holding for $p(x) \\equiv q(x)$\n",
    "\n",
    "This is given by the [Gibbs inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality)\n",
    "\n",
    "$$\n",
    "- \\int p(x) \\log p(x) \\,dx  \\leq - \\int p(x) \\log q(x) \\,dx \n",
    "$$\n",
    "\n",
    "> then entropy of $p(x)$ is equal or less than the cross-entropy of $q(x)$ relative to $p(x)$\n",
    "\n",
    "\n",
    "\n",
    "#### Relation with mutual information\n",
    "\n",
    "The KL is related to the mutual information between random variables as\n",
    "\n",
    "$$\n",
    "\\text{MI}(X, Y) = D_{\\text{KL}} \\left [ p(x, y) || p(x)p(y) \\right]\n",
    "$$\n",
    "\n",
    "#### Asymmetry\n",
    "\n",
    "The KL divergence is asymmetric\n",
    "$$\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] \\neq D_{\\text{KL}} \\left [ q(x) || p(x) \\right]\n",
    "$$\n",
    "\n",
    "- The KL is not a proper distance (no triangle inequility either)\n",
    "- Forward and Reverse KL have different meanings (we will explore them soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic generative models\n",
    "\n",
    "Let's say we have $N$ continuous observations \n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{x_1, x_2, \\ldots, x_N\\}\n",
    "$$ \n",
    "\n",
    "(Assuming that they are iid) there is unknown distribution that generated these samples\n",
    "\n",
    "$$\n",
    "x_i \\sim p^*(x)\n",
    "$$\n",
    "\n",
    "The goal of **generative modeling** is to learn a probabilistic model \n",
    "\n",
    "$$\n",
    "p_\\theta(x)\n",
    "$$ \n",
    "\n",
    "with parameters $\\theta~$ that \"mimics\" $p^*(x)$\n",
    "\n",
    "In a few words:\n",
    "\n",
    "> match  $p_\\theta (x)$ to $p^*(x)$\n",
    "\n",
    "After matching $p_\\theta(x)$ we can use it to sample new data: **generation**\n",
    "\n",
    "Later we will extend this definition to **joint distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe for fitting a generative model\n",
    "\n",
    "\n",
    "1. Select a parametric form for $p_\\theta (x)$\n",
    "1. Write the difference between $p_\\theta (x)$  and $p^*(x)$\n",
    "1. Minimize this difference as a function of $\\theta~$\n",
    "\n",
    "> How do we compute the difference between probability distributions?\n",
    "\n",
    "We can use the **forward** KL divergence\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg} \\min_\\theta D_{\\text{KL}} \\left [ p^*(x) || p_\\theta(x) \\right]  \\nonumber \\\\\n",
    "&= \\text{arg} \\min_\\theta \\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p^*(x) \\right ]  - \\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ] \\nonumber \\\\\n",
    "& = \\text{arg} \\max_\\theta \\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Bad news: We can't evaluate $\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p^*(x) \\right ]$ \n",
    "- Good news: It doesn't depend on $\\theta~$, we can drop it\n",
    "\n",
    "\n",
    "### Relation with Maximum Likelihood\n",
    "\n",
    "We found that\n",
    "\n",
    "$$\n",
    "\\min_\\theta D_{\\text{KL}} \\left [ p^*(x) || p_\\theta(x) \\right] = \\max_\\theta\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ]\n",
    "$$\n",
    "\n",
    "If we approximate the expected value with an average over our finite dataset\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ] \\approx \\sum_{i=1}^N \\log p_\\theta(x_i)\n",
    "$$\n",
    "\n",
    "We get the log likelihood of $\\theta~$!\n",
    "\n",
    "> Minimizing the forward KL divegence $\\equiv$ Maximizing the log likelihood of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Fitting a univariate Gaussian model\n",
    "\n",
    "We propose $p_\\theta(x) = \\mathcal{N}(x|\\mu, \\sigma^2)$\n",
    "\n",
    "The parameters are $\\theta=(\\mu, \\sigma)$\n",
    "\n",
    "The log likelihood is \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ] \\approx  \\sum_{i=1}^N \\log p_\\theta(x_i) =  -\\frac{N}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_{i=1}^N (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "The derivatives are\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\mu} \\sum_{i=1}^N \\log p_\\theta(x_i) = \\frac{1}{\\sigma^2}  \\sum_{i=1}^N (x_i - \\mu)\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{d}{d\\sigma^2} \\sum_{i=1}^N \\log p_\\theta(x_i) = -\\frac{N}{2\\sigma^2} +  \\frac{1}{\\sigma^4}  \\sum_{i=1}^N (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "By setting them to zero we obtain the analytical MLE of $\\mu$ and $\\sigma$\n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\hat \\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 5 + 2*np.random.randn(1000) # N(5, sqrt(2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data, bins=20, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional material\n",
    "\n",
    "- Daniel Commenges, [\"Information Theory and Statistics: an overview\"](https://arxiv.org/pdf/1511.00860.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.evjang.com/2016/08/variational-bayes.html\n",
    "\n",
    "https://www.inference.vc/maximum-likelihood-for-representation-learning-2/\n",
    "\n",
    "https://www.tuananhle.co.uk/notes/reverse-forward-kl.html\n",
    "\n",
    "https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/\n",
    "\n",
    "https://dibyaghosh.com/blog/probability/kldivergence.html\n",
    "\n",
    "A Tutorial on Deep Probabilistic Generative Models Ryan addams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
