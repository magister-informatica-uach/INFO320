{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In linear regression we have a \n",
    "- continuous one-dimensional target $y$ \n",
    "- continuous M-dimensional input $x$ \n",
    "\n",
    "related by a linear mapping\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^M w_k x_k + b = f_\\theta(x)  \\rightarrow y\n",
    "$$\n",
    "\n",
    "> The model is specified by $\\theta=(b, w_1, w_2, \\ldots, w_M)$\n",
    "\n",
    "Typically, we fit this model by \n",
    "$$\n",
    "\\min_\\theta\\sum_i \\left(y_i - f_\\theta(x_i) \\right)^2\n",
    "$$\n",
    "\n",
    "whose solution is\n",
    "\n",
    "$$\n",
    "\\theta = (\\Phi^T \\Phi)^{-1} \\Phi^T Y,\n",
    "$$\n",
    "\n",
    "where $\\Phi  = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\ldots & x_{1M} \\\\ \n",
    "1 & x_{21} & x_{22} & \\ldots & x_{2M} \\\\\n",
    "1 & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N1} & x_{N2} & \\ldots & x_{NM} \\end{pmatrix}$,  $Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}$ and  $\\theta =  \\begin{pmatrix} b \\\\ w_1 \\\\ \\vdots \\\\ w_M \\end{pmatrix}$\n",
    "\n",
    "> This is known as the ordinary least squares (OLS) solution\n",
    "\n",
    "#### Note: Linear regression is *linear on the parameters*\n",
    "\n",
    "If we apply transformations we obtain the same solution. The only difference is in $\\Phi$\n",
    "\n",
    "For example\n",
    "- Polynomial basis regression $f_\\theta(x) = \\sum_k w_k x^k + b$ \n",
    "- Sine-wave basis regression $f_\\theta(x) = \\sum_k a_k \\cos(2\\pi k f_0 x)  + \\sum_k b_k \\sin(2\\pi k f_0 x) + c$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic linear regression\n",
    "\n",
    "We can assume that observations are noisy and write\n",
    "\n",
    "$$\n",
    "y = f_\\theta(x) + \\epsilon = \\sum_{k=1}^M w_k x_k  + b + \\epsilon,\n",
    "$$\n",
    "\n",
    "If the noise is independent and Gaussian distributed (iid) then\n",
    "\n",
    "$$\n",
    "p(y|x, \\theta) = \\mathcal{N}\\left(y| f_\\theta(x) , \\sigma_\\epsilon^2 \\right)\n",
    "$$\n",
    "\n",
    "Additionally, we may want to discourage large values of $\\theta~$ by placing a prior\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "The prior on the parameters gives us the space of possible models (before presenting data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = np.linspace(-5, 5, num=100).astype('float32')\n",
    "\n",
    "sw, sb = 5., 5.\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "for i in range(10):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    #W = sw*np.random.randn(1)\n",
    "    #b = sb*np.random.randn(1)\n",
    "    torch.nn.init.normal_(linear_layer.weight, 0.0, sw)\n",
    "    torch.nn.init.normal_(linear_layer.bias, 0.0, sb)\n",
    "    #y = W*x + b\n",
    "    y = linear_layer(torch.from_numpy(x).unsqueeze(1)).detach().numpy()\n",
    "    ax.plot(x, y, label='w: %0.2f, b: %0.2f' %(linear_layer.weight, linear_layer.bias))\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constraint the space of solutions by presenting data\n",
    "\n",
    "### Point-estimate solution (MAP)\n",
    "\n",
    "For a dataset $\\mathcal{D} = \\{ (x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N) \\}$\n",
    "\n",
    "The Maximum a posteriori estimator of $\\theta~$ is given by\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) ~ \\mathcal{N} (\\theta|0, \\Sigma_\\theta) \\nonumber  \\\\\n",
    "&= \\text{arg}\\min_\\theta  \\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta) + \\frac{1}{2} \\theta^T \\Sigma_\\theta^{-1} \\theta  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "where the log likelihood is\n",
    "$$\n",
    "\\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) = \\sum_{i=1}^N \\log \\mathcal{N}(y_i|f_\\theta(x_i),\\sigma_\\epsilon^2)\n",
    "$$\n",
    "and the result is\n",
    "$$\n",
    "\\hat \\theta = (\\Phi^T \\Phi + \\lambda )^{-1} \\Phi^T Y\n",
    "$$\n",
    "where $\\lambda = \\sigma_\\epsilon^2 \\Sigma_\\theta^{-1}$\n",
    "\n",
    "> This is the ridge regression or **regularized least squares** solution\n",
    "\n",
    "What happens if the variance of the prior tends to infinite (uninformative prior)\n",
    "\n",
    "We get MLE : ordinary least squares solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the parameters\n",
    "\n",
    "In this case we want the posterior of $\\theta~$ given the dataset\n",
    "\n",
    "Assuming that we know $\\sigma_\\epsilon$\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) \\propto  \\mathcal{N}(Y| \\Phi \\theta, I\\sigma_\\epsilon^2) \\mathcal{N}(\\theta| \\theta_0, \\Sigma_{\\theta_0})\n",
    "$$\n",
    "\n",
    "The likelihood is normal and the prior is normal, so\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) p(\\theta|\\theta_0, \\Sigma_{\\theta_0}) = \\frac{1}{Z} \\exp \\left ( -\\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta)  - \\frac{1}{2} (\\theta - \\theta_{0})^{T} \\Sigma_{\\theta_0}^{-1} (\\theta - \\theta_0)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(With a bit of algebra) This corresponds to a normal distribution with parameters \n",
    "$$\n",
    "\\Sigma_{\\theta_1} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2  \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\Sigma_{\\theta_1} \\Sigma_{\\theta_0}^{-1} \\theta_{0} + \\frac{1}{\\sigma_\\epsilon^2} \\Sigma_{\\theta_1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "> **Iterative framework:** We can present data and update the distribution of $\\theta~$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "mw, mb = 0., 0.\n",
    "sw, sb = 5., 5.\n",
    "So = np.diag(np.array([sb, sw])**2)\n",
    "mo = np.array([mb, mw])\n",
    "seps = 0.25 # What happens if this is larger/smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample $x=2$, $y=0$ is presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update\n",
    "Phi = np.array([[1.0, 2.0]])\n",
    "y = np.array([0.0])\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the space of possible models is constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=100).astype('float32')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "for i in range(10):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    y = linear_layer(torch.from_numpy(x).unsqueeze(1)).detach().numpy()\n",
    "    ax.plot(x, y)\n",
    "\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "#ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we present now $x=-2$, $y=-2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "So = Sn\n",
    "mo = mn\n",
    "#Update\n",
    "Phi = np.array([[1.0, -2.0]])\n",
    "y = np.array([-2.0])\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=100).astype('float32')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "for i in range(10):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    y = linear_layer(torch.from_numpy(x).unsqueeze(1)).detach().numpy()\n",
    "    ax.plot(x, y)\n",
    "\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the predictions\n",
    "\n",
    "What we want\n",
    "\n",
    "> We train the model to predict $y$ for new values of $x$\n",
    "\n",
    "In the Bayesian setting we are interested in the **posterior predictive distribution**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y|x, \\mathcal{D}, \\sigma_\\epsilon^2) &= \\int p(y|f_\\theta(x), \\sigma_\\epsilon^2) p(\\theta| \\theta_{N}, \\Sigma_{\\theta_N}) d\\theta \\nonumber \\\\\n",
    "&= \\mathcal{N}\\left(y|f_{\\theta_N} (x), \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we consider that $N$ samples were presented and that $\\mu_0=0$ then \n",
    "\n",
    "$$\n",
    "\\theta_{N} =  (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "(MAP estimator) and\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\theta_N} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "\n",
    "And the variance (uncertainty) for the new $x$ is \n",
    "$$\n",
    "\\sigma^2(x) = \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the previous example we can see that the uncertainty grows when we depart from the observed data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "Phi_x = np.vstack(([1]*100, x)).T\n",
    "sx = np.sqrt(np.diag(seps**2 + np.dot(np.dot(Phi_x, Sn), Phi_x.T)))\n",
    "ax.plot(x, np.dot(Phi_x, mn), '--')\n",
    "ax.fill_between(x, np.dot(Phi_x, mn)-2*sx, np.dot(Phi_x, mn)+2*sx, alpha=0.5)\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-study\n",
    "\n",
    "- [Chapter 18 of D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIAS VARIANCE TRADE OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural networks\n",
    "\n",
    "[Artificial neural networks](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit?usp=sharing) (ANN) are non-linear parametric function approximators built by connecting simple units\n",
    "\n",
    "These units are simplified models of biological neurons: \n",
    "\n",
    "> linear regressor followed by a non-linear activation function\n",
    "\n",
    "Feed-forward ANN are organized in layers. Each layer has a certain amount of neurons (user-defined)\n",
    "\n",
    "> **Multilayer perceptron (MLP) architecture:** Every unit is connected to all units of its previous and next layers\n",
    "\n",
    "Different ways of connecting neurons yields different ANN architectures (convolutional, recurrent, etc)\n",
    "\n",
    "The parameter vector $\\theta$ includes the weights and biases of all the neurons\n",
    "\n",
    "- Let's consider a Gaussian prior for $\\theta$ and study the space of possible models\n",
    "- How does it compare to the linear regressor? \n",
    "    - What happens when you add more neurons? \n",
    "    - What happens if you remove the nonlinearity?\n",
    "    - What happens when you add more layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=100)[:, None]\n",
    "\n",
    "def activation(z):\n",
    "    #return np.maximum(0., z) # ReLU\n",
    "    return 1.0/(1.0 + np.exp(-z)) # Logistic/Sigmoid\n",
    "\n",
    "sw, sb = 5., 5.\n",
    "Nh = 10\n",
    "plt.figure(figsize=(7, 3))\n",
    "for i in range(10):\n",
    "    W = sw*np.random.randn(1, Nh)\n",
    "    b = sb*np.random.randn(Nh)\n",
    "    z = np.dot(x, W) + b\n",
    "    z = activation(z)\n",
    "    #W = sw*np.random.randn(Nh, Nh)\n",
    "    #b = sb*np.random.randn(Nh)\n",
    "    #z = activation(np.dot(z, W) + b)\n",
    "    \n",
    "    W = sw*np.random.randn(Nh, 1)\n",
    "    b = sb*np.random.randn(1)\n",
    "    \n",
    "    plt.plot(x, np.dot(z, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch tutorial: https://github.com/magister-informatica-uach/INFO267/blob/master/unidad1/3_redes_neuronales.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic interpretation of ANN\n",
    "\n",
    "MLP for regression\n",
    "- No activation in output layer\n",
    "- Trained by minimizing the **Mean Square Error** as in Linear Regression\n",
    "\n",
    "MLP for classification\n",
    "- Sigmoid or softmax activation in output layer\n",
    "- Trained by minimizing the **Cross Entropy Error** as in Logistic Regression\n",
    "\n",
    "These cost functions arise from assuming a certain likelihood on the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
