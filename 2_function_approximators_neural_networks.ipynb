{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In linear regression we have a \n",
    "- continuous one-dimensional target $y$ \n",
    "- continuous D-dimensional input $x$ \n",
    "\n",
    "related by a linear mapping\n",
    "\n",
    "$$\n",
    "b + \\sum_{d=1}^D w_d x_d = f_\\theta(x)  \\rightarrow y\n",
    "$$\n",
    "\n",
    "> The model is specified by $\\theta=(b, w_1, w_2, \\ldots, w_D)$\n",
    "\n",
    "Typically, we fit this model by \n",
    "$$\n",
    "\\min_\\theta\\sum_n \\left(y_n - f_\\theta(x_n) \\right)^2 = (Y - \\Phi \\theta)^T (Y - \\Phi \\theta)\n",
    "$$\n",
    "\n",
    "whose solution is\n",
    "\n",
    "$$\n",
    "\\theta = (\\Phi^T \\Phi)^{-1} \\Phi^T Y,\n",
    "$$\n",
    "\n",
    "where $\\Phi  = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\ldots & x_{1D} \\\\ \n",
    "1 & x_{21} & x_{22} & \\ldots & x_{2D} \\\\\n",
    "1 & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N1} & x_{N2} & \\ldots & x_{ND} \\end{pmatrix}$,  $Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}$ and  $\\theta =  \\begin{pmatrix} b \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{pmatrix}$\n",
    "\n",
    "> This is known as the ordinary least squares (OLS) solution\n",
    "\n",
    "#### Note: Linear regression is *linear on the parameters*\n",
    "\n",
    "If we apply transformations we obtain the same solution. The only difference is in $\\Phi$\n",
    "\n",
    "For example\n",
    "- Polynomial basis regression $f_\\theta(x) = \\sum_d w_d x^d + b$ \n",
    "- Sine-wave basis regression $f_\\theta(x) = \\sum_d \\alpha_d \\cos(2\\pi d f_0 x)  + \\sum_d \\beta_d \\sin(2\\pi d f_0 x) + c$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic linear regression\n",
    "\n",
    "We can assume that observations are noisy and write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f_\\theta(x) + \\epsilon \\nonumber \\\\\n",
    "&= b + \\sum_{d=1}^D w_d x_d   + \\epsilon, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If the noise is independent and Gaussian distributed (iid) with variance $\\sigma_\\epsilon^2$ then\n",
    "\n",
    "$$\n",
    "p(y|x, \\theta) = \\mathcal{N}\\left(y| f_\\theta(x) , \\sigma_\\epsilon^2 \\right)\n",
    "$$\n",
    "\n",
    "Additionally, we may want to discourage large values of $\\theta~$ by placing a prior\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "The prior on the parameters gives us the space of possible models (before presenting data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=100)[:, None].astype('float32') #100x1\n",
    "\n",
    "sw, sb = 5., 5.\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    torch.nn.init.normal_(linear_layer.weight, 0.0, sw)\n",
    "    torch.nn.init.normal_(linear_layer.bias, 0.0, sb)\n",
    "    #y = W*x + b\n",
    "    y = linear_layer(torch.from_numpy(x)).detach().numpy()\n",
    "    ax.plot(x, y, c='tab:blue', alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constraint the space of solutions by presenting data\n",
    "\n",
    "### Point-estimate solution (MAP)\n",
    "\n",
    "For a dataset $\\mathcal{D} = \\{ (x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N) \\}$\n",
    "\n",
    "The Maximum a posteriori estimator of $\\theta~$ is given by\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) ~ \\mathcal{N} (\\theta|0, \\Sigma_\\theta) \\nonumber  \\\\\n",
    "&= \\text{arg}\\min_\\theta  \\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta) + \\frac{1}{2} \\theta^T \\Sigma_\\theta^{-1} \\theta  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "where the log likelihood is\n",
    "$$\n",
    "\\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) = \\sum_{n=1}^N \\log \\mathcal{N}(y_n|f_\\theta(x_n),\\sigma_\\epsilon^2)\n",
    "$$\n",
    "and the result is\n",
    "$$\n",
    "\\hat \\theta = (\\Phi^T \\Phi + \\lambda )^{-1} \\Phi^T Y\n",
    "$$\n",
    "where $\\lambda = \\sigma_\\epsilon^2 \\Sigma_\\theta^{-1}$\n",
    "\n",
    "> This is the ridge regression or **regularized least squares** solution\n",
    "\n",
    "What happens if the variance of the prior tends to infinite (uninformative prior)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the parameters\n",
    "\n",
    "In this case we want the posterior of $\\theta~$ given the dataset\n",
    "\n",
    "Assuming that we know $\\sigma_\\epsilon$\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) \\propto  \\mathcal{N}(Y| \\Phi \\theta, I\\sigma_\\epsilon^2) \\mathcal{N}(\\theta| \\theta_0, \\Sigma_{\\theta_0})\n",
    "$$\n",
    "\n",
    "The likelihood is normal and the prior is normal, so\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) \\propto \\frac{1}{Z} \\exp \\left ( -\\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta)  - \\frac{1}{2} (\\theta - \\theta_{0})^{T} \\Sigma_{\\theta_0}^{-1} (\\theta - \\theta_0)\\right)\n",
    "$$\n",
    "\n",
    "and (with a bit of algebra) it can be shown that this corresponds to a normal distribution \n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) = \\mathcal{N}(\\theta|\\theta_1, \\Sigma_{\\theta_1} )\n",
    "$$\n",
    "\n",
    "with parameters \n",
    "$$\n",
    "\\Sigma_{\\theta_1} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2  \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\Sigma_{\\theta_1} \\Sigma_{\\theta_0}^{-1} \\theta_{0} + \\frac{1}{\\sigma_\\epsilon^2} \\Sigma_{\\theta_1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "> **Iterative framework:** We can present data and update the distribution of $\\theta~$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a line\n",
    "\n",
    "We assume a zero-mean and diagonal covariance normal prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "mw, mb = 0., 0.\n",
    "sw, sb = 5., 5.\n",
    "So = np.diag(np.array([sb, sw])**2)\n",
    "mo = np.array([mb, mw])\n",
    "seps = 1. # What happens if this is larger/smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empirical distribution of $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3.5), tight_layout=True)\n",
    "theta_plot = np.random.multivariate_normal(mo, So, size=10000)\n",
    "ax.hist2d(theta_plot[:, 0], theta_plot[:, 1], bins=30, \n",
    "          range=((-10, 10), (-10, 10)), cmap=plt.cm.Blues)\n",
    "ax.set_xlabel('b'); ax.set_ylabel('w');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe data at $x=2$, $y=0$ and we update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update\n",
    "Phi = np.array([[1.0, 2.0]])\n",
    "y = np.array([0.0])\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated empirical distribution is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3.5), tight_layout=True)\n",
    "theta_plot = np.random.multivariate_normal(mn, Sn, size=10000)\n",
    "ax.hist2d(theta_plot[:, 0], theta_plot[:, 1], bins=30, \n",
    "          range=((-10, 10), (-10, 10)), cmap=plt.cm.Blues)\n",
    "ax.set_xlabel('b'); ax.set_ylabel('w');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the space of possible models is constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    y = linear_layer(torch.from_numpy(x)).detach().numpy()\n",
    "    ax.plot(x, y, c='tab:blue', alpha=0.25)\n",
    "\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we observe a additional data point at $x=-2$, $y=-2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "So = Sn\n",
    "mo = mn\n",
    "#Update\n",
    "Phi = np.array([[1.0, -2.0]])\n",
    "y = np.array([-2.0])\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3.5), tight_layout=True)\n",
    "theta_plot = np.random.multivariate_normal(mn, Sn, size=10000)\n",
    "ax.hist2d(theta_plot[:, 0], theta_plot[:, 1], bins=30, \n",
    "          range=((-10, 10), (-10, 10)), cmap=plt.cm.Blues)\n",
    "ax.set_xlabel('b'); ax.set_ylabel('w');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the lines are constrained even more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    y = linear_layer(torch.from_numpy(x)).detach().numpy()\n",
    "    ax.plot(x, y, c='tab:blue', alpha=0.2)\n",
    "\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the predictions\n",
    "\n",
    "Don't forget our goal\n",
    "\n",
    "> We train the model to predict $y$ for new values of $x$\n",
    "\n",
    "In the Bayesian setting we are interested in the **posterior predictive distribution**\n",
    "\n",
    "This is obtained by marginalizing $\\theta$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y | x, \\mathcal{D}) &= \\int p(y, \\theta | x, \\mathcal{D}) d\\theta \\nonumber \\\\\n",
    "&= \\int p(y| \\theta, x, \\mathcal{D}) p(\\theta| \\mathcal{D}) d\\theta \\nonumber \\\\\n",
    "&= \\int p(y| \\theta, x) p(\\theta| \\mathcal{D}) d\\theta, \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "note that $y$ is conditionally independant on $\\mathcal{D}$ given $\\theta$\n",
    "\n",
    "For our linear regression\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y|x, \\mathcal{D}, \\sigma_\\epsilon^2) &= \\int p(y|f_\\theta(x), \\sigma_\\epsilon^2) p(\\theta| \\theta_{N}, \\Sigma_{\\theta_N}) d\\theta \\nonumber \\\\\n",
    "&= \\mathcal{N}\\left(y|f_{\\theta_N} (x), \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**The posterior predictive is Gaussian** (convolution of gaussians is gaussian)\n",
    "\n",
    "If we consider that $N$ samples were presented and that $\\mu_0=0$ then \n",
    "\n",
    "$$\n",
    "\\theta_{N} =  (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "which is the MAP estimator, and\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\theta_N} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, the variance (uncertainty) for the new $x$ is \n",
    "$$\n",
    "\\sigma^2(x) = \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\n",
    "$$\n",
    "\n",
    "> The variance of the prediction has contribution from the noise (irreducible) and the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty grows when we depart from the observed data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "Phi_x = np.vstack(([1]*100, x[:, 0])).T\n",
    "sx = np.sqrt(np.diag(seps**2 + np.dot(np.dot(Phi_x, Sn), Phi_x.T)))\n",
    "ax.plot(x, np.dot(Phi_x, mn), '--')\n",
    "ax.fill_between(x[:, 0], np.dot(Phi_x, mn)-2*sx, np.dot(Phi_x, mn)+2*sx, alpha=0.5)\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:**\n",
    "\n",
    "See how the posterior predictive distribution changes with increasing/decreasing $\\sigma_\\epsilon$ and $\\Sigma_{\\theta_0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-study\n",
    "\n",
    "- [Chapter 18 of D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)\n",
    "- In all this we assumed $\\sigma_\\epsilon$ known. For a bayesian treatment with unknown noise variance we would use a normal gamma prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evidence for Bayesian Linear Regression\n",
    "\n",
    "Next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch tutorial\n",
    "\n",
    "- https://github.com/magister-informatica-uach/INFO267/blob/master/unidad1/3_redes_neuronales.ipynb\n",
    "- https://github.com/magister-informatica-uach/INFO267/blob/master/unidad1/4_red_convolucional.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial (linear) regression with Pytorch\n",
    "\n",
    "Let's create synthetic data\n",
    "\n",
    "We will fit this with a polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "x_plot = np.linspace(0, 1, num=50).astype('float32') #100x1\n",
    "y_clean = x_plot*np.sin(10*x_plot)\n",
    "x = np.delete(x_plot, slice(20, 40))\n",
    "y_clean = np.delete(y_clean, slice(20, 40))\n",
    "y = y_clean + se*np.random.randn(len(x))\n",
    "#y = (y - np.mean(y))/np.std(y)\n",
    "y = y.astype('float32')\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x[:, None])\n",
    "x_test = torch.from_numpy(x_plot[:, None])\n",
    "y_torch = torch.from_numpy(y).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regressor model in PyTorch has one layer and no activation\n",
    "\n",
    "We keep the degree of the pynomial as a free parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(torch.nn.Module):\n",
    "    def __init__(self, degree=10):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        assert degree>0, \"Degree has to be greater than zero\"\n",
    "        assert type(degree)==int, \"Degree has to be an integer\"\n",
    "        self.degree = degree\n",
    "        self.linear = torch.nn.Linear(degree, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # polynomial expansion\n",
    "        phi = torch.stack([x[:, 0]**(k+1) for k in range(self.degree)], dim=1)\n",
    "        # linear layer\n",
    "        return self.linear(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train this model using the Mean Square Error loss, i.e.\n",
    "\n",
    "> We assumme a Gaussian likelihood\n",
    "\n",
    "and batch GD with adaptive learning rate and momentum (Adam)\n",
    "\n",
    "With the `weight_decay` parameters of Adam we can add L2 regularization easily\n",
    "\n",
    "> If we add L2 then we are obtaining MAP estimates with Gaussian likelihood and a Gaussian prior \n",
    "\n",
    "**Activity:**\n",
    "\n",
    "1. Change the number of basis and describe the results\n",
    "1. Increase the noise and repeat the previous step\n",
    "1. Modify the `weight_decay` parameter in Adam and repeat the previous steps \n",
    "\n",
    "Concepts: Complexity, generalization, overfitting, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressor(degree=20) # Change the degree\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1, \n",
    "                             amsgrad=False, weight_decay=0.0) # Change the weight decay\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "f = model.forward(x_test).detach().numpy()\n",
    "line1 = ax[0].plot(x_test.detach().numpy(), f, 'k-')\n",
    "line2 = ax[1].plot([], [])\n",
    "ax[0].scatter(x, y)\n",
    "\n",
    "epoch_loss = np.zeros(shape=(5000,))\n",
    "for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "    optimizer.zero_grad()\n",
    "    f = model.forward(x_torch)\n",
    "    loss = criterion(y_torch, f)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss[k] = loss.item()\n",
    "    #break    \n",
    "    if k % 100 == 0:\n",
    "        f = model.forward(x_test).detach().numpy()\n",
    "        line1[0].set_ydata(f)\n",
    "        line2[0].set_xdata(range(k))\n",
    "        line2[0].set_ydata(epoch_loss[:k])\n",
    "        ax[1].relim()\n",
    "        ax[1].autoscale_view()\n",
    "        fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural networks\n",
    "\n",
    "[Artificial neural networks](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit?usp=sharing) (ANN) are non-linear parametric function approximators built by connecting simple units\n",
    "\n",
    "These units are simplified models of biological neurons: \n",
    "\n",
    "> Linear regressor followed by a non-linear activation function\n",
    "\n",
    "Feed-forward ANN are organized in layers. Each layer has a certain amount of neurons (user-defined)\n",
    "\n",
    "> **Multilayer perceptron (MLP) architecture:** Every unit is connected to all units of its previous and next layers\n",
    "\n",
    "Different ways of connecting neurons yields different ANN architectures (convolutional, recurrent, etc)\n",
    "\n",
    "Very deep neural network models are the current state of the art in pattern recognition problems \n",
    "\n",
    "> Subsequent layers form higher abstraction concepts\n",
    "\n",
    "#### Why are deep models needed?\n",
    "\n",
    "The MLP with one hidden layer (shallow network) is a universal approximator\n",
    "\n",
    "> It theory one could obtain a shallow network that is as flexible as a deep network, but it may require an extremely large number of hidden-layer neurons (infinite)\n",
    "\n",
    "In practice you need flexible but also compact models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP with Gaussian prior\n",
    "\n",
    "The parameter vector $\\theta$ includes the weights and biases of all the neurons\n",
    "\n",
    "Let's consider a Gaussian prior for $\\theta$ and study the space of possible models\n",
    "- How does it compare to the linear regressor? \n",
    "- What happens when you add more neurons? \n",
    "- What happens if you remove the nonlinearity? \n",
    "- What happens if you change the nonlinearity?\n",
    "- What happens when you add more layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "x_nn = np.linspace(-5, 5, num=1000)[:, None].astype('float32') #100x1\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, Nh=10, sw=5, sb=5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(1, Nh)\n",
    "        self.hidden2 = torch.nn.Linear(Nh, Nh)\n",
    "        self.output = torch.nn.Linear(Nh, 1)\n",
    "        for layer in [self.hidden1, self.hidden2, self.output]:\n",
    "            torch.nn.init.normal_(layer.weight, 0.0, sw)\n",
    "            torch.nn.init.normal_(layer.bias, 0.0, sb)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.hidden1(x))\n",
    "        z = self.activation(self.hidden2(z))\n",
    "        return self.output(z)\n",
    "    \n",
    "for i in range(10):\n",
    "    model = MLP(Nh=1000)\n",
    "    y_nn = model.forward(torch.from_numpy(x_nn)).detach().numpy()\n",
    "    ax.plot(x_nn, y_nn, c='royalblue', alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic interpretation of ANN\n",
    "\n",
    "Let's consider a simple MLP architecture for regression\n",
    "- one hidden layer with $H$ neurons\n",
    "- input dimensionality $D$ and output dimensionality $K$\n",
    "- $g(\\cdot)$ a nonlinear activation function (sigmoid, tanh, ReLU, etc)\n",
    "\n",
    "The jth neuron in the hidden layer\n",
    "$$\n",
    "z_j =  g \\left( b_j + \\sum_{d=1}^D w_{jd} x_d  \\right)\n",
    "$$\n",
    "The ith neuron in the output layer\n",
    "$$\n",
    "\\begin{align}\n",
    "f_i &=   b_i + \\sum_{j=1}^H w_{ij} z_j  \\nonumber \\\\\n",
    "&=  b_i + \\sum_{j=1}^H w_{ij} g \\left( b_j + \\sum_{d=1}^D w_{jd} x_d  \\right) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The vector parameter $\\vec \\theta$ contains the weight and biases of both layers\n",
    "\n",
    "We fit the parameters by minimizing the **Mean Square Error** cost function \n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_n  \\sum_i \\left(y_{i}^{(n)} - f_i(x^{(n)}) \\right)^2\n",
    "$$\n",
    "\n",
    "> This is equivalent to the **MLE solution with Gaussian likelihood** (known spherical covariance)\n",
    "\n",
    "Typically an L2 regularizer is included to penalize complexity and improve generalization\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_n  \\sum_i  \\left(y_{i}^{(n)} - f_i(x^{(n)}) \\right)^2 + \\lambda \\sum_k \\theta_k^2\n",
    "$$\n",
    "\n",
    "> This is equivalent to the **MAP solution with Gaussian likelihood and Gaussian prior** (zero-mean)\n",
    "\n",
    "In both cases there is no closed-form solution and we optimize with iterative methods (gradient descent)\n",
    "\n",
    "\n",
    "### In summary: Conventional neural network training obtains MLE/MAP point estimates\n",
    "\n",
    "For classification we arrive to the same conclusion except that \n",
    "- sigmoid or softmax activation is used in the output layer\n",
    "- cross-entropy cost function is used instead of MSE: **Bernoulli/Categorical likelihood**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear regressor using MLP\n",
    "\n",
    "Let's go back to the polynomial regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we use a Multi-Layer Perceptron (MLP) neural network\n",
    "\n",
    "We use a non-linear hyperbolic tangent activation function as hidden activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_hidden=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(1, n_hidden, bias=True)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, n_hidden, bias=True)\n",
    "        self.output = torch.nn.Linear(n_hidden, 1, bias=True)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.hidden1(x))\n",
    "        return self.output(self.activation(self.hidden2(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many hyperbolic tangent basis do we need to fit this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = MLP(n_hidden=5) # Change the degree\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, \n",
    "                             amsgrad=False, weight_decay=0.01) # Change the weight decay\n",
    "\n",
    "x_torch = torch.from_numpy(x[:, None])\n",
    "x_test = torch.from_numpy(x_plot[:, None])\n",
    "y_torch = torch.from_numpy(y).unsqueeze(1)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "f = model.forward(x_test).detach().numpy()\n",
    "line1 = ax[0].plot(x_test.detach().numpy(), f, 'k-')\n",
    "line2 = ax[1].plot([], [])\n",
    "ax[0].scatter(x, y)\n",
    "\n",
    "epoch_loss = np.zeros(shape=(5000,))\n",
    "for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "    optimizer.zero_grad()\n",
    "    f = model.forward(x_torch)\n",
    "    loss = criterion(y_torch, f)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss[k] = loss.item()\n",
    "    #break    \n",
    "    if k % 100 == 0:\n",
    "        f = model.forward(x_test).detach().numpy()\n",
    "        line1[0].set_ydata(f)\n",
    "        line2[0].set_xdata(range(k))\n",
    "        line2[0].set_ydata(epoch_loss[:k])\n",
    "        ax[1].relim()\n",
    "        ax[1].autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Networks\n",
    "\n",
    "In the bayesian setting we are interested in the posterior of the parameters and predictions\n",
    "\n",
    "Assuming *iid* samples $\\mathcal{D} =\\{(x^{(1)}, y^{(1)}), \\ldots \\}$ we can write the posterior of $\\theta$\n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})} = \\frac{1}{{p(\\mathcal{D})}} \\prod_n \\mathcal{N}(y^{(n)} | f(x^{(n)}), \\sigma^2) \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "> Because of the nested nonlinearities this posterior is not Gaussian\n",
    "\n",
    "\n",
    "> There is no easy way to get the normalizing constant (evidence) and hence the posterior\n",
    "\n",
    "In this case we choose between MCMC and approximate inference: Laplace method, Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro\n",
    "\n",
    "Pyro can be used to perform MCMC and/or approximate inference for intractable posteriors\n",
    "\n",
    "We can use Pyro to move from point estimates to posteriors in our **torch-based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "\n",
    "print(pyro.__version__)\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a normal distributions\n",
    "\n",
    "Distributions in Puro are picked from [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal, Uniform\n",
    "\n",
    "Normal?\n",
    "\n",
    "w_prior = Normal(torch.tensor(0.), torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Normal` object expects location $\\mu$ and scale $\\sigma$\n",
    "\n",
    "We can sample from this distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = w_prior.rsample(sample_shape=(1000, ))\n",
    "print(samples.shape)\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.hist(samples.detach().numpy(), bins=20, density=True)\n",
    "w_plot = np.linspace(-3, 3, num=100)\n",
    "w_pdf = torch.exp(w_prior.log_prob(torch.from_numpy(w_plot))).detach().numpy()\n",
    "plt.plot(w_plot, w_pdf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(w_prior.mean)\n",
    "display(w_prior.stddev)\n",
    "display(w_prior.entropy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribution has two shapes\n",
    "\n",
    "`event_shape` refers to the dimensionality of the distribution, e.g. normal (number): 0, multivariate normal (vector): 1, Cholesky (matrix): 2\n",
    "\n",
    "> `event_shape` denotes dependent random variables\n",
    "\n",
    "`batch_shape` refers to a batch of distributions\n",
    "\n",
    "> `batch_shape` denotes conditionally independent random variables\n",
    "\n",
    "We can create a batched distribution by batching the parameters\n",
    "\n",
    "The shape of a sampled tensor will be the sum of event and batch shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two conditionally independent normal RVs\n",
    "w_prior = Normal(torch.tensor([0., 2.]), torch.tensor([1., 1.]))\n",
    "\n",
    "display(w_prior.batch_shape)\n",
    "display(w_prior.event_shape)\n",
    "display(w_prior.rsample().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random variables\n",
    "\n",
    "To create random variables that we can track within a model we use [`pyro.sample`](http://pyro.ai/examples/intro_part_i.html#The-pyro.sample-Primitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.sample?\n",
    "\n",
    "def model():\n",
    "    return pyro.sample('w', w_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This expects a name and an object from [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning\n",
    "\n",
    "We can create conditions on RV \n",
    "\n",
    "Here output we have conditioned the RV \"output\" by the RV \"parameter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    theta = pyro.sample(\"parameter\", Normal(0.0, 1.0))\n",
    "    return pyro.sample(\"output\", Normal(x*theta, 1.0))\n",
    "\n",
    "def conditioned_model(x, y):\n",
    "    return pyro.condition(model, data={\"output\": y})(x)\n",
    "\n",
    "# or equivalently\n",
    "\n",
    "def model_obs(x, y):  # equivalent to conditioned_scale above\n",
    "    theta = pyro.sample(\"parameter\", Normal(0.0, 1.0))\n",
    "    return pyro.sample(\"output\", Normal(x*theta, 1.0), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "To estimate the distribution of \"parameter\" given x and y we will use **Stochastic Variational Inference** (SVI)\n",
    "\n",
    "> We will review VI in detail in a future class\n",
    "\n",
    "For the moment let's focus on the practical aspects\n",
    "\n",
    "First we need a **guide**, a simple function that acts as our approximate posterior, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "def guide(x, y):\n",
    "    a = pyro.param(\"a\", torch.tensor(0.))\n",
    "    b = pyro.param(\"b\", torch.tensor(1.), \n",
    "                   constraint=constraints.positive)\n",
    "    return pyro.sample(\"parameter\", Normal(a, b))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approximate posterior is Normal and and we want to estimate the values of $a$ and $b$\n",
    "\n",
    "### Training\n",
    "\n",
    "We train this model using [`pyro.infer.SVI`](http://docs.pyro.ai/en/stable/inference_algos.html)\n",
    "\n",
    "This is the unified Variational Inference interface of Pyro\n",
    "\n",
    "> SVI expects model, guide, optimizer, loss and the number of samples\n",
    "\n",
    "We will use adam as optimizer and the Evidence Lower Bound (ELBO) as cost function \n",
    "\n",
    "> We will review the ELBO in detail in a future class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model=model_obs, guide=guide,\n",
    "                     optim=pyro.optim.Adam({\"lr\": 0.001}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(7, 3), tight_layout=True)\n",
    "lines = [ax_.plot([], [])[0] for ax_ in ax]\n",
    "losses, a,b  = [], [], []\n",
    "ax[0].set_title('ELBO')\n",
    "ax[1].set_title('a')\n",
    "ax[2].set_title('b')\n",
    "for k in tqdm_notebook(range(7500)):\n",
    "    losses.append(svi.step(torch.tensor([2.]), torch.tensor([2.])))\n",
    "    a.append(pyro.param(\"a\").item())\n",
    "    b.append(pyro.param(\"b\").item())\n",
    "    if np.mod(k, 100) == 0:\n",
    "        for i, data_plot in enumerate([losses, a, b]):\n",
    "            lines[i].set_xdata(range(k))\n",
    "            lines[i].set_ydata(data_plot[:k])\n",
    "            ax[i].relim()\n",
    "            ax[i].autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ELBO and the parameters have converged\n",
    "\n",
    "We can evaluate our results by observing the marginal and posterior predictive distributions\n",
    "\n",
    "Let's do this with an actual neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Writing a pyro model for a Bayesian Neural Network\n",
    "\n",
    "\n",
    "> Let's convert our MLP into a Bayesian MLP using the very helpful function: [`pyro.random_module()`](http://docs.pyro.ai/en/stable/primitives.html?highlight=random_module#pyro.random_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.random_module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function grabs an object that inherites from `torch.nn.Module` and adds priors to its parameters\n",
    "\n",
    "In this case the parameters of the model are `hidden.weight`, `hidden.bias`, `output.weight` and `output.bias`\n",
    "\n",
    "> We will add a Normal prior to these parameters\n",
    "\n",
    "> We will set $\\sigma_\\epsilon$ (the noise scale) as a random variable with Uniform distribution\n",
    "\n",
    "\n",
    "We will use conditioning for the likelihood \n",
    "\n",
    "> The likelihood is set to normal with  $f_\\theta(x^{(n)})$ as its mean and $\\sigma_\\epsilon$ as its scale\n",
    "\n",
    "We condition on the whole dataset (assuming independence) using [`pyro.plate`](http://docs.pyro.ai/en/stable/primitives.html#pyro.plate)\n",
    "\n",
    "which expects a name and the size of the dataset\n",
    "\n",
    "`pyro.plate` can be used as iterator or as a context (vectorized plate)\n",
    "\n",
    "In depth about plates: http://pyro.ai/examples/svi_part_ii.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_hidden=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(1, n_hidden, bias=True)\n",
    "        self.output = torch.nn.Linear(n_hidden, 1, bias=True)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.output(self.activation(self.hidden(x)))\n",
    "    \n",
    "mlp_model = MLP(n_hidden=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal, Uniform\n",
    "\n",
    "def model(x, y):\n",
    "    # We add normal priors to w and b\n",
    "    w_hidden_prior = Normal(torch.zeros_like(mlp_model.hidden.weight), \n",
    "                            torch.ones_like(mlp_model.hidden.weight)).to_event(1)\n",
    "    b_hidden_prior = Normal(torch.zeros_like(mlp_model.hidden.bias), \n",
    "                            torch.ones_like(mlp_model.hidden.bias)).to_event(1)\n",
    "    w_output_prior = Normal(torch.zeros_like(mlp_model.output.weight), \n",
    "                            torch.ones_like(mlp_model.output.weight)).to_event(1)\n",
    "    b_output_prior = Normal(torch.zeros_like(mlp_model.output.bias), \n",
    "                            torch.ones_like(mlp_model.output.bias)).to_event(1)\n",
    "    priors = {'hidden.weight': w_hidden_prior, 'hidden.bias': b_hidden_prior,\n",
    "              'output.weight': w_output_prior, 'output.bias': b_output_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", mlp_model, priors)    \n",
    "    lifted_mlp_model = lifted_module()\n",
    "    # We create a random variable for the scale\n",
    "    #scale = pyro.sample(\"sigma\", Uniform(0.01, 10.0))\n",
    "    # Condition on the dataset assuming iid using a vectorized plate\n",
    "    with pyro.plate(\"observed_data\", size=len(x)):\n",
    "        #Get prediction (forward)\n",
    "        prediction_mean = lifted_mlp_model(x).squeeze(-1)\n",
    "        pyro.sample(\"likelihood\", Normal(prediction_mean, 0.05), obs=y)\n",
    "        return prediction_mean\n",
    "\n",
    "#http://pyro.ai/examples/tensor_shapes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyro.poutine.trace(model).get_trace(x_torch, y_torch.squeeze(-1)).format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the guide we will use the [\"auto guide\"](https://docs.pyro.ai/en/0.3.0-release/contrib.autoguide.html) functionality\n",
    "\n",
    "> We will see how to build custom guides for neural nets in a future class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(x, y):\n",
    "    # hidden weight\n",
    "    w_hidden_mu = pyro.param(\"w_hidden_mu\", torch.randn_like(mlp_model.hidden.weight))\n",
    "    w_hidden_sigma = softplus(pyro.param(\"w_hidden_sigma\", torch.ones_like(mlp_model.hidden.weight)))\n",
    "    w_hidden_prior = Normal(loc=w_hidden_mu, scale=w_hidden_sigma)\n",
    "    # Hidden bias\n",
    "    b_hidden_mu = pyro.param(\"b_hidden_mu\", torch.randn_like(mlp_model.hidden.bias))\n",
    "    b_hidden_sigma = softplus(pyro.param(\"b_hidden_sigma\",torch.ones_like(mlp_model.hidden.bias)))\n",
    "    b_hidden_prior = Normal(loc=b_hidden_mu, scale=b_hidden_sigma)\n",
    "    # output weight\n",
    "    w_output_mu = pyro.param(\"w_output_mu\", torch.randn_like(mlp_model.output.weight))\n",
    "    w_output_sigma = softplus(pyro.param(\"w_output_sigma\", torch.ones_like(mlp_model.output.weight)))\n",
    "    w_output_prior = Normal(loc=w_output_mu, scale=w_output_sigma)\n",
    "    # output bias\n",
    "    b_output_mu = pyro.param(\"b_output_mu\", torch.randn_like(mlp_model.output.bias))\n",
    "    b_output_sigma = softplus(pyro.param(\"b_output_sigma\", torch.ones_like(mlp_model.output.bias)))\n",
    "    b_output_prior = Normal(loc=b_output_mu, scale=b_output_sigma)\n",
    "    \n",
    "    priors = {'hidden.weight': w_hidden_prior, 'hidden.bias': b_hidden_prior,\n",
    "              'output.weight': w_output_prior, 'output.bias': b_output_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", mlp_model, priors)    \n",
    "    \n",
    "    #return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyro.enable_validation(False)\n",
    "\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model, guide, pyro.optim.Adam({\"lr\": 0.1}), \n",
    "                     loss=pyro.infer.Trace_ELBO(), num_samples=100)\n",
    "\n",
    "#pyro.param(\"auto_loc\", torch.randn(guide.latent_dim))\n",
    "#pyro.param(\"auto_scale\", 10*torch.ones(guide.latent_dim))\n",
    "epoch_loss = np.zeros(shape=(20000,))\n",
    "for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "    loss = svi.step(x_torch, y_torch.squeeze(-1))\n",
    "    epoch_loss[k] = loss / len(x_torch)\n",
    "    if k % 1000 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (k + 1, loss / len(x_torch)))\n",
    "        \n",
    "fig, ax = plt.subplots(1, figsize=(5, 3), tight_layout=True)\n",
    "ax.plot(epoch_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the posterior\n",
    "\n",
    "To do this we get the empirical marginal for each parameter\n",
    "\n",
    "From the samples we can create histograms and compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyro.infer import EmpiricalMarginal\n",
    "\n",
    "posterior = svi.run(x_torch, y_torch.squeeze(-1))\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(9, 3), tight_layout=True)\n",
    "for row, param in enumerate([\"module$$$hidden.weight\", \"module$$$output.weight\"]):\n",
    "    #marginal_trace = EmpiricalMarginal(posterior, param).sample(sample_shape=torch.Size([1000]))\n",
    "    marginal_trace = EmpiricalMarginal(posterior, param).enumerate_support()\n",
    "    samples = marginal_trace.reshape(100, -1).detach().numpy()\n",
    "    for k in range(5): #samples.shape[1]\n",
    "        print(\"%s\\t%d\\t%0.4f\\t%0.4f\\t%0.4f\" %(param, k, *np.quantile(samples[:, k], (0.05, 0.5, 0.95))))\n",
    "        ax[row, k].hist(samples[:, k], density=True, label=param+' '+str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Delta\n",
    "from pyro.infer import TracePredictive\n",
    "\n",
    "def wrapped_model(x_data, y_data):\n",
    "    pyro.sample(\"prediction\", Delta(model(x_data, y_data)))\n",
    "    \n",
    "trace_predictive = TracePredictive(wrapped_model, posterior, num_samples=100)\n",
    "x_plot = np.linspace(0.0, 1.0, num=200).astype('float32')\n",
    "posterior_predictive = trace_predictive.run(torch.from_numpy(x_plot[:, None]), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_trace = EmpiricalMarginal(posterior_predictive, \"likelihood\").enumerate_support().detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(posterior_trace.shape[0]):\n",
    "    ax.plot(x_plot, posterior_trace[i, :], '-', c='royalblue', alpha=0.01)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "med = np.median(posterior_trace, axis=[0])\n",
    "qua = np.quantile(posterior_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "ax.plot(x_plot, med)\n",
    "ax.fill_between(x_plot, qua[0], qua[1], alpha=0.5)\n",
    "ax.plot(x, y, 'k.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and scale learned for the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "posterior_trace = EmpiricalMarginal(posterior_predictive, \"prediction\").enumerate_support().detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(posterior_trace.shape[0]):\n",
    "    ax.plot(x_plot, posterior_trace[i, :, 0], '-', c='royalblue', alpha=0.01)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "med = np.median(posterior_trace, axis=0)\n",
    "qua = np.quantile(posterior_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "ax.plot(x_plot, med)\n",
    "ax.fill_between(x_plot, qua[0][:, 0], qua[1][:, 0], alpha=0.5)\n",
    "ax.plot(x, y, 'k.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://pyro.ai/examples/\n",
    "- https://alsibahi.xyz/snippets/2019/06/15/pyro_mnist_bnn_kl.html\n",
    "- https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd\n",
    "- https://github.com/paraschopra/bayesian-neural-network-mnist\n",
    "- https://in.pycon.org/cfp/workshops-2019/proposals/pyro-demystified-bayesian-deep-learning~en4lb/\n",
    "- https://www.reddit.com/r/MachineLearning/comments/a15r9o/p_bayesian_image_classifier_using_pyro_to_give/eangqch/\n",
    "- https://medium.com/informatics-lab/probabilistic-programming-1535d7882dbe\n",
    "   \n",
    "- https://cscherrer.github.io/post/pyro/        \n",
    "- https://bookdown.org/robertness/causalml/docs/tutorial-on-deep-probabilitic-modeling-with-pyro.html\n",
    "- theoretical: https://www.nitarshan.com/bayes-by-backprop/\n",
    "- independent: https://pdfs.semanticscholar.org/a3fa/183640b1aa6916caaaed6cdea782907d05c4.pdf\n",
    "- Bayesian layers: https://alsibahi.xyz/snippets/2019/06/15/pyro_mnist_bnn_kl.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.5.2-VarBayesLogistic.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
