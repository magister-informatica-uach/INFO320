{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "## Summary of OLS\n",
    "\n",
    "In linear regression we have a \n",
    "\n",
    "- continuous one-dimensional target $y$ \n",
    "- continuous D-dimensional input $x$ \n",
    "\n",
    "related by a linear mapping\n",
    "\n",
    "$$\n",
    "b + \\sum_{d=1}^D w_d x_d = f_\\theta(x)  \\rightarrow y\n",
    "$$\n",
    "\n",
    "> The model is specified by $\\theta=(b, w_1, w_2, \\ldots, w_D)$\n",
    "\n",
    "Typically, we fit this model by \n",
    "\n",
    "$$\n",
    "\\min_\\theta\\sum_n \\left(y_n - f_\\theta(x_n) \\right)^2 = (Y - \\Phi \\theta)^T (Y - \\Phi \\theta)\n",
    "$$\n",
    "\n",
    "whose solution is\n",
    "\n",
    "$$\n",
    "\\theta = (\\Phi^T \\Phi)^{-1} \\Phi^T Y,\n",
    "$$\n",
    "\n",
    "where $\\Phi  = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\ldots & x_{1D} \\\\ \n",
    "1 & x_{21} & x_{22} & \\ldots & x_{2D} \\\\\n",
    "1 & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N1} & x_{N2} & \\ldots & x_{ND} \\end{pmatrix}$,  $Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}$ and  $\\theta =  \\begin{pmatrix} b \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{pmatrix}$\n",
    "\n",
    "> This is known as the ordinary least squares (OLS) solution\n",
    "\n",
    "**Note: Linear regression is *linear on the parameters***\n",
    "\n",
    "If we apply transformations we obtain the same solution. The only difference is in $\\Phi$\n",
    "\n",
    "For example\n",
    "- Polynomial basis regression $f_\\theta(x) = \\sum_d w_d x^d + b$ \n",
    "- Sine-wave basis regression $f_\\theta(x) = \\sum_d \\alpha_d \\cos(2\\pi d f_0 x)  + \\sum_d \\beta_d \\sin(2\\pi d f_0 x) + c$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic linear regression\n",
    "\n",
    "We can assume that observations are noisy and write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f_\\theta(x) + \\epsilon \\nonumber \\\\\n",
    "&= b + \\sum_{d=1}^D w_d x_d   + \\epsilon, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If the noise is independent and Gaussian distributed (iid) with variance $\\sigma_\\epsilon^2$ then\n",
    "\n",
    "$$\n",
    "p(y|x, \\theta) = \\mathcal{N}\\left(y| f_\\theta(x) , \\sigma_\\epsilon^2 \\right)\n",
    "$$\n",
    "\n",
    "Additionally, we may want to discourage large values of $\\theta~$ by placing a prior\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "The prior on the parameters gives us the space of possible models (before presenting data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_x = np.linspace(-5, 5, num=100)[:, None].astype('float32') #100x1\n",
    "\n",
    "sw, sb = 5., 5.\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    torch.nn.init.normal_(linear_layer.weight, 0.0, sw)\n",
    "    torch.nn.init.normal_(linear_layer.bias, 0.0, sb)\n",
    "    #y = W*x + b\n",
    "    line_y = linear_layer(torch.from_numpy(line_x)).detach().numpy()\n",
    "    ax.plot(line_x, line_y, c='tab:blue', alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constraint the space of solutions by presenting data\n",
    "\n",
    "### Point-estimate solution (MAP)\n",
    "\n",
    "For a dataset $\\mathcal{D} = \\{ (x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N) \\}$\n",
    "\n",
    "The Maximum a posteriori estimator of $\\theta~$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) ~ \\mathcal{N} (\\theta|0, \\Sigma_\\theta) \\nonumber  \\\\\n",
    "&= \\text{arg}\\min_\\theta  \\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta) + \\frac{1}{2} \\theta^T \\Sigma_\\theta^{-1} \\theta  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the log likelihood is\n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) = \\sum_{n=1}^N \\log \\mathcal{N}(y_n|f_\\theta(x_n),\\sigma_\\epsilon^2)\n",
    "$$\n",
    "\n",
    "and the result is\n",
    "\n",
    "$$\n",
    "\\hat \\theta = (\\Phi^T \\Phi + \\lambda )^{-1} \\Phi^T Y\n",
    "$$\n",
    "\n",
    "where $\\lambda = \\sigma_\\epsilon^2 \\Sigma_\\theta^{-1}$\n",
    "\n",
    "> This is the ridge regression or **regularized least squares** solution\n",
    "\n",
    "What happens if the variance of the prior tends to infinite (uninformative prior)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the parameters\n",
    "\n",
    "In this case we want the posterior of $\\theta~$ given the dataset\n",
    "\n",
    "Assuming that we know $\\sigma_\\epsilon$\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) \\propto  \\mathcal{N}(Y| \\Phi \\theta, I\\sigma_\\epsilon^2) \\mathcal{N}(\\theta| \\theta_0, \\Sigma_{\\theta_0})\n",
    "$$\n",
    "\n",
    "The likelihood is normal and the prior is normal, so\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) \\propto \\frac{1}{Z} \\exp \\left ( -\\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta)  - \\frac{1}{2} (\\theta - \\theta_{0})^{T} \\Sigma_{\\theta_0}^{-1} (\\theta - \\theta_0)\\right)\n",
    "$$\n",
    "\n",
    "and (with a bit of algebra) it can be shown that this corresponds to a normal distribution \n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) = \\mathcal{N}(\\theta|\\theta_1, \\Sigma_{\\theta_1} )\n",
    "$$\n",
    "\n",
    "with parameters \n",
    "$$\n",
    "\\Sigma_{\\theta_1} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2  \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_1 = \\Sigma_{\\theta_1} \\Sigma_{\\theta_0}^{-1} \\theta_{0} + \\frac{1}{\\sigma_\\epsilon^2} \\Sigma_{\\theta_1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "> **Iterative framework:** We can present data and update the distribution of $\\theta~$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Fitting a line**\n",
    "\n",
    "We assume a zero-mean and diagonal covariance normal prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "mw, mb = 0., 0.\n",
    "sw, sb = 5., 5.\n",
    "So = np.diag(np.array([sb, sw])**2)\n",
    "mo = np.array([mb, mw])\n",
    "seps = 1. # What happens if this is larger/smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empirical distribution of $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_plot = np.random.multivariate_normal(mo, So, size=10000)\n",
    "\n",
    "import corner\n",
    "figure = corner.corner(theta_plot, smooth=1.,\n",
    "                       labels=[\"b\", \"w\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-8, 8), (-8, 8)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe data at $x=2$, $y=2$ and we update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update\n",
    "Phi = np.array([[1.0, 2.0]])\n",
    "y = np.array([2.0])\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this the space of possible models is constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    line_y = linear_layer(torch.from_numpy(line_x)).detach().numpy()\n",
    "    ax.plot(line_x, line_y, c='tab:blue', alpha=0.25)\n",
    "\n",
    "ax.errorbar(2, 2, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the updated empirical distribution is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_plot = np.random.multivariate_normal(mn, Sn, size=10000)\n",
    "\n",
    "figure = corner.corner(theta_plot, smooth=1.,\n",
    "                       labels=[\"b\", \"w\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-8, 8), (-8, 8)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we observe a additional data point at $x=-2$, $y=-2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "So = Sn\n",
    "mo = mn\n",
    "#Update\n",
    "Phi = np.array([[1.0, -2.0]])\n",
    "y = np.array([-2.0])\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The space of possible models is further reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    line_y = linear_layer(torch.from_numpy(line_x)).detach().numpy()\n",
    "    ax.plot(line_x, line_y, c='tab:blue', alpha=0.2)\n",
    "\n",
    "ax.errorbar(2, 2, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the empirical distribution is constrained even more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_plot = np.random.multivariate_normal(mn, Sn, size=10000)\n",
    "\n",
    "figure = corner.corner(theta_plot, smooth=1.,\n",
    "                       labels=[\"b\", \"w\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-8, 8), (-8, 8)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the predictions\n",
    "\n",
    "Don't forget our goal\n",
    "\n",
    "> We train the model to predict $y$ for new values of $x$\n",
    "\n",
    "In the Bayesian setting we are interested in the **posterior predictive distribution**\n",
    "\n",
    "This is obtained by marginalizing $\\theta$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y | x, \\mathcal{D}) &= \\int p(y, \\theta | x, \\mathcal{D}) d\\theta \\nonumber \\\\\n",
    "&= \\int p(y| \\theta, x, \\mathcal{D}) p(\\theta| \\mathcal{D}) d\\theta \\nonumber \\\\\n",
    "&= \\int p(y| \\theta, x) p(\\theta| \\mathcal{D}) d\\theta, \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "note that $y$ is conditionally independant on $\\mathcal{D}$ given $\\theta$\n",
    "\n",
    "For our linear regression\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y|x, \\mathcal{D}, \\sigma_\\epsilon^2) &= \\int p(y|f_\\theta(x), \\sigma_\\epsilon^2) p(\\theta| \\theta_{N}, \\Sigma_{\\theta_N}) d\\theta \\nonumber \\\\\n",
    "&= \\mathcal{N}\\left(y|f_{\\theta_N} (x), \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**The posterior predictive is Gaussian** (convolution of gaussians is gaussian)\n",
    "\n",
    "If we consider that $N$ samples were presented and that $\\mu_0=0$ then \n",
    "\n",
    "$$\n",
    "\\theta_{N} =  (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "which is the MAP estimator, and\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\theta_N} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, the variance (uncertainty) for the new $x$ is \n",
    "$$\n",
    "\\sigma^2(x) = \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\n",
    "$$\n",
    "\n",
    "> The variance of the prediction has contribution from the noise (irreducible) and the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty grows when we depart from the observed data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "Phi_x = np.vstack(([1]*100, line_x[:, 0])).T\n",
    "sx = np.sqrt(np.diag(seps**2 + np.dot(np.dot(Phi_x, Sn), Phi_x.T)))\n",
    "ax.plot(line_x, np.dot(Phi_x, mn), '--')\n",
    "ax.fill_between(line_x[:, 0], np.dot(Phi_x, mn)-2*sx, np.dot(Phi_x, mn)+2*sx, alpha=0.5)\n",
    "ax.errorbar(2, 2, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:**\n",
    "\n",
    "See how the posterior predictive distribution changes with increasing/decreasing $\\sigma_\\epsilon$ and $\\Sigma_{\\theta_0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-study\n",
    "\n",
    "- [Chapter 18 of D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)\n",
    "- In all this we assumed $\\sigma_\\epsilon$ known. For a bayesian treatment with unknown noise variance we would use a normal inverse gamma prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evidence for Bayesian Linear Regression\n",
    "\n",
    "Next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
