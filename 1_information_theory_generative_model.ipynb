{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is information? Can we measure it?\n",
    "\n",
    "Information Theory is the mathematical study of the quantification and transmission of information proposed by **Claude Shannon** on this seminal work: *A Mathematical Theory of Communication*, 1948\n",
    "\n",
    "Shannon considered the output of a noisy source as a random variable $X$ taking $M$ possible values $\\mathcal{A} = \\{x_1, x_2, x_3, \\ldots, x_M\\}$\n",
    "\n",
    "Each value $x_i$ have an associated probability $P(X=x_i) = p_i$\n",
    "\n",
    "> What is the amount of information carried by $x_i$?\n",
    "\n",
    "Shannon defined the amount of information as\n",
    "\n",
    "$$\n",
    "I(x_i) = \\log_2 \\frac{1}{p_i},\n",
    "$$\n",
    "\n",
    "which is measured in **bits**\n",
    "\n",
    "> One bit is the amount of information needed to choose between two **equiprobable** states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: A meteorological station that sends tomorrow's weather prediction\n",
    "\n",
    "The dictionary of messages: (1) Rainy, (2) Cloudy, (3) Partially cloudy, (4) Sunny\n",
    "\n",
    "Their probabilities are: $p_1=1/2$, $p_2=1/4$, $p_3=1/8$, $p_4=1/8$\n",
    "\n",
    "The minimum number of yes/no questions (equiprobable) needed to guess tomorrow's weather:\n",
    "\n",
    "- Is it going to rain? \n",
    "- No: Is it going to be cloudy?\n",
    "- No: Is it going to be sunny?\n",
    "\n",
    "Amount of information:\n",
    "- Rainy: $\\log_2 \\frac{1}{p_1} = \\log_2 2 = 1$ bits\n",
    "- Cloudy: $2$ bits \n",
    "- Partially cloudy and Sunny: $3$ bits\n",
    "\n",
    "> The larger the probability the smallest information it carries\n",
    "\n",
    "> Amount of information is also called surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon's entropy\n",
    "\n",
    "After defining the amount of information for a state Shannon's defined the average information of the source $X$ as\n",
    "\n",
    "$$\n",
    "H(X) = \\mathbb{E}_{x\\sim X}\\left [\\log_2 \\frac{1}{P(x)} \\right] = - \\sum_{i=1}^M p_i \\log_2 p_i  ~ \\text{[bits]}\n",
    "$$\n",
    "\n",
    "and called it the **entropy** of the source\n",
    "\n",
    "> Entropy is the \"average information of the source\"\n",
    "\n",
    "#### Properties:\n",
    "- Entropy is nonnegative: $H(X)>0$\n",
    "- Entropy is equal to zero when $p_j = 1 \\wedge p_i = 0, i \\neq j$\n",
    "- Entropy is maximum when $X$ is uniformly distributed $p_i = \\frac{1}{M}$, $H(X) = \\log_2(M)$\n",
    "\n",
    "> The more random the source is the larger its entropy\n",
    "\n",
    "Differential entropy for continuous variables as \n",
    "\n",
    "$$\n",
    "H(p) = - \\int p(x) \\log p(x) \\,dx ~ \\text{[nats]}\n",
    "$$\n",
    "\n",
    "where $p(x)$ is the probability density function (pdf) of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Entropy: Kullback Leibler divergence\n",
    "\n",
    "Consider a continuous random variable $X$ and two distributions $q(x)$ and $p(x)$ defined on its probability space\n",
    "\n",
    "The relative entropy between these distributions is \n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] &= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log \\frac{p(x)}{q(x)} \\right ] \\nonumber \\\\\n",
    "&= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log p(x) \\right ]  - \\mathbb{E}_{x \\sim p(x)} \\left [ \\log q(x) \\right ],  \\nonumber \\\\\n",
    "&= \\int p(x) \\log p(x) \\,dx  - \\int p(x) \\log q(x) \\,dx  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "which is also known as the Kullback-Leibler divergence\n",
    "\n",
    "- The left hand side term is the negative entropy of p(x)\n",
    "- The right hand side term is called the **cross-entropy of q(x) relative to p(x)** \n",
    "    - Cross entropy is the average information of distribution q(x)\n",
    "\n",
    "#### Intepretations of KL\n",
    "- Coding: Expected number of \"extra bits\" needed to code p(x) using a code optimal for q(x)\n",
    "- Bayesian modeling: Amount of information lost when q(x) is used as a model for p(x)\n",
    "\n",
    "#### Properties\n",
    "\n",
    "- Non-negative\n",
    "- Equal to zero only if $p(x) \\equiv q(x)$\n",
    "- Additive for independent distributions\n",
    "- Related to Mutual Information: $\\text{MI}(X, Y) = D_{\\text{KL}} \\left [ p(x, y) || p(x)p(y) \\right]$\n",
    "\n",
    "\n",
    "**Important:** \n",
    "\n",
    "KL divergence is asymmetric\n",
    "$$\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] \\neq D_{\\text{KL}} \\left [ q(x) || p(x) \\right]\n",
    "$$\n",
    "- Not a proper distance (no triangle inequility either)\n",
    "- Forward and Reverse KL have different meanings (we will explore them soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic generative models\n",
    "\n",
    "Assume that we have $N$ continuous observations \n",
    "$$\n",
    "(x_1, x_2, \\ldots, x_N)\n",
    "$$ \n",
    "\n",
    "These observations come from a certain distribution which we don't know \n",
    "\n",
    "$$\n",
    "x_i \\sim p^*(x)\n",
    "$$\n",
    "\n",
    "The goal of **generative modeling** is to learn a probabilistic model \n",
    "\n",
    "$$\n",
    "p_\\theta(x)\n",
    "$$ \n",
    "\n",
    "with parameters $\\theta$ that \"mimics\" $p^*(x)$, *i.e.*\n",
    "\n",
    "> match  $p_\\theta (x)$ to $p^*(x)$\n",
    "\n",
    "We can express this mathematically by \n",
    "1. Select a parametric form for $p_\\theta (x)$\n",
    "1. Write the difference between $p_\\theta (x)$  and $p^*(x)$\n",
    "1. Minimize this difference as a function of $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How do we compute the difference between probability distributions?\n",
    "\n",
    "\n",
    "## KL divergence\n",
    "\n",
    "One way to do this is through the Kullback-Leibler (KL) divergence\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}} \\left [ p^*(x) || p_\\theta(x) \\right] &= \\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log \\frac{p^*(x)}{p_\\theta(x)} \\right ] \\nonumber \\\\\n",
    "&= \\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p^*(x) \\right ]  - \\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ],  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim p(x)} [q(x) ] = \\int p(x) q(x) \\,dx\n",
    "$$\n",
    "is the expected value of $q(x)$ given that $x$ is sampled from $p(x)$\n",
    "\n",
    "Note that the KL divergence is non-negative but **asymmetric** (not a proper distance)\n",
    "\n",
    "\n",
    "**Problem:** We don't know $p^*(x)$, so we cannot evaluate $\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p^*(x) \\right ]$\n",
    "\n",
    "\n",
    "## Relation with Maximum Likelihood\n",
    "\n",
    "We want to minimize the KL divergence as a function of $\\theta$\n",
    "\n",
    "The term $\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p^*(x) \\right ]$ does not depend on $\\theta$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\min_\\theta D_{\\text{KL}} \\left [ p^*(x) || p_\\theta(x) \\right] = \\max_\\theta\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ]\n",
    "$$\n",
    "\n",
    "> Minimizing the KL divergence between the real distribution and the model $\\equiv$ maximizing the log likelihood of the model given the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Fitting a univariate Gaussian model\n",
    "\n",
    "We assume that $p_\\theta(x)$ is Gaussian\n",
    "\n",
    "The parameters are $\\theta=(\\mu, \\sigma)$\n",
    "\n",
    "The log likelihood is \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p^*(x)} \\left [ \\log p_\\theta(x) \\right ] \\approx \\frac{1}{N} \\sum_{i=1}^N \\log p_\\theta(x_i) =  -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "if we set the derivatives with respect to $\\theta$ to zero we obtain the analytical solutions for the **maximum likelihood estimators** of $\\mu$ and $\\sigma$\n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\hat \\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a44b73c51d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# N(5, sqrt(2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "data = 5 + 2*np.random.randn(1000) # N(5, sqrt(2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional material\n",
    "\n",
    "- Daniel Commenges, [\"Information Theory and Statistics: an overview\"](https://arxiv.org/pdf/1511.00860.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
