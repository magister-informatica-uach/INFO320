{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#import pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is information? Can we measure it?\n",
    "\n",
    "Information Theory is the mathematical study of the quantification and transmission of information proposed by **Claude Shannon** on this seminal work: *A Mathematical Theory of Communication*, 1948\n",
    "\n",
    "Shannon considered the output of a noisy source as a random variable $X$ taking $M$ possible values $\\{x_1, x_2, x_3, \\ldots, x_M\\}$\n",
    "\n",
    "Each value $x_i$ can appear with a probability $P(X=x_i) = p_i$\n",
    "\n",
    "> What is the amount of information carried by $x_i$?\n",
    "\n",
    "Shannon defined the amount of information as\n",
    "\n",
    "$$\n",
    "I(x_i) = \\log_2 \\frac{1}{p_i},\n",
    "$$\n",
    "\n",
    "which is measured in **bits**\n",
    "\n",
    "> One bit is the amount of information needed to choose between two **equiprobable** states\n",
    "\n",
    "#### Example: Meteorological station that sends tomorrow's weather prediction\n",
    "\n",
    "The dictionary of messages: (1) Rainy, (2) Cloudy, (3) Partially cloudy, (4) Sunny\n",
    "\n",
    "Their probabilities are: $p_1=1/2$, $p_2=1/4$, $p_3=1/8$, $p_4=1/8$\n",
    "\n",
    "The minimum number of yes/no questions (equiprobable) needed to guess tomorrow's weather:\n",
    "\n",
    "- Is it going to rain? \n",
    "- No: Is it going to be cloudy?\n",
    "- No: Is it going to be sunny?\n",
    "\n",
    "Amount of information:\n",
    "- Rainy: $\\log_2 \\frac{1}{p_1} = \\log_2 2 = 1$ bits\n",
    "- Cloudy: $2$ bits \n",
    "- Partially cloudy and Sunny: $3$ bits\n",
    "\n",
    "> The larger the probability the smallest information it carries\n",
    "\n",
    "> Amount of information is also called surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shannon's entropy\n",
    "\n",
    "After defining the amount of information for a state Shannon's defined the average information of the source $X$ as\n",
    "\n",
    "$$\n",
    "H(X) = \\mathbb{E}_{x\\sim X}\\left [\\log_2 \\frac{1}{P(x)} \\right] = - \\sum_{i=1}^M p_i \\log_2 p_i  ~ \\text{[bits]}\n",
    "$$\n",
    "\n",
    "and called it the **entropy** of the source\n",
    "\n",
    "> Entropy is the \"average information of the source\"\n",
    "\n",
    "#### Properties:\n",
    "- Entropy is nonnegative: $H(X)>0$\n",
    "- Entropy is equal to zero when $p_j = 1 \\wedge p_i = 0, i \\neq j$\n",
    "- Entropy is maximum when $X$ is uniformly distributed $p_i = \\frac{1}{M}$, $H(X) = \\log_2(M)$\n",
    "\n",
    "> The more random the source is the larger its entropy\n",
    "\n",
    "Differential entropy for continuous variables as \n",
    "\n",
    "$$\n",
    "H(p) = - \\int p(x) \\log p(x) \\,dx ~ \\text{[nats]}\n",
    "$$\n",
    "\n",
    "where $p(x)$ is the probability density function (pdf) of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Entropy: Kullback Leibler divergence\n",
    "\n",
    "Consider a continuous random variable $X$ and two distributions $q(x)$ and $p(x)$ defined on its probability space\n",
    "\n",
    "The relative entropy between these distributions is \n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] &= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log \\frac{p(x)}{q(x)} \\right ] \\nonumber \\\\\n",
    "&= \\mathbb{E}_{x \\sim p(x)} \\left [ \\log p(x) \\right ]  - \\mathbb{E}_{x \\sim p(x)} \\left [ \\log q(x) \\right ],  \\nonumber \\\\\n",
    "&= \\int p(x) \\log p(x) \\,dx  - \\int p(x) \\log q(x) \\,dx  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "which is also known as the Kullback-Leibler divergence\n",
    "\n",
    "- The left hand side term is the negative entropy of p(x)\n",
    "- The right hand side term is called the **cross-entropy of q(x) relative to p(x)** \n",
    "    - Cross entropy is the average information of distribution q(x)\n",
    "\n",
    "#### Intepretations of KL\n",
    "- Coding: Expected number of \"extra bits\" needed to code p(x) using a code optimal for q(x)\n",
    "- Bayesian modeling: Amount of information lost when q(x) is used as a model for p(x)\n",
    "\n",
    "#### Properties\n",
    "\n",
    "- Non-negative\n",
    "- Equal to zero only if $p(x) \\equiv q(x)$\n",
    "- Additive for independent distributions\n",
    "- Related to Mutual Information: $\\text{MI}(X, Y) = D_{\\text{KL}} \\left [ p(x, y) || p(x)p(y) \\right]$\n",
    "\n",
    "\n",
    "**Important:** \n",
    "\n",
    "KL divergence is asymmetric\n",
    "$$\n",
    "D_{\\text{KL}} \\left [ p(x) || q(x) \\right] \\neq D_{\\text{KL}} \\left [ q(x) || p(x) \\right]\n",
    "$$\n",
    "- Not a proper distance (no triangle inequility either)\n",
    "- Forward and Reverse KL have different meanings (we will explore them soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional material\n",
    "\n",
    "- Daniel Commenges, [\"Information Theory and Statistics: an overview\"](https://arxiv.org/pdf/1511.00860.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
