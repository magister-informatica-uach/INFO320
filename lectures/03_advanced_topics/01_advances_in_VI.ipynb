{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep GP: https://arxiv.org/pdf/1602.04133.pdf, https://arxiv.org/abs/1211.0358, https://github.com/otokonoko8/deep-Bayesian-nonparametrics-papers/blob/master/README.md\n",
    "- OPVI: https://arxiv.org/abs/1610.09033\n",
    "- neural AR flows: http://proceedings.mlr.press/v80/huang18d.html\n",
    "- deep emsembles: https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf\n",
    "- https://csc2541-f17.github.io/\n",
    "- discusión BNN: https://jacobbuckman.com/2020-01-22-bayesian-neural-networks-need-not-concentrate/, https://cims.nyu.edu/~andrewgw/caseforbdl/\n",
    "- https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html\n",
    "- Comparación interesante al final: https://wjmaddox.github.io/assets/BNN_tutorial_CILVR.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Advances in Variational Inference](https://arxiv.org/pdf/1711.05597.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap from class 3\n",
    "\n",
    "We are interested in a posterior \n",
    "$$\n",
    "p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})}\n",
    "$$\n",
    "which may be intractable\n",
    "\n",
    "In that case we do approximate inference either through sampling (MCMC) or optimization (VI). \n",
    "\n",
    "In the latter we select a (simple) approximate posterior $q_\\nu(\\theta)$ and we optimize the parameters $\\nu$ by maximizing the evidence lower bound (ELBO)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}) \\geq  \\mathcal{L}(\\nu) &= - \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta  \\\\\n",
    "&= \\mathbb{E}_{\\theta \\sim q_\\nu(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta) \\frac{p(\\theta)}{q_\\nu(\\theta)}\\right ]  \\nonumber \\\\\n",
    "&= \\mathbb{E}_{\\theta \\sim q_\\nu(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\right]- D_{KL}[q_\\nu(\\theta) || p(\\theta)]  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat \\nu = \\text{arg}\\max_\\nu \\mathbb{E}_{\\theta \\sim q_\\nu(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\right]- D_{KL}[q_\\nu(\\theta) || p(\\theta)] \n",
    "$$\n",
    "which makes $q_\\nu(\\theta)$ close to $p(\\theta|\\mathcal{D})$\n",
    "\n",
    "> There is a trade-off between how flexible/expressive the posterior is and how simple is to approximate this expression\n",
    "\n",
    "We have seen in this course how VI is coupled with **stochastic gradient descent** and  **parameter amortization through neural networks** making this scalable to large datasets. We have also seen different estimators that reduce variance and make VI applicable to more general models.\n",
    "\n",
    "In what follows we review different ways to improve VI beyond these\n",
    "\n",
    "*Disclaimer:* This is an active area of research and I may have missed something in this review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. More flexible approximate posteriors for VI\n",
    "\n",
    "\n",
    "#### Normalizing flows\n",
    "\n",
    "One way to obtain a more flexible posterior that is still tractable is to start with a simple distribution and apply a sequence of invertible transformations\n",
    "\n",
    "This is the key idea behind [normalizing flows](https://arxiv.org/abs/1505.05770)\n",
    "\n",
    "Let's say that $z\\sim q(z)$ where $q$ is simple, *e.g.* standard gaussian\n",
    "\n",
    "and that there is a smooth and invertible transformation $f$ such that $f^{-1}(f(z)) = z$\n",
    "\n",
    "Then $z' = f(z)$ is a random variable too but its distribution is\n",
    "\n",
    "$$\n",
    "q_{z'}(z') = q(z) \\left| \\frac{\\partial f^{-1}}{\\partial z'} \\right| = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1}\n",
    "$$\n",
    "\n",
    "which is the original distribution times the inverse of jacobian of the transformation\n",
    "\n",
    "And we can apply a chain of transformations $f_1, f_2, \\ldots, f_K$ obtaining\n",
    "\n",
    "$$\n",
    "q_K(z_K) = q_0(z_0) \\prod_{k=1}^K \\left| \\frac{\\partial f_k}{\\partial z_{k-1}} \\right|^{-1}\n",
    "$$\n",
    "\n",
    "With this we can go from a simple Gaussian to more expressive/complex/multi-modal distributions \n",
    "\n",
    "Nowadays several types of flows exist in the literature, *e.g.* planar, radial, autoregresive\n",
    "\n",
    "[Normalizing flows have been used to make the approximate posterior in VAE more expressive](https://arxiv.org/abs/1809.05861)\n",
    "\n",
    "Three excellent blog posts covering normalizing flows:\n",
    "- https://blog.evjang.com/2018/01/nf1.html\n",
    "- http://akosiorek.github.io/ml/2018/04/03/norm_flows.html\n",
    "- https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html\n",
    "\n",
    "[Normalizing flows in Pyro](https://bmazoure.github.io/posts/nf-in-pyro/)\n",
    "\n",
    "#### Adding more structure\n",
    "\n",
    "Another way to improve the variational approximation is including auxiliary variables. For example in [Auxiliary Deep Generative Models](https://arxiv.org/abs/1602.05473) the VAE was extended by introducing a variable $a$ that does not modify the generative process but makes the approximate posterior more expressive\n",
    "\n",
    "In this case the graphical model of the approximate posterior is $q(a, z |x) = q(z|a,x)q(a|x)$, so that the marginal $q(z|x)$ can fit more complicated posteriors. The graphical model of the generative process is $p(a,x,z) = p(a|x,z)p(x,z)$, *i.e.* under margnalization of $a$, $p(x,z)$ is recovered\n",
    "\n",
    "The ELBO in this case is \n",
    "\n",
    "$$\n",
    "\\log p(x) = \\int_z \\int_a \\log p(a, x, z) dz dz \\geq \\mathbb{E}_{\\theta \\sim q_\\nu(a|z,x)} \\left[\\log \\frac{p_\\theta(a|x,z)p_\\theta(x|z)p(z)}{q_\\nu(a|x)q_\\nu(z|a,x)}\\right ]  \\nonumber$$\n",
    "we have $\\log p(x) = \\int_z \\int_a p(x, z, a) dz dz$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tigher bounds for the KL divergence\n",
    "\n",
    "#### Importance weighting\n",
    "\n",
    "Idea based on importance sampling. Tigher bounds for the ELBO can be obtained by sampling several $z$ for a given $x$. This was explored for autoencoders in [Importance Weighted Autoencoders](https://arxiv.org/abs/1509.00519)\n",
    "\n",
    "Let's say we sample independently $K$ latent variables, this yields progressively tighter lower bounds for the evidence:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_k = \\mathbb{E}_{z_K, \\ldots, z_2, z_1 \\sim q_\\phi(z|x)} \\log \\frac{1}{K}\\sum_{k=1}^K \\frac{p_\\theta(x, z_k)}{q_\\phi(z_k|x)}\n",
    "$$\n",
    "\n",
    "where $w_k = \\frac{p_\\theta(x, z_k)}{q_\\phi(z_k|x)}$ are called the importance weights. Note that for $K=1$ we recover the VAE bound.\n",
    "\n",
    "This tighter bound [has been shown to be equivalent to using the regular bound with a more complex posterior](https://arxiv.org/pdf/1808.09034.pdf) \n",
    "\n",
    "An interesting [blog post](http://artem.sobolev.name/posts/2016-07-14-neural-variational-importance-weighted-autoencoders.html) and recent follow-up: [Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference](https://openreview.net/forum?id=HyZoi-WRb) and discussion [Tighter Variational Bounds are Not Necessarily Better](https://arxiv.org/abs/1802.04537)\n",
    "\n",
    "[Support for importance sampling in Pyro?](http://docs.pyro.ai/en/stable/inference_algos.html#module-pyro.infer.importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Other divergence measures\n",
    "\n",
    "#### $\\alpha$ divergence \n",
    "\n",
    "The KL divergence is computationally-convenient but there are other options to measure how far two distributions are\n",
    "\n",
    "For example the family of $\\alpha$ divergences (Renyi's formulation)\n",
    "\n",
    "$$\n",
    "D_\\alpha(p||q) = \\frac{1}{\\alpha -1} \\log p(x)^\\alpha q(x)^{1-\\alpha} \\,dx\n",
    "$$\n",
    "\n",
    "which is a generalization of the KL divergence: for $\\alpha \\to 1$ the KL is recovered\n",
    "\n",
    "$\\alpha$ represents a trade-of between the mass-covering and zero-forcing effects\n",
    "\n",
    "The $\\alpha$ divergence has been explored for [VI recently](https://arxiv.org/pdf/1511.03243.pdf) and is [implemented in Pyro](http://docs.pyro.ai/en/stable/inference_algos.html#pyro.infer.renyi_elbo.RenyiELBO)\n",
    "\n",
    "#### f divergence\n",
    "\n",
    "The $\\alpha$ divergence is a particular case of the f-divergence\n",
    "\n",
    "$$\n",
    "D_f(p||q) =  q(x) f \\left ( \\frac{p(x)}{q(x)} \\right) \\,dx\n",
    "$$\n",
    "\n",
    "where $f$ is a convex function with $f(0) = 1$. The KL is recovered for $f(z) = z \\log(z)$\n",
    "\n",
    "$f$ should be such that the result in the bound does not depend on the marginal likelihood\n",
    "\n",
    "[VI with f-divergence](https://papers.nips.cc/paper/7816-variational-inference-with-tail-adaptive-f-divergence.pdf) and its [Pyro implementation](http://docs.pyro.ai/en/stable/inference_algos.html#pyro.infer.trace_tail_adaptive_elbo.TraceTailAdaptive_ELBO)\n",
    "\n",
    "#### Stein variational gradient descent (SVGD)\n",
    "\n",
    "Other totally different approach is based on the **Stein operator**\n",
    "\n",
    "$$\n",
    "\\mathcal{A}_p \\phi(x) = \\phi(x) \\nabla_x \\log p(x)  + \\nabla_x \\phi(x)\n",
    "$$\n",
    "\n",
    "where $p(x)$ is a distribution and $\\phi(x) = [\\phi_1(x), \\phi_2(x), \\ldots, \\phi_d(x)]$ a smooth vector function\n",
    "\n",
    "Under this following, known as the **Stein identity**, holds\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim p(x)} \\left [  \\mathcal{A}_p \\phi(x)  \\right] = 0,\n",
    "$$\n",
    "\n",
    "\n",
    "Now, for another distribution $q(x)$ with the same support as $p$, we can write \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim q(x)} \\left [ \\mathcal{A}_p \\phi(x) \\right] - \\mathbb{E}_{x\\sim q(x)} \\left [ \\mathcal{A}_q \\phi(x) \\right]= \\mathbb{E}_{x\\sim q(x)} \\left [ \\phi(x) ( \\nabla_x \\log p(x) - \\nabla_x \\log q(x)) \\right]\n",
    "$$ \n",
    "\n",
    "from which the **Stein discrepancy** between two distributions is defined\n",
    "\n",
    "$$\n",
    "\\sqrt{S(q, p)} = \\max_{\\phi\\in \\mathcal{F}} \\mathbb{E}_{x\\sim q(x)} \\left [ \\mathcal{A}_p \\phi(x) \\right]\n",
    "$$\n",
    "\n",
    "Which to actually work requires $\\mathcal{F}$ to be broad enough\n",
    "\n",
    "This is were kernels can be used. By taking an infinite amount of basis function $\\phi(x)$ on the stein discrepancy it can be shown that the optimization is solved by\n",
    "\n",
    "$$\n",
    "\\textbf{S}(q, p) = \\mathbb{E}_{x, x' \\sim q(x)} \\left [ \\mathcal{A}_p^x \\mathcal{A}_p^{x'} \\kappa(x, x')\\right]\n",
    "$$\n",
    "\n",
    "where $\\kappa$ is a kernel function, *e.g.* RBF or rational quadratic\n",
    "\n",
    "From this one can use stochastic gradient descent\n",
    "\n",
    "\n",
    "- [List of papers related SVGD](https://www.cs.dartmouth.edu/~qliu/stein.html)\n",
    "- [Pyro implementation](http://docs.pyro.ai/en/stable/inference_algos.html#module-pyro.infer.svgd)\n",
    "- [Operator Variational Inference (OPVI)](https://papers.nips.cc/paper/6091-operator-variational-inference.pd) also employs the Stein operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Bayesian Neural Networks\n",
    "\n",
    "As we have seen in this course training a full bayesian neural network is on state-of-the-art and several problems exist: slow convergence, high variance, too simple posteriors, etc\n",
    "\n",
    "Training a Bayesian neural network using VI resort to maximizing \n",
    "$$\n",
    "\\mathcal{L}(\\nu) = \\mathbb{E}_{\\theta \\sim q_\\nu(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\right]- D_{KL}[q_\\nu(\\theta) || p(\\theta)]\n",
    "$$\n",
    "where $\\nu$ are the parameters of the approximate posterior\n",
    "\n",
    "\n",
    "The strategy proposed in [Blundel et al 2015 called Bayes by Backprop](https://arxiv.org/pdf/1505.05424.pdf) consists on replacing the expectation with monte-carlo estimates \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\nu) \\approx  \\sum_{i=1}^N \\sum_{k=1}^K \\log p(x_i|\\theta_k)-  \\log q_\\nu(\\theta_k)  + \\log p(\\theta_k)\n",
    "$$\n",
    "\n",
    "where $N$ is the number of data samples in the minibatch and $K$ is the number of times we sample from the parameters $\\theta$\n",
    "\n",
    "- Freedom on the variational posterior (we don't require a closed form for the KL divergence)\n",
    "- The reparameterization trick is used to reduce variance\n",
    "\n",
    "##### Prior in Bayes by backprop\n",
    "\n",
    "As no closed-form is needed more complex priors can be used. In the original Bayes-by-backprop paper the following is considered\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\pi_1 \\mathcal{N}(0, \\sigma_1^2) + \\pi_2 \\mathcal{N}(0, \\sigma_2^2)\n",
    "$$\n",
    "\n",
    "with $\\sigma_1<<<\\sigma_2$ \n",
    "\n",
    "The term with smaller variance allows for automatic \"shut-down\" (pruning) of weights: **sparsification**\n",
    "\n",
    "[Gaussian scale mixtures are implemented in Pyro](http://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.GaussianScaleMixture) but are a bit tricky to use\n",
    "\n",
    "Other implementations of Bayes by backprop\n",
    "- https://www.nitarshan.com/bayes-by-backprop/\n",
    "- http://krasserm.github.io/2019/03/14/bayesian-neural-networks/\n",
    "- https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html\n",
    "\n",
    "##### [Local reparametrization trick to reduce noise](http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick)\n",
    "\n",
    "In BNN we sample from every weight as \n",
    "\n",
    "$$\n",
    "w_{ji}\\sim \\mathcal{N}(\\mu_{ji}, \\sigma_{ji}^2)\n",
    "$$\n",
    "\n",
    "using the reperameterization trick\n",
    "\n",
    "\n",
    "$$\n",
    "w_{ji} = \\mu_{ji} +\\epsilon_{ji} \\cdot\\sigma_{ji}, \\quad \\epsilon_{ji} \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "but this is still quite a lot. The idea behind local reparameterization is instead of sampling from every weight we sample from the pre-activations\n",
    "\n",
    "$$\n",
    "Z = WX + B\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "z_i = \\nu_i + \\eta_i  \\cdot \\epsilon_{i}\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is still a standard normal and $\\nu_i = \\sum_j x_j \\mu_{ji}$ and $\\eta_i = \\sqrt{\\sum_j x_j^2 \\sigma_{ji}^2}$\n",
    "\n",
    "This reduces the amounts of samples we take by orders and magnitud and also reduces the variance of the estimator\n",
    "\n",
    "[Implementation in pyro](http://docs.pyro.ai/en/stable/contrib.bnn.html#pyro.contrib.bnn.hidden_layer.HiddenLayer) with [demonstration](https://alsibahi.xyz/snippets/2019/06/15/pyro_mnist_bnn_kl.html)\n",
    "\n",
    "##### Bayes-by-backprop for convolutional neural networks\n",
    "\n",
    "Local reparameterization trick for convolutional layers\n",
    "\n",
    "- Blog post: https://medium.com/neuralspace/bayesian-convolutional-neural-networks-with-bayes-by-backprop-c84dcaaf086e\n",
    "- [A Comprehensive guide to Bayesian Convolutional Neural Network with Variational Inference](https://arxiv.org/pdf/1901.02731.pdf)\n",
    "- [Uncertainty Estimations by Softplus normalization inBayesian Convolutional Neural Networks withVariational Inference](https://arxiv.org/pdf/1806.05978.pdf)\n",
    "\n",
    "##### [Dropout as a Bayesian approximation](https://arxiv.org/abs/1506.02142)\n",
    "\n",
    "Alternative take on Bayesian neural networks based on the dropout technique for regularization\n",
    "\n",
    "Dropout turns-off neurons following a certain distribution. The authors argue that these is like having an ensemble of neural networks and hence uncertainties can be computed\n",
    "\n",
    "TLDR: Use dropout not only in the training set but also when predicting to estimate uncertainty\n",
    "\n",
    "[How good this is?](http://bayesiandeeplearning.org/2016/papers/BDL_4.pdf): Uncertainty with this approach (fixed dropout probability) does not decrease as new data points arrive. [A solution to this?](https://papers.nips.cc/paper/6949-concrete-dropout)\n",
    "\n",
    "\n",
    "##### [FLIPOUT](https://arxiv.org/abs/1803.04386)\n",
    "\n",
    "Decorrelation of the gradients within a minibatch speeding up bayesian neural networks with gaussian perturbations\n",
    "\n",
    "##### [Natural gradient VI](https://papers.nips.cc/paper/8681-practical-deep-learning-with-bayesian-principles.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advances in MCMC\n",
    "\n",
    "If MCMC would be faster we probably would not need VI\n",
    "\n",
    "- https://github.com/pyro-ppl/numpyro\n",
    "- [Approximate MCMC](https://arxiv.org/abs/1908.03491)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimization\n",
    "\n",
    "http://pyro.ai/examples/bo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
