{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch tutorial\n",
    "\n",
    "- https://github.com/magister-informatica-uach/INFO267/blob/master/unidad1/3_redes_neuronales.ipynb\n",
    "- https://github.com/magister-informatica-uach/INFO267/blob/master/unidad1/4_red_convolucional.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In linear regression we have a \n",
    "- continuous one-dimensional target $y$ \n",
    "- continuous D-dimensional input $x$ \n",
    "\n",
    "related by a linear mapping\n",
    "\n",
    "$$\n",
    "b + \\sum_{d=1}^D w_d x_d = f_\\theta(x)  \\rightarrow y\n",
    "$$\n",
    "\n",
    "> The model is specified by $\\theta=(b, w_1, w_2, \\ldots, w_D)$\n",
    "\n",
    "Typically, we fit this model by \n",
    "$$\n",
    "\\min_\\theta\\sum_n \\left(y_n - f_\\theta(x_n) \\right)^2 = (Y - \\Phi \\theta)^T (Y - \\Phi \\theta)\n",
    "$$\n",
    "\n",
    "whose solution is\n",
    "\n",
    "$$\n",
    "\\theta = (\\Phi^T \\Phi)^{-1} \\Phi^T Y,\n",
    "$$\n",
    "\n",
    "where $\\Phi  = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\ldots & x_{1D} \\\\ \n",
    "1 & x_{21} & x_{22} & \\ldots & x_{2D} \\\\\n",
    "1 & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N1} & x_{N2} & \\ldots & x_{ND} \\end{pmatrix}$,  $Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}$ and  $\\theta =  \\begin{pmatrix} b \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{pmatrix}$\n",
    "\n",
    "> This is known as the ordinary least squares (OLS) solution\n",
    "\n",
    "#### Note: Linear regression is *linear on the parameters*\n",
    "\n",
    "If we apply transformations we obtain the same solution. The only difference is in $\\Phi$\n",
    "\n",
    "For example\n",
    "- Polynomial basis regression $f_\\theta(x) = \\sum_d w_d x^d + b$ \n",
    "- Sine-wave basis regression $f_\\theta(x) = \\sum_d \\alpha_d \\cos(2\\pi d f_0 x)  + \\sum_d \\beta_d \\sin(2\\pi d f_0 x) + c$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic linear regression\n",
    "\n",
    "We can assume that observations are noisy and write\n",
    "\n",
    "$$\n",
    "y = f_\\theta(x) + \\epsilon = b + \\sum_{d=1}^D w_d x_d   + \\epsilon,\n",
    "$$\n",
    "\n",
    "If the noise is independent and Gaussian distributed (iid) with variance $\\sigma_\\epsilon^2$ then\n",
    "\n",
    "$$\n",
    "p(y|x, \\theta) = \\mathcal{N}\\left(y| f_\\theta(x) , \\sigma_\\epsilon^2 \\right)\n",
    "$$\n",
    "\n",
    "Additionally, we may want to discourage large values of $\\theta~$ by placing a prior\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "The prior on the parameters gives us the space of possible models (before presenting data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = np.linspace(-5, 5, num=100)[:, None].astype('float32') #100x1\n",
    "\n",
    "sw, sb = 5., 5.\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    #W = sw*np.random.randn(1)\n",
    "    #b = sb*np.random.randn(1)\n",
    "    torch.nn.init.normal_(linear_layer.weight, 0.0, sw)\n",
    "    torch.nn.init.normal_(linear_layer.bias, 0.0, sb)\n",
    "    #y = W*x + b\n",
    "    y = linear_layer(torch.from_numpy(x)).detach().numpy()\n",
    "    ax.plot(x, y, c='royalblue', alpha=0.25)\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constraint the space of solutions by presenting data\n",
    "\n",
    "### Point-estimate solution (MAP)\n",
    "\n",
    "For a dataset $\\mathcal{D} = \\{ (x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N) \\}$\n",
    "\n",
    "The Maximum a posteriori estimator of $\\theta~$ is given by\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) ~ \\mathcal{N} (\\theta|0, \\Sigma_\\theta) \\nonumber  \\\\\n",
    "&= \\text{arg}\\min_\\theta  \\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta) + \\frac{1}{2} \\theta^T \\Sigma_\\theta^{-1} \\theta  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "where the log likelihood is\n",
    "$$\n",
    "\\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) = \\sum_{n=1}^N \\log \\mathcal{N}(y_n|f_\\theta(x_n),\\sigma_\\epsilon^2)\n",
    "$$\n",
    "and the result is\n",
    "$$\n",
    "\\hat \\theta = (\\Phi^T \\Phi + \\lambda )^{-1} \\Phi^T Y\n",
    "$$\n",
    "where $\\lambda = \\sigma_\\epsilon^2 \\Sigma_\\theta^{-1}$\n",
    "\n",
    "> This is the ridge regression or **regularized least squares** solution\n",
    "\n",
    "What happens if the variance of the prior tends to infinite (uninformative prior)\n",
    "\n",
    "We get MLE : ordinary least squares solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the parameters\n",
    "\n",
    "In this case we want the posterior of $\\theta~$ given the dataset\n",
    "\n",
    "Assuming that we know $\\sigma_\\epsilon$\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}, \\sigma_\\epsilon^2) \\propto  \\mathcal{N}(Y| \\Phi \\theta, I\\sigma_\\epsilon^2) \\mathcal{N}(\\theta| \\theta_0, \\Sigma_{\\theta_0})\n",
    "$$\n",
    "\n",
    "The likelihood is normal and the prior is normal, so\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) p(\\theta|\\theta_0, \\Sigma_{\\theta_0}) = \\frac{1}{Z} \\exp \\left ( -\\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta)  - \\frac{1}{2} (\\theta - \\theta_{0})^{T} \\Sigma_{\\theta_0}^{-1} (\\theta - \\theta_0)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(With a bit of algebra) This corresponds to a normal distribution with parameters \n",
    "$$\n",
    "\\Sigma_{\\theta_1} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2  \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\Sigma_{\\theta_1} \\Sigma_{\\theta_0}^{-1} \\theta_{0} + \\frac{1}{\\sigma_\\epsilon^2} \\Sigma_{\\theta_1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "> **Iterative framework:** We can present data and update the distribution of $\\theta~$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "mw, mb = 0., 0.\n",
    "sw, sb = 5., 5.\n",
    "So = np.diag(np.array([sb, sw])**2)\n",
    "mo = np.array([mb, mw])\n",
    "seps = 1. # What happens if this is larger/smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample $x=2$, $y=0$ is presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update\n",
    "Phi = np.array([[1.0, 2.0]])\n",
    "y = np.array([0.0])\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the space of possible models is constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    y = linear_layer(torch.from_numpy(x)).detach().numpy()\n",
    "    ax.plot(x, y, c='royalblue', alpha=0.25)\n",
    "\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we present now $x=-2$, $y=-2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "So = Sn\n",
    "mo = mn\n",
    "#Update\n",
    "Phi = np.array([[1.0, -2.0]])\n",
    "y = np.array([-2.0])\n",
    "Sn = seps**2*np.linalg.inv(np.dot(Phi.T, Phi) +  seps**2*np.linalg.inv(So))\n",
    "mn = np.dot(Sn, np.linalg.solve(So, mo)) + np.dot(Sn, np.dot(Phi.T, y))/seps**2\n",
    "display(Sn, mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "for i in range(100):\n",
    "    linear_layer = torch.nn.Linear(1, 1)\n",
    "    rparam = torch.from_numpy(np.random.multivariate_normal(mn, Sn).astype('float32'))\n",
    "    linear_layer.weight.data = rparam[1].reshape(-1, 1)\n",
    "    linear_layer.bias.data = rparam[0].reshape(-1, 1)\n",
    "    y = linear_layer(torch.from_numpy(x)).detach().numpy()\n",
    "    ax.plot(x, y, c='royalblue', alpha=0.2)\n",
    "\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian solution for the predictions\n",
    "\n",
    "Don't forget the goal\n",
    "\n",
    "> We train the model to predict $y$ for new values of $x$\n",
    "\n",
    "In the Bayesian setting we are interested in the **posterior predictive distribution**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y|x, \\mathcal{D}, \\sigma_\\epsilon^2) &= \\int p(y|f_\\theta(x), \\sigma_\\epsilon^2) p(\\theta| \\theta_{N}, \\Sigma_{\\theta_N}) d\\theta \\nonumber \\\\\n",
    "&= \\mathcal{N}\\left(y|f_{\\theta_N} (x), \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(convolution of gaussians is gaussian)\n",
    "\n",
    "If we consider that $N$ samples were presented and that $\\mu_0=0$ then \n",
    "\n",
    "$$\n",
    "\\theta_{N} =  (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1} \\Phi^T y\n",
    "$$\n",
    "\n",
    "(MAP estimator) and\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\theta_N} = \\sigma_\\epsilon^2 (\\Phi^T \\Phi + \\sigma_\\epsilon^2 \\Sigma_{\\theta_0}^{-1})^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, the variance (uncertainty) for the new $x$ is \n",
    "$$\n",
    "\\sigma^2(x) = \\sigma_\\epsilon^2 + x^T \\Sigma_{\\theta_N} x\n",
    "$$\n",
    "\n",
    "> The variance of the prediction has contribution from the noise (irreducible) and the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty grows when we depart from the observed data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "Phi_x = np.vstack(([1]*100, x[:, 0])).T\n",
    "sx = np.sqrt(np.diag(seps**2 + np.dot(np.dot(Phi_x, Sn), Phi_x.T)))\n",
    "ax.plot(x, np.dot(Phi_x, mn), '--')\n",
    "ax.fill_between(x[:, 0], np.dot(Phi_x, mn)-2*sx, np.dot(Phi_x, mn)+2*sx, alpha=0.5)\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2*seps, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2*seps, fmt='none', c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:**\n",
    "\n",
    "See how the posterior predictive distribution changes with increasing/decreasing $\\sigma_\\epsilon$ and $\\Sigma_{\\theta_0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-study\n",
    "\n",
    "- [Chapter 18 of D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with Pytorch\n",
    "\n",
    "Let's create synthetic data\n",
    "\n",
    "We will fit this with a polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "x = np.linspace(0, 1, num=100).astype('float32') #100x1\n",
    "y_clean = x*np.sin(10*x)\n",
    "y = y_clean + se*np.random.randn(len(x))\n",
    "y = y.astype('float32')\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regressor model in PyTorch is a neural network with one layer and no activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(torch.nn.Module):\n",
    "    def __init__(self, degree=10):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        assert degree>0, \"Degree has to be greater than zero\"\n",
    "        assert type(degree)==int, \"Degree has to be an integer\"\n",
    "        self.degree = degree\n",
    "        self.linear = torch.nn.Linear(degree, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        phi = torch.stack([x**(k+1) for k in range(self.degree)], dim=-1)\n",
    "        return self.linear(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the degree of the polynomial expansion\n",
    "\n",
    "We will train this model using the MSE loss and batch GD with adaptive learning rate and momentum (Adam)\n",
    "\n",
    "With the `weight_decay` parameters of Adam we can add L2 regularization easily\n",
    "\n",
    "> If we add L2 then we are obtaining MAP estimates with Gaussian likelihood and a Gaussian prior (more on this later)\n",
    "\n",
    "**Activity:**\n",
    "\n",
    "1. Change the number of basis and describe the results\n",
    "1. Increase the noise and repeat the previous step\n",
    "1. Modify the `weight_decay` parameter in Adam and repeat the previous steps \n",
    "\n",
    "Concepts: Complexity, generalization, overfitting, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressor(degree=5) # Change the degree\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, \n",
    "                             amsgrad=False, weight_decay=0.0) # Change the weight decay\n",
    "\n",
    "x_torch = torch.from_numpy(x)\n",
    "y_torch = torch.from_numpy(y).unsqueeze(1)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "f = model.forward(torch.from_numpy(x)).detach().numpy()\n",
    "line = ax[0].plot(x, f, 'k-')\n",
    "\n",
    "ax[0].scatter(x, y)\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "    optimizer.zero_grad()\n",
    "    f = model.forward(x_torch)\n",
    "    loss = criterion(y_torch, f)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss[k] = loss.item()\n",
    "    #break    \n",
    "    if k % 100 == 0:\n",
    "        f = model.forward(torch.from_numpy(x)).detach().numpy()\n",
    "        line[0].set_ydata(f)\n",
    "        fig.canvas.draw()\n",
    "#ax[0].plot(x, f, 'k-')\n",
    "ax[1].plot(epoch_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "\n",
    "print(pyro.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression with Pyro\n",
    "\n",
    "Pyro can be used to perform MCMC and/or approximate inference for intractable posteriors\n",
    "\n",
    "We can use Pyro to move from point estimates to posteriors in our **torch-based model**\n",
    "\n",
    "> For linear regression the posterior is tractable. Later we will move to actual intractable posteriors (neural nets)\n",
    "\n",
    "We will be using the very helpful function: [`pyro.random_module()`](http://docs.pyro.ai/en/stable/primitives.html?highlight=random_module#pyro.random_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.random_module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function grabs an object that inherites from `torch.nn.Module` and adds priors to its parameters\n",
    "\n",
    "In this case the parameters of the model are `linear.weight` and `linear.bias`\n",
    "\n",
    "> We will add a Normal prior to these parameters\n",
    "\n",
    "To set a prior we pick an object from [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal, Uniform\n",
    "\n",
    "Normal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Normal` object expects location $\\mu$ and scale $\\sigma$\n",
    "\n",
    "### Random variables\n",
    "\n",
    "To create random variables we use [`pyro.sample`](http://pyro.ai/examples/intro_part_i.html#The-pyro.sample-Primitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This expects a name and an object from [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)\n",
    "\n",
    "> We will set $\\sigma_\\epsilon$ (the noise scale) as a random variable with Uniform distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning\n",
    "\n",
    "We create a random variable for the likelihood using `pyro.sample`\n",
    "\n",
    "> The likelihood is set to normal with model prediction on $x^{(n)}$ as its mean and $\\sigma_\\epsilon$ as its scale\n",
    "\n",
    "We condition this RV to $y^{(n)}$ using the `obs` keyword\n",
    "\n",
    "We condition on the whole dataset (assuming independence) using [`pyro.plate`](http://docs.pyro.ai/en/stable/primitives.html#pyro.plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.plate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which expects a name and the size of the dataset\n",
    "\n",
    "`pyro.plate` can be used as iterator or as a context (vectorized plate)\n",
    "\n",
    "In depth about plates: http://pyro.ai/examples/svi_part_ii.html\n",
    "\n",
    "## Writing a pyro model\n",
    "\n",
    "Mixing all the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First we create the regressor\n",
    "regressor = LinearRegressor(degree=10)\n",
    "\n",
    "def model(x, y):\n",
    "    # We add normal priors to w and b\n",
    "    w_prior = Normal(torch.zeros(1, regressor.degree), \n",
    "                     1*torch.ones(1, regressor.degree)).to_event(1)\n",
    "    b_prior = Normal(torch.tensor([[0.]]), \n",
    "                     torch.tensor([[1.]])).to_event(1)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", regressor, \n",
    "                                       {'linear.weight': w_prior, 'linear.bias': b_prior})    \n",
    "    lifted_reg_model = lifted_module()\n",
    "    # We create a random variable for the scale\n",
    "    scale = pyro.sample(\"sigma\", Uniform(0., 1.))  \n",
    "    # Condition on the dataset assuming iid using a vectorized plate\n",
    "    with pyro.plate(\"observed_data\", size=len(x)):\n",
    "        #Get prediction (forward)\n",
    "        prediction_mean = lifted_reg_model(x)\n",
    "        pyro.sample(\"likelihood\", Normal(prediction_mean, scale), obs=y)\n",
    "        return prediction_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide\n",
    "\n",
    "In addition to the model Pyro requires a guide\n",
    "\n",
    "The guide tells which assumptions we will use for the approximate inference\n",
    "\n",
    "> We will review this in depth in the following classes\n",
    "\n",
    "In this case we will use an \"auto guide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
    "guide = AutoMultivariateNormal(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train this model using [`pyro.infer.SVI`](http://docs.pyro.ai/en/stable/inference_algos.html)\n",
    "\n",
    "This is the unified Variational Inference interface of Pyro\n",
    "\n",
    "> SVI expects model, guide, optimizer, loss and the number of samples\n",
    "\n",
    "We will use adam as optimizer and the Evidence Lower Bound (ELBO) as cost function \n",
    "\n",
    "> We will review what ELBO is in depth in the following classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "optim = pyro.optim.Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO(), num_samples=1000)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "    # svi.step has the same arguments as model()\n",
    "    loss = svi.step(x_torch, y_torch)\n",
    "    epoch_loss[k] = loss / len(x_torch)\n",
    "    if k % 1000 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (k + 1, loss / len(x_torch)))\n",
    "        \n",
    "fig, ax = plt.subplots(1, figsize=(5, 3), tight_layout=True)\n",
    "ax.plot(epoch_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the posterior\n",
    "\n",
    "To do this we get the empirical marginal for each parameter\n",
    "\n",
    "From the samples we can create histograms and compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import EmpiricalMarginal\n",
    "\n",
    "posterior = svi.run(x_torch, y_torch)\n",
    "\n",
    "params = [\"sigma\", \"module$$$linear.weight\", \"module$$$linear.bias\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "for param in params:\n",
    "    marginal_trace = EmpiricalMarginal(posterior, param).enumerate_support().detach().cpu().numpy()\n",
    "    if marginal_trace.ndim > 1:\n",
    "        for k in range(marginal_trace.shape[2]):\n",
    "            print(\"%s\\t%d\\t%0.4f\\t%0.4f\\t%0.4f\" %(param, k, np.median(marginal_trace[:, 0, k]),\n",
    "                                                 np.quantile(marginal_trace[:, 0, k], 0.05), \n",
    "                                                 np.quantile(marginal_trace[:, 0, k], 0.95)))\n",
    "            ax.hist(marginal_trace[:, 0, k], density=True, label=param+' '+str(k))\n",
    "            \n",
    "    else:\n",
    "        ax.hist(marginal_trace, density=True, label=param)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Delta\n",
    "from pyro.infer import TracePredictive\n",
    "\n",
    "def wrapped_model(x_data, y_data):\n",
    "    pyro.sample(\"prediction\", Delta(model(x_data, y_data)))\n",
    "trace_predictive = TracePredictive(wrapped_model, posterior, num_samples=100)\n",
    "x_plot = np.linspace(0, 1.0, num=100).astype('float32')\n",
    "posterior_predictive = trace_predictive.run(torch.from_numpy(x_plot), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_trace = EmpiricalMarginal(posterior_predictive, \"likelihood\").enumerate_support().detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(posterior_trace.shape[0]):\n",
    "    ax.plot(x_plot, posterior_trace[i, :, 0], 'k-', alpha=0.01)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "med = np.median(posterior_trace, axis=[0, -1])\n",
    "qua = np.quantile(posterior_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "ax.plot(x_plot, med)\n",
    "ax.fill_between(x_plot, qua[0][:, 0], qua[1][:, 0], alpha=0.5)\n",
    "ax.plot(x, y, 'k.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_trace = EmpiricalMarginal(posterior_predictive, \"prediction\").enumerate_support().detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(posterior_trace.shape[0]):\n",
    "    ax.plot(x_plot, posterior_trace[i, :, 0], 'k-', alpha=0.01)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "med = np.median(posterior_trace, axis=0)\n",
    "qua = np.quantile(posterior_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "ax.plot(x_plot, med)\n",
    "ax.fill_between(x_plot, qua[0][:, 0], qua[1][:, 0], alpha=0.5)\n",
    "ax.plot(x, y, 'k.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and scale learned for the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural networks\n",
    "\n",
    "[Artificial neural networks](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit?usp=sharing) (ANN) are non-linear parametric function approximators built by connecting simple units\n",
    "\n",
    "These units are simplified models of biological neurons: \n",
    "\n",
    "> linear regressor followed by a non-linear activation function\n",
    "\n",
    "Feed-forward ANN are organized in layers. Each layer has a certain amount of neurons (user-defined)\n",
    "\n",
    "> **Multilayer perceptron (MLP) architecture:** Every unit is connected to all units of its previous and next layers\n",
    "\n",
    "Different ways of connecting neurons yields different ANN architectures (convolutional, recurrent, etc)\n",
    "\n",
    "The parameter vector $\\theta$ includes the weights and biases of all the neurons\n",
    "\n",
    "- Let's consider a Gaussian prior for $\\theta$ and study the space of possible models\n",
    "- How does it compare to the linear regressor? \n",
    "    - What happens when you add more neurons? \n",
    "    - What happens if you remove the nonlinearity?\n",
    "    - What happens when you add more layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, Nh=10, sw=5, sb=5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(1, Nh)\n",
    "        self.output = torch.nn.Linear(Nh, 1)\n",
    "        for layer in [self.hidden1, self.output]:\n",
    "            torch.nn.init.normal_(layer.weight, 0.0, sw)\n",
    "            torch.nn.init.normal_(layer.bias, 0.0, sb)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.hidden1(x))\n",
    "        return self.output(z)\n",
    "    \n",
    "for i in range(10):\n",
    "    model = MLP()\n",
    "    y = model.forward(torch.from_numpy(x)).detach().numpy()\n",
    "    ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic interpretation of ANN\n",
    "\n",
    "Let's consider a simple MLP architecture for regression\n",
    "- one hidden layer with $H$ neurons\n",
    "- input dimensionality $D$ and output dimensionality $K$\n",
    "- $g(\\cdot)$ a nonlinear activation function (sigmoid, tanh, ReLU, etc)\n",
    "\n",
    "The jth neuron in the hidden layer\n",
    "$$\n",
    "z_j =  g \\left( b_j + \\sum_{d=1}^D w_{jd} x_d  \\right)\n",
    "$$\n",
    "The ith neuron in the output layer\n",
    "$$\n",
    "\\begin{align}\n",
    "f_i &=   b_i + \\sum_{j=1}^H w_{ij} z_j  \\nonumber \\\\\n",
    "&=  b_i + \\sum_{j=1}^H w_{ij} g \\left( b_j + \\sum_{d=1}^D w_{jd} x_d  \\right) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The vector parameter $\\vec \\theta$ contains the weight and biases of both layers\n",
    "\n",
    "We fit the parameters by minimizing the **Mean Square Error** cost function \n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_n  \\sum_i \\left(y_{i}^{(n)} - f_i(x^{(n)}) \\right)^2\n",
    "$$\n",
    "\n",
    "> This is equivalent to the **MLE solution with Gaussian likelihood** (known variance)\n",
    "\n",
    "Typically an L2 regularizer is included to penalize complexity and improve generalization\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_n  \\sum_i  \\left(y_{i}^{(n)} - f_i(x^{(n)}) \\right)^2 + \\lambda \\sum_k \\theta_k^2\n",
    "$$\n",
    "\n",
    "> This is equivalent to the **MAP solution with Gaussian likelihood and Gaussian prior** (zero-mean)\n",
    "\n",
    "In both cases there is no closed-form solution and we optimize with iterative methods (gradient descent)\n",
    "\n",
    "\n",
    "### In summary: Conventional neural network training obtains MLE/MAP point estimates\n",
    "\n",
    "For classification we arrive to the same conclusion except that \n",
    "- sigmoid or softmax activation is used in the output layer\n",
    "- cross-entropy cost function is used instead of MSE: **Bernoulli/Categorical likelihood**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Networks\n",
    "\n",
    "In the bayesian setting we are interested in the posterior of the parameters and predictions\n",
    "\n",
    "Assuming *iid* samples $\\mathcal{D} =\\{(x^{(1)}, y^{(1)}), \\ldots \\}$ we can write the posterior of $\\theta$\n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) \\propto p(\\mathcal{D}|\\theta) p(\\theta) = \\prod_n \\mathcal{N}(y^{(n)} | f(x^{(n)}), \\sigma^2) \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "> Because of the nested nonlinearities this posterior is not Gaussian!\n",
    "\n",
    "We have to use approximations: Laplacian method, Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://pyro.ai/examples/\n",
    "- https://alsibahi.xyz/snippets/2019/06/15/pyro_mnist_bnn_kl.html\n",
    "- https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd\n",
    "- https://github.com/paraschopra/bayesian-neural-network-mnist\n",
    "- https://in.pycon.org/cfp/workshops-2019/proposals/pyro-demystified-bayesian-deep-learning~en4lb/\n",
    "- https://www.reddit.com/r/MachineLearning/comments/a15r9o/p_bayesian_image_classifier_using_pyro_to_give/eangqch/\n",
    "- Bayesian layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
