{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "path_to_datasets = '/home/phuijse/datasets'\n",
    "mnist_train_data = torchvision.datasets.MNIST(path_to_datasets, train=True, download=True,\n",
    "                                              transform=torchvision.transforms.ToTensor())\n",
    "mnist_test_data = torchvision.datasets.MNIST(path_to_datasets, train=False, download=True,\n",
    "                                             transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(6, 1), tight_layout=True)\n",
    "for i in range(10):\n",
    "    image, label = mnist_train_data[i]\n",
    "    ax[i].imshow(image.numpy()[0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x_1, x_2) = p(x_2|x_1) p(x_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x_1|z)p(x_2|z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Variable Models (LVM)\n",
    "\n",
    "\n",
    "Let's say we want to model a dataset $X = (x_1, x_2, \\ldots, x_N)$ with $x_i \\in \\mathbb{R}^D$ \n",
    "\n",
    "> We are looking for $p(x)$\n",
    "\n",
    "Each sample has D attributes\n",
    "\n",
    "> These are the **observed variables** (visible space)\n",
    "\n",
    "To model the data we have to propose dependency relationships between attributes\n",
    "\n",
    "> Modeling correlation is difficult\n",
    "\n",
    "One alternative is to assume that what we observe is correlated due to *hidden causes*\n",
    "\n",
    "> These are the **latent variables** (hidden space)\n",
    "\n",
    "Models with latent variables are called **Latent Variable Models** (LVM)\n",
    "\n",
    "Then we get the marginal using\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) &= \\int_z p(x, z) \\,dz \\nonumber \\\\\n",
    "&= \\int_z p(x|z) p(z) \\,dz \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Did we gain anything? \n",
    "\n",
    "> The integral can be hard to solve (in some cases it is tractable)\n",
    "\n",
    "The answer is YES\n",
    "\n",
    "> We can propose simple $p(x|z)$ and $p(z)$ and get complex $p(x)$\n",
    "\n",
    "If the integral is intractable we try approximate inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is an algorithm to reduce the dimensionality of continous data\n",
    "\n",
    "Let's say we have $X = (x_1, x_2, \\ldots, x_N) \\in \\mathbb{R}^{N \\times D}$ \n",
    "\n",
    "In classical PCA we \n",
    "\n",
    "1. Compute covariance matrix $C = \\frac{1}{N} X^T X$\n",
    "1. Solve the eigen value problem $(C - \\lambda I)W = 0$\n",
    "\n",
    "This comes from \n",
    "\n",
    "$$\n",
    "\\min_W W^T C W, \\text{s.t.} ~ W^T W = I\n",
    "$$\n",
    "\n",
    "> PCA finds an **orthogonal transformation** $W$ that **minimizes the variance** of the projected data $XW$\n",
    "\n",
    "Then we can reduce the amount of columns of $W$ to reduce the dimensionality of $XW$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Classical PCA for MNIST using pytorch\n",
    "\n",
    "Implementation using Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, data, K=2):\n",
    "        self.data_mean = torch.mean(data, dim=0)\n",
    "        data_centered = data - self.data_mean.expand_as(data)\n",
    "        U, S, V = torch.svd(data_centered.T)\n",
    "        # S is sorted in decreasing order\n",
    "        self.W = U[:, :K]\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return torch.mm(x - self.data_mean.expand_as(x), self.W)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.data_mean + torch.mm(z, self.W.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project data and plot the reduced space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(test_data, K=2)\n",
    "Z = pca.encode(test_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "for digit in range(10):\n",
    "    mask = mnist_test_data.targets == digit\n",
    "    ax.scatter(Z[mask, 0].detach().numpy(), Z[mask, 1].detach().numpy(), \n",
    "               s=5, alpha=0.5, cmap=plt.cm.tab10, label=str(digit))\n",
    "plt.legend()\n",
    "ax.set_xlabel('PC 1'); ax.set_ylabel('PC 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most important principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(4, 1.5), tight_layout=True)\n",
    "for i in range(2):\n",
    "    ax[i].imshow(pca.W[:, i].reshape(28, 28).detach().numpy())\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title('PC %d' %(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 10, figsize=(8, 2), tight_layout=True)\n",
    "reconstructions = pca.decode(Z[:10, :]).reshape(-1, 28, 28).detach().numpy()\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(test_data[i, :].reshape(28, 28).detach().numpy(), cmap=plt.cm.Greys_r)\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].imshow(reconstructions[i], cmap=plt.cm.Greys_r)\n",
    "    ax[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic interpretation for PCA\n",
    "\n",
    "We can give a probabilistic interpretation to PCA as an LVM\n",
    "\n",
    "An observed sample $x_i \\in \\mathbb{R}^D$ is modeled as \n",
    "\n",
    "$$\n",
    "x_i = W z_i + B + \\epsilon\n",
    "$$\n",
    "\n",
    "> The observed variable is related to the latent variable via a **linear mapping**\n",
    "\n",
    "where \n",
    "- $B \\in \\mathbb{R}^D$ is the mean of $X$\n",
    "- $W \\in \\mathbb{R}^{D\\times K}$ is a linear transformation matrix\n",
    "- $\\epsilon$ is noise\n",
    "\n",
    "> $z_i \\in  \\mathbb{R}^K$ is a continuous latent variable with $K<D$\n",
    "\n",
    "#### Assumption: The noise is independent and Gaussian distributed with variance $\\sigma^2$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "p(x_i | z_i) = \\mathcal{N}(B + W z_i, I \\sigma^2)\n",
    "$$\n",
    "\n",
    "Note: In general factor analysis the noise has a diagonal covariance\n",
    "\n",
    "#### Assumption: The latent variable has a standard Gaussian prior\n",
    "\n",
    "$$\n",
    "p(z_i) = \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Marginal likelihood\n",
    "\n",
    "The Gaussian is conjugated to itself (convolution of Gaussians is Gaussian)\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) &= \\int p(x|z) p(z) \\,dz \\nonumber \\\\\n",
    "&= \\mathcal{N}(x|B, W^T W + I\\sigma^2 ) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> We have parametrized a normal with full covariance from to normals with diagonal covariance\"\n",
    "\n",
    "The parameters are calculated from \n",
    "- $\\mathbb{E}[x] = W\\mathbb{E}[z] + \\mu + \\mathbb{E}[\\epsilon]$\n",
    "- $\\mathbb{E}[(Wz + \\epsilon)(Wz + \\epsilon)^T] = W \\mathbb{E}[zz^T] W^T + \\mathbb{E}[\\epsilon \\epsilon^T]$\n",
    "\n",
    "#### Posterior\n",
    "\n",
    "Using Bayes we can obtain the posterior to go from observed to latent\n",
    "\n",
    "$$\n",
    "p(z|x) = \\mathcal{N}(z|M^{-1}W^T(x-B), M\\sigma^{-2} )\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "M = W^T W + I\\sigma^2\n",
    "$$\n",
    "\n",
    "#### Training\n",
    "\n",
    "We fit the model to find $W$, $\\mu$ and $\\sigma$ by maximizing the marginal likelihood\n",
    "\n",
    "$$\n",
    "\\max \\log L(W, B, \\sigma^2) = \\sum_{i=1}^N \\log p(x_i)\n",
    "$$\n",
    "\n",
    "From here we can do derivatives and obtain closed form solutions of the parameters\n",
    "\n",
    "> Solution for $W$ is equivalent to conventional PCA ($\\sigma^2 \\to 0$)\n",
    "\n",
    "> Now we have estimated $\\sigma$, we have error-bars for $z$ and the model is generative\n",
    "\n",
    "\n",
    "## Self-study\n",
    "- Barber, Chapter 21 and Murphy, Chapter 12\n",
    "- Model with categorical latent variables: **Gaussian Mixture Model** (INFO337)\n"
   ]
  },
  {
   "attachments": {
    "nn.svg": {
     "image/svg+xml": [
      "<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="909.12323"
   height="380.1163"
   version="1.1"
   id="svg289"
   sodipodi:docname="nn.svg"
   inkscape:version="0.92.4 5da689c313, 2019-01-14">
  <metadata
     id="metadata293">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title></dc:title>
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <sodipodi:namedview
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1"
     objecttolerance="10"
     gridtolerance="10"
     guidetolerance="10"
     inkscape:pageopacity="0"
     inkscape:pageshadow="2"
     inkscape:window-width="632"
     inkscape:window-height="1024"
     id="namedview291"
     showgrid="false"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0"
     inkscape:zoom="0.22083879"
     inkscape:cx="272.71758"
     inkscape:cy="191.74783"
     inkscape:window-x="386"
     inkscape:window-y="26"
     inkscape:window-maximized="0"
     inkscape:current-layer="svg289" />
  <g
     transform="matrix(1.1095695,0,0,1.1095695,-512.41399,-217.47562)"
     id="g282">
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,211.5 180,80"
       id="path2"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,211.5 180,120"
       id="path4"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,211.5 180,160"
       id="path6"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,251.5 180,40"
       id="path8"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,251.5 180,80"
       id="path10"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,251.5 180,120"
       id="path12"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,291.5 h 180"
       id="path14"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,291.5 180,40"
       id="path16"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,291.5 180,80"
       id="path18"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,331.5 180,-40"
       id="path20"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,331.5 h 180"
       id="path22"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,331.5 180,40"
       id="path24"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,371.5 180,-80"
       id="path26"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,371.5 180,-40"
       id="path28"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,371.5 h 180"
       id="path30"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,411.5 180,-120"
       id="path32"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,411.5 180,-80"
       id="path34"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,411.5 180,-40"
       id="path36"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,451.5 180,-160"
       id="path38"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,451.5 180,-120"
       id="path40"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,451.5 180,-80"
       id="path42"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,491.5 180,-200"
       id="path44"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,491.5 180,-160"
       id="path46"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,491.5 180,-120"
       id="path48"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,251.5 180,120"
       id="path50"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,291.5 180,80"
       id="path52"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,331.5 180,40"
       id="path54"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,371.5 h 180"
       id="path56"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,331.5 179.99997,-40"
       id="path58"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,371.5 179.99997,-80"
       id="path60"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="M 856.33333,331.5 H 1036.3333"
       id="path62"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,371.5 179.99997,-40"
       id="path64"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,331.5 179.99997,40"
       id="path66"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="M 856.33333,371.5 H 1036.3333"
       id="path68"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,251.5 180,-40"
       id="path70"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,251.5 h 180"
       id="path72"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,251.5 180,40"
       id="path74"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,251.5 180,80"
       id="path76"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,251.5 180,120"
       id="path78"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,251.5 180,160"
       id="path80"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,251.5 180,200"
       id="path82"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,251.5 180,240"
       id="path84"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,291.5 180,-80"
       id="path86"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,291.5 180,-40"
       id="path88"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,291.5 h 180"
       id="path90"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,291.5 180,40"
       id="path92"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,291.5 180,80"
       id="path94"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,291.5 180,120"
       id="path96"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,291.5 180,160"
       id="path98"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,291.5 180,200"
       id="path100"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,331.5 180,-120"
       id="path102"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,331.5 180,-80"
       id="path104"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,331.5 180,-40"
       id="path106"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,331.5 h 180"
       id="path108"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,331.5 180,40"
       id="path110"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,331.5 180,80"
       id="path112"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,331.5 180,120"
       id="path114"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,331.5 180,160"
       id="path116"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,371.5 180,-160"
       id="path118"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,371.5 180,-120"
       id="path120"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,371.5 180,-80"
       id="path122"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,371.5 180,-40"
       id="path124"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,371.5 h 180"
       id="path126"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,371.5 180,40"
       id="path128"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,371.5 180,80"
       id="path130"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,371.5 180,120"
       id="path132"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,211.5 180,200"
       id="path134"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,251.5 180,160"
       id="path136"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,291.5 180,120"
       id="path138"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,331.5 180,80"
       id="path140"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,371.5 180,40"
       id="path142"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,411.5 h 180"
       id="path144"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,451.5 180,-40"
       id="path146"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,491.5 180,-80"
       id="path148"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,411.5 180,-40"
       id="path150"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,211.5 180,240"
       id="path152"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,251.5 180,200"
       id="path154"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,291.5 180,160"
       id="path156"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,331.5 180,120"
       id="path158"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,371.5 180,80"
       id="path160"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,411.5 180,40"
       id="path162"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,451.5 h 180"
       id="path164"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,491.5 180,-40"
       id="path166"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,451.5 180,-80"
       id="path168"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,331.5 179.99997,80"
       id="path170"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,371.5 179.99997,40"
       id="path172"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,411.5 180,-200"
       id="path174"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,411.5 180,-160"
       id="path176"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,411.5 180,-120"
       id="path178"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,411.5 180,-80"
       id="path180"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,411.5 180,-40"
       id="path182"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,411.5 h 180"
       id="path184"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,411.5 180,40"
       id="path186"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,411.5 180,80"
       id="path188"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,331.5 179.99997,120"
       id="path190"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,371.5 179.99997,80"
       id="path192"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,451.5 180,-240"
       id="path194"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,451.5 180,-200"
       id="path196"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,451.5 180,-160"
       id="path198"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,451.5 180,-120"
       id="path200"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,451.5 180,-80"
       id="path202"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,451.5 180,-40"
       id="path204"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,451.5 h 180"
       id="path206"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 1036.3333,451.5 180,40"
       id="path208"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,211.5 180,40"
       id="path210"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,251.5 h 180"
       id="path212"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,291.5 180,-40"
       id="path214"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,331.5 180,-80"
       id="path216"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,371.5 180,-120"
       id="path218"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,411.5 180,-160"
       id="path220"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,451.5 180,-200"
       id="path222"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 496.33333,491.5 180,-240"
       id="path224"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,251.5 180,80"
       id="path226"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,291.5 180,40"
       id="path228"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,331.5 h 180"
       id="path230"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,371.5 180,-40"
       id="path232"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,411.5 180,-80"
       id="path234"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 676.33333,451.5 180,-120"
       id="path236"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,331.5 179.99997,-80"
       id="path238"
       inkscape:connector-curvature="0" />
    <path
       class="link"
       style="stroke:#505050;stroke-width:0.5px;stroke-opacity:1;marker-end:"
       d="m 856.33333,371.5 179.99997,-120"
       id="path240"
       inkscape:connector-curvature="0" />
    <circle
       r="15"
       class="node"
       id="0_0"
       style="fill:#ffffff;stroke:#333333"
       cx="496.33334"
       cy="211.5" />
    <circle
       r="15"
       class="node"
       id="0_1"
       style="fill:#ffffff;stroke:#333333"
       cx="496.33334"
       cy="251.5" />
    <circle
       r="15"
       class="node"
       id="0_2"
       style="fill:#ffffff;stroke:#333333"
       cx="496.33334"
       cy="291.5" />
    <circle
       r="15"
       class="node"
       id="0_3"
       style="fill:#ffffff;stroke:#333333"
       cx="496.33334"
       cy="331.5" />
    <circle
       r="15"
       class="node"
       id="0_4"
       style="fill:#ffffff;stroke:#333333"
       cx="496.33334"
       cy="371.5" />
    <circle
       r="15"
       class="node"
       id="0_5"
       style="fill:#ffffff;stroke:#333333"
       cx="496.33334"
       cy="411.5" />
    <circle
       r="15"
       class="node"
       id="0_6"
       style="fill:#ffffff;stroke:#333333"
       cx="496.33334"
       cy="451.5" />
    <circle
       r="15"
       class="node"
       id="0_7"
       style="fill:#ffffff;stroke:#333333"
       cx="496.33334"
       cy="491.5" />
    <circle
       r="15"
       class="node"
       id="1_0"
       style="fill:#ffffff;stroke:#333333"
       cx="676.33331"
       cy="251.5" />
    <circle
       r="15"
       class="node"
       id="1_1"
       style="fill:#ffffff;stroke:#333333"
       cx="676.33331"
       cy="291.5" />
    <circle
       r="15"
       class="node"
       id="1_2"
       style="fill:#ffffff;stroke:#333333"
       cx="676.33331"
       cy="331.5" />
    <circle
       r="15"
       class="node"
       id="1_3"
       style="fill:#ffffff;stroke:#333333"
       cx="676.33331"
       cy="371.5" />
    <circle
       r="15"
       class="node"
       id="1_4"
       style="fill:#ffffff;stroke:#333333"
       cx="676.33331"
       cy="411.5" />
    <circle
       r="15"
       class="node"
       id="1_5"
       style="fill:#ffffff;stroke:#333333"
       cx="676.33331"
       cy="451.5" />
    <circle
       r="15"
       class="node"
       id="2_0"
       style="fill:#ffffff;stroke:#333333"
       cx="856.33331"
       cy="331.5" />
    <circle
       r="15"
       class="node"
       id="2_1"
       style="fill:#ffffff;stroke:#333333"
       cx="856.33331"
       cy="371.5" />
    <circle
       r="15"
       class="node"
       id="3_0"
       style="fill:#ffffff;stroke:#333333"
       cx="1036.3334"
       cy="251.5" />
    <text
       class="text"
       dy="0.34999999em"
       style="font-size:12px"
       x="461.33334"
       y="531.5"
       id="text259">Input Layer ∈ ℝ⁸</text>
    <text
       class="text"
       dy="0.34999999em"
       style="font-size:12px"
       x="641.33331"
       y="531.5"
       id="text261">Hidden Layer ∈ ℝ⁶</text>
    <text
       class="text"
       dy="0.34999999em"
       style="font-size:12px"
       x="821.33331"
       y="531.5"
       id="text263">Hidden Layer ∈ ℝ²</text>
    <text
       class="text"
       dy="0.34999999em"
       style="font-size:12px"
       x="1001.3333"
       y="531.5"
       id="text265">Hidden Layer ∈ ℝ⁶</text>
    <circle
       r="15"
       class="node"
       id="3_1"
       style="fill:#ffffff;stroke:#333333"
       cx="1036.3334"
       cy="291.5" />
    <circle
       r="15"
       class="node"
       id="3_2"
       style="fill:#ffffff;stroke:#333333"
       cx="1036.3334"
       cy="331.5" />
    <circle
       r="15"
       class="node"
       id="3_3"
       style="fill:#ffffff;stroke:#333333"
       cx="1036.3334"
       cy="371.5" />
    <circle
       r="15"
       class="node"
       id="3_4"
       style="fill:#ffffff;stroke:#333333"
       cx="1036.3334"
       cy="411.5" />
    <circle
       r="15"
       class="node"
       id="3_5"
       style="fill:#ffffff;stroke:#333333"
       cx="1036.3334"
       cy="451.5" />
    <circle
       r="15"
       class="node"
       id="4_0"
       style="fill:#ffffff;stroke:#333333"
       cx="1216.3334"
       cy="211.5" />
    <circle
       r="15"
       class="node"
       id="4_1"
       style="fill:#ffffff;stroke:#333333"
       cx="1216.3334"
       cy="251.5" />
    <circle
       r="15"
       class="node"
       id="4_2"
       style="fill:#ffffff;stroke:#333333"
       cx="1216.3334"
       cy="291.5" />
    <circle
       r="15"
       class="node"
       id="4_3"
       style="fill:#ffffff;stroke:#333333"
       cx="1216.3334"
       cy="331.5" />
    <circle
       r="15"
       class="node"
       id="4_4"
       style="fill:#ffffff;stroke:#333333"
       cx="1216.3334"
       cy="371.5" />
    <circle
       r="15"
       class="node"
       id="4_5"
       style="fill:#ffffff;stroke:#333333"
       cx="1216.3334"
       cy="411.5" />
    <circle
       r="15"
       class="node"
       id="4_6"
       style="fill:#ffffff;stroke:#333333"
       cx="1216.3334"
       cy="451.5" />
    <circle
       r="15"
       class="node"
       id="4_7"
       style="fill:#ffffff;stroke:#333333"
       cx="1216.3334"
       cy="491.5" />
    <text
       class="text"
       dy="0.34999999em"
       style="font-size:12px"
       x="1181.3334"
       y="531.5"
       id="text280">Output Layer ∈ ℝ⁸</text>
  </g>
  <defs
     id="defs287">
    <marker
       id="arrow"
       viewBox="0 -5 10 10"
       markerWidth="7"
       markerHeight="7"
       orient="auto"
       refX="54">
      <path
         d="M 0,-5 10,0 0,5"
         style="fill:#505050;stroke:#505050"
         id="path284"
         inkscape:connector-curvature="0" />
    </marker>
  </defs>
</svg>
"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "Autoencoders are deep neural networks for dimensionality reduction\n",
    "\n",
    "![nn.svg](attachment:nn.svg)\n",
    "\n",
    "\n",
    "#### Architecture\n",
    "- Input and output dimensionality are equivalent\n",
    "- Code (bottleneck) has smaller dimensionality than input/output\n",
    "- **Encoder:** Neural net that maps input to code\n",
    "\n",
    "$$\n",
    "z = g_\\phi(x)\n",
    "$$\n",
    "\n",
    "- **Decoder:** Neural net that maps code to output\n",
    "\n",
    "$$\n",
    "\\hat x = f_\\theta(z)\n",
    "$$\n",
    "\n",
    "- Model is trained by matching the input with the output (data as targets) with MSE (or cross-entropy)\n",
    "\n",
    "$$\n",
    "\\hat \\theta, \\hat \\phi = \\text{arg} \\min_{\\phi, \\theta} \\| x - f_\\theta(g_\\phi(x)) \\|^2\n",
    "$$\n",
    "\n",
    "> **Probabilistic intepretation:** Maximum likelihood with spherical Gaussian (or Bernoulli) likelihood\n",
    "\n",
    "Typically an L2 regularizer on $\\theta$ and $\\phi$ is used\n",
    "\n",
    "> **Probabilistic intepretation:** Spherical Gaussian prior\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Autoencoder for MNIST in pytorch\n",
    "\n",
    "One module for the encoder and one for the decoder\n",
    "\n",
    "Two hidden layers each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim=28*28, hidden_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.activation(self.hidden1(z))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return self.output(h)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=28*28, hidden_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.code = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.hidden1(x))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return (self.code(h))\n",
    "    \n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=128):\n",
    "        super(AutoEncoder, self).__init__() \n",
    "        self.encoder = Encoder(latent_dim, hidden_dim=hidden_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim=hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = list(range(len(mnist_train_data)))\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.7*len(idx))\n",
    "\n",
    "train_loader = DataLoader(mnist_train_data, batch_size=128, drop_last=True,\n",
    "                          sampler=SubsetRandomSampler(idx[:split]))\n",
    "\n",
    "valid_loader = DataLoader(mnist_train_data, batch_size=128, drop_last=True,\n",
    "                          sampler=SubsetRandomSampler(idx[split:]))\n",
    "\n",
    "test_loader = DataLoader(mnist_test_data, batch_size=1024, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(latent_dim=2)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for nepoch in tqdm_notebook(range(10)):\n",
    "    # Plot latent space on the fly\n",
    "    Z = torch.tensor([], device='cuda') if use_gpu else torch.tensor([], device='cpu')\n",
    "    for x, label in test_loader:\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        Z = torch.cat((Z, model.encoder(x.reshape(-1, 28*28))))\n",
    "    Z = Z.detach().cpu().numpy()\n",
    "    ax.cla()\n",
    "    for digit in range(10):\n",
    "        mask = mnist_test_data.targets == digit\n",
    "        ax.scatter(Z[mask, 0], Z[mask, 1], \n",
    "                   s=5, alpha=0.5, cmap=plt.cm.tab10, label=str(digit))\n",
    "    plt.legend()\n",
    "    fig.canvas.draw()\n",
    "    # Actual training\n",
    "    epoch_loss = 0.0\n",
    "    for x, label in train_loader:\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        hatx = model.forward(x.reshape(-1, 28*28))\n",
    "        loss = criterion(hatx, x.reshape(-1, 28*28))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(\"%d %f\" %(nepoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_activation = torch.nn.Sigmoid()\n",
    "fig, ax = plt.subplots(2, 10, figsize=(8, 2), tight_layout=True)\n",
    "reconstructions = output_activation(hatx).reshape(-1, 28, 28).detach().numpy()\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(x.detach().numpy()[i+10, 0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].imshow(reconstructions[i+10], cmap=plt.cm.Greys_r)\n",
    "    ax[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI for LVM\n",
    "\n",
    "The LVM is defined by the joint density between observed $x$ and latent variables $z$\n",
    "\n",
    "$$\n",
    "p(x, z) = \\prod_i p(x_i|z_i) p(z_i)\n",
    "$$\n",
    "\n",
    "If we follow the **PCA recipe** (Linear mapping, Gaussian likelihood and Gaussian prior) we obtained an analytical posterior\n",
    "\n",
    "If we use a more complex (non-linear) mapping the posterior and evidence may not be tractable\n",
    "\n",
    "> In such case, we can use **VI**\n",
    "\n",
    "We propose an approximate posterior $q_\\nu(z)$ and maximize the ELBO\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(x) \\geq \\mathcal{L}(\\nu) &= \\mathbb{E}_{z\\sim q_\\nu(z|x)} \\left[\\log \\frac{p(x, z)}{q_\\nu(z|x)}\\right] \\nonumber \\\\\n",
    "&=- \\int q_\\nu(z|x) \\log \\frac{q_\\nu(z|x)}{p(x, z)} dz \\nonumber \n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "to find the best parameters $\\hat \\nu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "\n",
    "The Variational Autoencoder (VAE) is an LVM where **deep neural networks** are used to model the **conditional distributions** between latent $z$ and observed $x$ variables\n",
    "\n",
    "It was proposed simultaneously by [(Kingma and Welling, ICLR, Dec. 2013)](https://arxiv.org/pdf/1312.6114.pdf) and [(Rezende *et al*, ICML, Jan. 2014)](https://arxiv.org/abs/1401.4082) perhaps sparking the revived interest into **Deep Learning plus Approximate Bayesian Inference** that we see today\n",
    "\n",
    "\n",
    "The difference with a regular autoencoder is that the latent (code) is now a stochastic variable\n",
    "- a prior distribution is placed on $z$: $p(z)$\n",
    "- a neural network is used to model the likelihood: $p_\\theta(x | z)$\n",
    "- a neural network is used to model the approximate posterior: $q_\\phi(z|x)$\n",
    "- The parameters of the networks $\\theta$ and $\\phi$ are deterministic, *i.e.* not a \"fully\" bayesian neural net\n",
    "\n",
    "> Variational Inference is used to obtain the posterior and point estimates of the global parameters\n",
    "\n",
    "In what follows we will review the assumptions, training and the key contributions of this work to the field of Bayesian Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### VAE Assumption 1: \n",
    "\n",
    "The latent variable has a standard Gaussian prior\n",
    "\n",
    "$$\n",
    "p(z_i) = \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "#### VAE Assumption 2: \n",
    "\n",
    "The approximate posterior is a Factorized (diagonal) Gaussian\n",
    "\n",
    "$$\n",
    "q_\\phi(z_i|x_i) = \\mathcal{N}(\\mu_i, I \\sigma_i^2)\n",
    "$$\n",
    "\n",
    "**Problem:** The number of variational parameters scales with $N$, inpractical for large datasets\n",
    "\n",
    "> **Solution:** Local variational parameters are replaced by a function\n",
    "\n",
    "This is called **amortization**\n",
    "\n",
    "For example\n",
    "\n",
    "$$\n",
    "\\mu_i, \\sigma_i = g_\\phi(x_i)\n",
    "$$\n",
    "\n",
    "where $g_\\phi(\\cdot)$ is the **encoder network**\n",
    "\n",
    "#### VAE Assumption 3: \n",
    "\n",
    "The likelihood is chosen depending on the data\n",
    "\n",
    "- Continuous data: Factorized (diagonal) Gaussian\n",
    "$$\n",
    "\\mu_i, \\sigma_i = f_\\theta(z_i)\n",
    "$$\n",
    "- Binary data: Bernoulli\n",
    "$$\n",
    "p_i = f_\\theta(z_i)\n",
    "$$\n",
    "\n",
    "where $f_\\theta(\\cdot)$ is the **decoder network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a VAE in Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a \"dual-headed\" encoder\n",
    "\n",
    "The encoder outputs the parameters of the factorized gaussian associated to the latent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDual(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=28*28, hidden_dim=128):\n",
    "        super(EncoderDual, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.z_loc = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.z_scale = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.hidden1(x))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return self.z_loc(h), torch.exp(self.z_scale(h))\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim=28*28, hidden_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.activation(self.hidden1(z))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return self.output(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative process in the model \n",
    "\n",
    ">For each $i=1,2,\\ldots, N$\n",
    "- Sample: $z_i \\sim \\mathcal{N}(0, I)$ # Prior\n",
    "- Compute: $p_i = f_\\theta(z_i)$ # Decoder\n",
    "- Sample: $x_i \\sim \\text{Bernoulli}(p_i)$ # Likelihood\n",
    "\n",
    "And for the guide\n",
    "\n",
    ">For each $i=1,2,\\ldots, N$\n",
    "- Compute: $\\mu_i, \\sigma_i = g_\\phi(x_i)$ # Encoder\n",
    "- Sample: $z_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)$ # Approximate posterior\n",
    "\n",
    "\n",
    "Note that we are not  using are not using `pyro.param` in the guide\n",
    "> Instead of having $2N$ variational parameters we **amortize** with the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Bernoulli, Normal\n",
    "\n",
    "class VariationalAutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dim=128):\n",
    "        super(VariationalAutoEncoder, self).__init__() \n",
    "        self.encoder = EncoderDual(latent_dim, hidden_dim=hidden_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim=hidden_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def model(self, x):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", size=x.shape[0]):\n",
    "            # p(z)\n",
    "            z_loc = torch.zeros(x.shape[0], self.latent_dim, device=x.device)\n",
    "            z_scale = torch.ones(x.shape[0], self.latent_dim, device=x.device)\n",
    "            z = pyro.sample(\"latent\", Normal(z_loc, z_scale).to_event(1))\n",
    "            # p(x|z)\n",
    "            p_logits = self.decoder.forward(z)\n",
    "            pyro.sample(\"observed\", Bernoulli(logits=p_logits, validate_args=False).to_event(1), \n",
    "                        obs=x.reshape(-1, 28*28))\n",
    "    \n",
    "    def guide(self, x):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", size=x.shape[0]):\n",
    "            # q(z|x)\n",
    "            z_loc, z_scale  = self.encoder.forward(x.reshape(-1, 28*28))\n",
    "            pyro.sample(\"latent\", Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the VAE using SVI with the Mean field ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) # BUG?\n",
    "pyro.clear_param_store()\n",
    "\n",
    "vae = VariationalAutoEncoder(latent_dim=2)\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    vae = vae.cuda()\n",
    "    \n",
    "svi = pyro.infer.SVI(model=vae.model, \n",
    "                     guide=vae.guide, \n",
    "                     optim=pyro.optim.Adam({\"lr\": 1e-2}), \n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for nepoch in tqdm_notebook(range(10)):\n",
    "    # Plot latent space on the fly\n",
    "    Z = torch.tensor([], device='cuda') if use_gpu else torch.tensor([], device='cpu')\n",
    "    for x, label in test_loader:\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        Z = torch.cat((Z, torch.cat((vae.encoder(x.reshape(-1, 28*28))), dim=1)), dim=0)\n",
    "    Z = Z.detach().cpu().numpy()\n",
    "    ax.cla()\n",
    "    for digit in range(10):\n",
    "        mask = mnist_test_data.targets == digit\n",
    "        ax.errorbar(x=Z[mask, 0], y=Z[mask, 1], \n",
    "                    xerr=Z[mask, 2], yerr=Z[mask, 3],\n",
    "                    fmt='none', alpha=0.5, label=str(digit))\n",
    "    plt.legend()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # Actual training\n",
    "    epoch_loss = 0.0\n",
    "    for x, label in train_loader:\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        epoch_loss += svi.step(x)\n",
    "    print(\"%d %f\" %(nepoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    vae = vae.cpu()\n",
    "    \n",
    "output_activation = torch.nn.Sigmoid()\n",
    "fig, ax = plt.subplots(4, 10, figsize=(8, 4), tight_layout=True)\n",
    "\n",
    "x, label = next(iter(train_loader))\n",
    "z_loc, z_scale = vae.encoder.forward(x.reshape(-1, 28*28))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(x.detach().numpy()[i, 0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[0, i].axis('off')\n",
    "    reconstructions_mean = output_activation(vae.decoder(z_loc)).reshape(-1, 28, 28). detach().numpy()\n",
    "    ax[1, i].imshow(reconstructions_mean[i], cmap=plt.cm.Greys_r)\n",
    "    ax[1, i].axis('off')\n",
    "    z = Normal(z_loc, z_scale).rsample()\n",
    "    reconstructions = output_activation(vae.decoder(z)).reshape(-1, 28, 28). detach().numpy()\n",
    "    ax[2, i].imshow(reconstructions[i], cmap=plt.cm.Greys_r)\n",
    "    ax[2, i].axis('off')\n",
    "    ax[3, i].imshow(reconstructions_mean[i] - reconstructions[i], cmap=plt.cm.RdBu_r, \n",
    "                    vmin=-0.05, vmax=0.05)\n",
    "    ax[3, i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 30\n",
    "z_plot = np.linspace(-3, 3, num=M)\n",
    "big_imag = np.zeros(shape=(28*M, 28*M))\n",
    "\n",
    "for i in range(M):\n",
    "    for j in range(M):\n",
    "        z = torch.tensor(np.array([z_plot[j], z_plot[M-1-i]]), dtype=torch.float32)\n",
    "        xhat = output_activation(vae.decoder.forward(z)).reshape(28, 28). detach().numpy()\n",
    "        big_imag[i*28:(i+1)*28, j*28:(j+1)*28] = xhat\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9), tight_layout=True)\n",
    "Z_plot1, Z_plot2 = np.meshgrid(z_plot, z_plot)\n",
    "ax.matshow(big_imag, vmin=0.0, vmax=1.0, cmap=plt.cm.gray, extent=[-4, 4, -4, 4])\n",
    "H, xedge, yedge = np.histogram2d(Z[:, 0], Z[:, 1], bins=30, range=[[-4, 4], [-4, 4]])\n",
    "ax.contour(Z_plot1, Z_plot2, H.T, linewidths=3, levels=[1], cmap=plt.cm.Reds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details on the VAE training\n",
    "\n",
    "The ELBO in this case is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta, \\phi) = \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z) p(z) - \\log q_\\phi(z|x) \\right ] ,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "the VAE is trained by maximizing the ELBO via gradient descent\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_\\theta \\mathcal{L}(\\theta_{t}, \\phi_{t})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi_{t+1} = \\phi_{t} - \\eta \\nabla_\\phi \\mathcal{L}(\\theta_{t}, \\phi_{t})\n",
    "$$\n",
    "\n",
    "> We need the derivates of the ELBO wrt to $\\theta$ and $\\phi$\n",
    "\n",
    "### The derivative wrt to $\\theta$ \n",
    "\n",
    "We can ignore the terms not dependent on $\\theta$ \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}(\\theta, \\phi)  = \\nabla_\\theta \\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [\\log p_\\theta(x|z)\\right ] = \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\nabla_\\theta \\log  p_\\theta(x|z)\\right ] \n",
    "$$\n",
    "\n",
    "If we can sample from $q_\\phi(z|x)$ then we can \"Monte-Carlo approximate\" the expected value \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}(\\theta, \\phi) \\approx \\frac{1}{S} \\sum_{s=1}^S \\nabla_\\theta \\log p_\\theta(x|z^{(s)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The derivative wrt to $\\phi$ \n",
    "\n",
    "Let's consider a general function $f(z)$ that depends on $z$, e.g. $\\log p_\\theta(x|z) p(z)$ and $\\log q_\\phi(z|x)$\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi  \\mathcal{L}(\\theta, \\phi) =  \\nabla_\\phi \\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [f(z) \\right ] = \\nabla_\\phi \\int q_\\phi(z|x) f(z) dz\n",
    "$$\n",
    "\n",
    "Cannot do monte-carlo sampling. How do we solve this?\n",
    "\n",
    "##### Traditional solution: [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf) aka score function (SF)\n",
    "\n",
    "Using the identify $\\nabla_\\phi q_\\phi(z) = q_\\phi(z) \\nabla_\\phi\\log q_\\phi(z) $, and assuming that $f(z)$ does not depend on $\\phi$\n",
    "\n",
    "This way we can again rely on Monte-Carlo sampling of $q_\\phi(z|x)$\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi \\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [f(z)\\right ] = \\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [ f(z) \\nabla_\\phi \\log q_\\phi(z|x) \\right ] \\approx \\frac{1}{S} \\sum_{s=1}^S f(z^{(s)}) \\nabla_\\phi \\log q_\\phi(z^{(s)}|x)\n",
    "$$\n",
    "\n",
    "The estimator is unbiased but has high variance, in most case is not usable\n",
    "\n",
    "##### Key contribution in VAE: **Reparameterization trick**\n",
    "\n",
    "The latent is $z \\sim \\mathcal{N}(\\mu_\\phi, \\sigma_\\phi^2)$ so\n",
    "\n",
    "$$\n",
    "z = g(\\phi, \\epsilon) = \\mu_\\phi + \\epsilon \\sigma_\\phi, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "Instead of sampling $z$ we sample $\\epsilon$ and apply $g$ to obtain $z$, so\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [f(z) \\right ] =  \\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0, I)}\\left [  f(g(\\phi, \\epsilon))  \\right ] \n",
    "$$\n",
    "\n",
    "Now the expectation does not depend on $\\phi$ so the gradient\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi \\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0, I)}\\left [  f(g(\\phi, \\epsilon))  \\right ] = \\mathbb{E}_{\\epsilon\\sim \\mathcal{N}(0, I)}\\left [  f'(g(\\phi, \\epsilon)) \\nabla_\\phi g(\\phi, \\epsilon) \\right ] \n",
    "$$\n",
    "\n",
    "This estimator is unbiased and has a [much lower variance than REINFORCE](https://nbviewer.jupyter.org/github/gokererdogan/Notebooks/blob/master/Reparameterization%20Trick.ipynb)\n",
    "\n",
    "And we can do Monte-Carlo sampling\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi \\mathbb{E}_{z\\sim q_\\phi(z|x)}\\left [f(z)\\right ] \\approx \\frac{1}{S} \\sum_{s=1}^S f'(g(\\phi, \\epsilon^{(s)})) \\nabla_\\phi g(\\phi, \\epsilon^{(s)}) \n",
    "$$\n",
    "\n",
    "We only require that $z = g(\\phi, \\epsilon)$ and that $\\nabla_\\phi g$ exists\n",
    "\n",
    "#### Beyond the reparameterization trick: [Pathwise derivatives](https://arxiv.org/pdf/1806.01851.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Once again) more attention on the ELBO\n",
    "\n",
    "We can write the ELBO as\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta, \\phi) &= \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z) p(z) - \\log q_\\phi(z|x) \\right ] \\nonumber \\\\\n",
    "&= \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z) \\right ] - D_{KL}\\left[ q_\\phi(z|x) || p(z) \\right]\\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence maximizing de ELBO\n",
    "\n",
    "> Maximize the log likelihood when sampling from the approximate posterior: **Minimize Reconstruction error**\n",
    "\n",
    "> Minimize the divergence between the approximate posterior and prior: **Regularization**\n",
    "\n",
    "\n",
    "\n",
    "### Further reducing the variance by using closed-form terms\n",
    "\n",
    "\n",
    "The RHS term in the ELBO is the KL divergence between two multivariate Gaussian distributions which has a closed [form](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions)\n",
    "\n",
    "If we consider the assumptions in this case we have\n",
    "\n",
    "$$\n",
    "D_\\text{KL}\\left[q_\\phi(z|x) || p(z) \\right] = \\frac{1}{2}\\sum_{j=1}^K \\left(\\mu_j^2 + \\sigma_j^2 - \\log \\sigma_j^2 - 1 \\right)\n",
    "$$\n",
    "\n",
    "where $K$ is the dimensionality of the latent variable\n",
    "\n",
    "> The derivatives are straighforward and the variance is low\n",
    "\n",
    "### Pyro notes:\n",
    "\n",
    "- [`TraceMeanField_ELBO`](https://docs.pyro.ai/en/stable/inference_algos.html#pyro.infer.trace_mean_field_elbo.TraceMeanField_ELBO) assumes reparameterized latent variables in the guide and uses the analytical KL when available\n",
    "- [More on Pyro's variance reduction](https://pyro.ai/examples/svi_part_iii.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
