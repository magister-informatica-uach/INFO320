{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "from tqdm.notebook import tqdm\n",
    "#import sklearn.datasets\n",
    "print(pyro.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='int') # class labels\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0, 0.5, N) # radius\n",
    "    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2 # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = sklearn.datasets.make_moons(200, noise=0.2)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral_r, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial neural network\n",
    "\n",
    "- Artificial Neuron: Linear regressor plus activation function (e.g. sigmoid -> Logistic regression)\n",
    "\n",
    "$$\n",
    "\\hat y = \\mathcal{S}(WX + B)\\\\ \\mathcal{S}(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "- Fully connected layer: Several artificial neurons with the same input\n",
    "- Multilayer perceptron: Several fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network model in pytorch\n",
    "- class that inherits from `torch.nn.Module`\n",
    "- `__init__(self, args):` Define layers, e.g. fully-connected (`Linear`), convolutional (`Conv1D`, `Conv2D`)\n",
    "- `forward(self, x):` Define how layers are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet_classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=10):\n",
    "        super(NNet_classifier, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, num_hidden) \n",
    "        self.layer2 = torch.nn.Linear(num_hidden, num_hidden)\n",
    "        self.layer3 = torch.nn.Linear(num_hidden, 3)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        z = self.activation(self.layer1(x))\n",
    "        z = self.activation(self.layer2(z))\n",
    "        return self.layer3(z) #Neural net output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network for classification\n",
    "\n",
    "- Categorical label $y={0, 1, 2, 3, ..., K-1}$ \n",
    "- A softmax activation is used in the last layer\n",
    "$$\n",
    "\\hat y_k = \\frac{e^{f_k}}{\\sum_{j=1}^K e^{f_j}}, k=0,\\ldots, K-1\n",
    "$$\n",
    "- The network is trained by minimizing\n",
    "$$\n",
    "L(\\hat y, y) = - \\log \\frac{e^{f_y}}{\\sum_{j=1}^K e^{f_j}} = - f_y + \\log \\sum_{j=1}^K e^{f_j}\n",
    "$$\n",
    "the negative log likelihood (categorical)\n",
    "\n",
    "\n",
    "\n",
    "#### In Pytorch\n",
    "\n",
    "Define\n",
    "- `criterion`: Cost function to be minimized \n",
    "- `optimizer`: Optimization algorithm, typically based on stochastic gradient descent ($\\eta$ is the learning rate)\n",
    "$$\n",
    "w_{t+1} = w_{t} - \\eta \\nabla_w L(w_t)\n",
    "$$\n",
    "\n",
    "Training is performed by\n",
    "1. Evaluating the network using `forward`\n",
    "1. Calculating the error/loss selected in `criterion`\n",
    "1. Computing the derivatives of the error using the `backward` attribute of the error\n",
    "1. Updating parameters according to `optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, model):\n",
    "    ax[0].cla()\n",
    "    Z = model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "    zz = torch.nn.Softmax(dim=1)(Z).argmax(dim=1).detach().numpy().reshape(xx.shape[0], xx.shape[1])\n",
    "    ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "    \n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    for ax_ in ax:\n",
    "        ax_.relim()\n",
    "        ax_.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNet_classifier(num_hidden=10)\n",
    "display(model)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(x, y, phase='train'):\n",
    "    haty = model.forward(x) # Evaluate the model\n",
    "    loss = criterion(haty, y) # Calculate errors\n",
    "    if phase == 'train':\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Compute derivatives\n",
    "        optimizer.step() # Update parameters \n",
    "    return loss.item()\n",
    "\n",
    "x_train = torch.from_numpy(X.astype('float32'))#.reshape(-1, 1)\n",
    "y_train = torch.from_numpy(y)#.reshape(-1, 1)\n",
    "epoch_loss = np.zeros(shape=(6000,)) \n",
    "\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = train_one_epoch(x_train, y_train)\n",
    "    if k % 100 == 0: \n",
    "        update_plot(k, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "Z = torch.nn.Softmax(dim=1)(model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32'))))\n",
    "zz = Z.argmax(dim=1).detach().numpy().reshape(xx.shape[0], xx.shape[1])\n",
    "ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "    \n",
    "zz = -(Z*(Z+1e-32).log()).sum(dim=1).reshape(xx.shape).detach().numpy()\n",
    "cf = ax[1].contourf(xx, yy, zz, cmap=plt.cm.Blues, alpha=0.75, vmin=0., vmax=np.log(3))\n",
    "fig.colorbar(cf, ax=ax[1])\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[1].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian neural network\n",
    "\n",
    "Instead of training via maximum likelihood we place a prior on the parameters $p(w)$ and aim for the  posterior\n",
    "\n",
    "$$\n",
    "p(w| \\mathcal{D}) = \\frac{p(\\mathcal{D}|w)p(w)}{\\int p(\\mathcal{D}|w) p(w) \\,dw}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Problem:  No closed form for the evidence for hierchical/non-linear models\n",
    "\n",
    "#### Solution for models with few parameters: MCMC\n",
    "\n",
    "#### Solution for models with millions of parameters: Variational inference\n",
    "\n",
    "Propose an approximate (simple) posterior $q_\\phi(w)$ and optimize so that it looks similar to the actual posterior\n",
    "\n",
    "We do this by maximizing a lower bound on the evidence\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\mathbb{E}_{q_\\phi(w)}[ \\log p(\\mathcal{D}|w)] - \\text{KL}[q_\\phi(w)|p(w)]\n",
    "$$\n",
    "\n",
    "An we use $q_\\phi(w)$ as our replacement for $p(w|\\mathcal{D})$ to calculate the **posterior predictive distribution**\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{x}, \\mathcal{D}) = \\int p(\\mathbf{y}|\\mathbf{x}, w) p(w| \\mathcal{D}) \\,dw\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "class BayesianNNet_classifier(pyro.nn.PyroModule):\n",
    "    def __init__(self, num_hidden=10, prior_std=1.):\n",
    "        super().__init__()\n",
    "        prior = dist.Normal(0, prior_std)\n",
    "        self.layer1 = pyro.nn.PyroModule[torch.nn.Linear](2, num_hidden)\n",
    "        self.layer1.weight = pyro.nn.PyroSample(prior.expand([num_hidden, 2]).to_event(2))\n",
    "        self.layer1.bias = pyro.nn.PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer2 = pyro.nn.PyroModule[torch.nn.Linear](num_hidden, num_hidden)\n",
    "        self.layer2.weight = pyro.nn.PyroSample(prior.expand([num_hidden, num_hidden]).to_event(2))\n",
    "        self.layer2.bias = pyro.nn.PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer3 = pyro.nn.PyroModule[torch.nn.Linear](num_hidden, 3)\n",
    "        self.layer3.weight = pyro.nn.PyroSample(prior.expand([3, num_hidden]).to_event(2))\n",
    "        self.layer3.bias = pyro.nn.PyroSample(prior.expand([3]).to_event(1))        \n",
    "        \n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.activation(self.layer1(x))\n",
    "        h = self.activation(self.layer2(h))\n",
    "        p = self.layer3(h).squeeze(1)\n",
    "        with pyro.plate(\"data\", size=x.shape[0], dim=-1):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(logits=p), obs=y)\n",
    "            #obs = pyro.sample(\"obs\", dist.Bernoulli(logits=p), obs=y)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, samples):\n",
    "    ax[0].cla()\n",
    "    p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)\n",
    "    zz = p.argmax(dim=1).reshape(xx.shape).detach().numpy()\n",
    "    ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    \n",
    "\n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    for ax_ in ax:\n",
    "        ax_.relim()\n",
    "        ax_.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "model = BayesianNNet_classifier(num_hidden=10, prior_std=10.)\n",
    "print(pyro.poutine.trace(model).get_trace(x_train, y_train).format_shapes())\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-3}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = svi.step(x_train, y_train)\n",
    "    if k % 100 == 0:\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=10)\n",
    "        samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "        update_plot(k, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=100)\n",
    "samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(9, 2), tight_layout=True)\n",
    "\n",
    "for k in range(4):\n",
    "    zz = samples[\"obs\"][k].reshape(xx.shape).detach().numpy()\n",
    "    ax[k].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[k].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)\n",
    "zz = p.argmax(dim=1).reshape(xx.shape).detach().numpy()\n",
    "ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "\n",
    "zz = -(p/100.*(p/100.+1e-32).log()).sum(dim=1).reshape(xx.shape).detach().numpy()\n",
    "cf = ax[1].contourf(xx, yy, zz, cmap=plt.cm.Blues, alpha=0.75)\n",
    "fig.colorbar(cf, ax=ax[1])\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[1].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian neural network for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "mnist_test = torchvision.datasets.MNIST(root='~/datasets', train=False, transform=torchvision.transforms.ToTensor())\n",
    "mnist_loader = torch.utils.data.DataLoader(mnist_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "class BayesianNNet_classifier(pyro.nn.PyroModule):\n",
    "    def __init__(self, ninput=28*28, num_hidden=10, prior_std=1.):\n",
    "        super().__init__()\n",
    "        prior = dist.Normal(0, prior_std)\n",
    "        \n",
    "        self.layer1 = pyro.nn.PyroModule[torch.nn.Linear](ninput, num_hidden)\n",
    "        self.layer1.weight = pyro.nn.PyroSample(prior.expand([num_hidden, ninput]).to_event(2))\n",
    "        self.layer1.bias = pyro.nn.PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer2 = pyro.nn.PyroModule[torch.nn.Linear](num_hidden, 10)\n",
    "        self.layer2.weight = pyro.nn.PyroSample(prior.expand([10, num_hidden]).to_event(2))\n",
    "        self.layer2.bias = pyro.nn.PyroSample(prior.expand([10]).to_event(1))\n",
    "        \n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        #x = self.activation(self.conv1(x))\n",
    "        p = self.layer2(self.activation(self.layer1(x))).squeeze(1)\n",
    "        with pyro.plate(\"data\", size=x.shape[0], dim=-1):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(logits=p), obs=y)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "model = BayesianNNet_classifier(num_hidden=100)\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "epoch_loss = np.zeros(shape=(100,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    for images, labels in mnist_loader:\n",
    "        # calculate the loss and take a gradient step\n",
    "        epoch_loss[k] += svi.step(images.reshape(-1, 28*28), labels)\n",
    "    #break    \n",
    "    if k % 1 == 0:\n",
    "        ax[0].cla()\n",
    "        line2[0].set_xdata(range(k))\n",
    "        line2[0].set_ydata(epoch_loss[:k])\n",
    "        for ax_ in ax:\n",
    "            ax_.relim()\n",
    "            ax_.autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the mode and entropy from the posterior predictive distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=100)\n",
    "samples = predictive(mnist_test.data.reshape(-1, 28*28)/255.)\n",
    "p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=10).sum(dim=0)\n",
    "mode = p.argmax(dim=1)\n",
    "entropy = -(p/100.*(p/100.+1e-32).log()).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the most uncertain digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(5, 3), tight_layout=True)\n",
    "\n",
    "digit = 4\n",
    "mask = mnist_test.targets == digit\n",
    "idx = np.argsort(entropy[mask].numpy())[::-1]\n",
    "k = 0\n",
    "def update(x):\n",
    "    global k\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    ax[0].imshow(mnist_test.data[mask][idx[k]], cmap=plt.cm.Greys_r)\n",
    "    res = ax[1].hist(samples['obs'][:, mask][:, idx[k]], range=(0, 10))\n",
    "    ax[1].set_title(\"%d %0.4f\" %(mode[mask][idx[k]], entropy[mask][idx[k]]))\n",
    "    ax[1].set_xticks(range(10));\n",
    "    k+=1\n",
    "\n",
    "bnext = widgets.Button(description='next')\n",
    "bnext.on_click(update)\n",
    "bnext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder in pyro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDual(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=28*28, hidden_dim=128):\n",
    "        super(EncoderDual, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.z_loc = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.z_scale = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.hidden1(x))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return self.z_loc(h), torch.exp(self.z_scale(h))\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim=28*28, hidden_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.activation(self.hidden1(z))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return self.output(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Bernoulli, Normal\n",
    "\n",
    "class VariationalAutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dim=128):\n",
    "        super(VariationalAutoEncoder, self).__init__() \n",
    "        self.encoder = EncoderDual(latent_dim, hidden_dim=hidden_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim=hidden_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def model(self, x):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", size=x.shape[0]):\n",
    "            # p(z)\n",
    "            z_loc = torch.zeros(x.shape[0], self.latent_dim, device=x.device)\n",
    "            z_scale = torch.ones(x.shape[0], self.latent_dim, device=x.device)\n",
    "            z = pyro.sample(\"latent\", Normal(z_loc, z_scale).to_event(1))\n",
    "            # p(x|z)\n",
    "            p_logits = self.decoder.forward(z)\n",
    "            pyro.sample(\"observed\", Bernoulli(logits=p_logits, validate_args=False).to_event(1), \n",
    "                        obs=x.reshape(-1, 28*28))\n",
    "    \n",
    "    def guide(self, x):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", size=x.shape[0]):\n",
    "            # q(z|x)\n",
    "            z_loc, z_scale  = self.encoder.forward(x.reshape(-1, 28*28))\n",
    "            pyro.sample(\"latent\", Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(root='~/datasets', train=True, \n",
    "                                         transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = list(range(len(mnist_train)))\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.7*len(idx))\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=128, drop_last=True,\n",
    "                          sampler=SubsetRandomSampler(idx[:split]))\n",
    "\n",
    "valid_loader = DataLoader(mnist_train, batch_size=128, drop_last=True,\n",
    "                          sampler=SubsetRandomSampler(idx[split:]))\n",
    "\n",
    "test_loader = DataLoader(mnist_test, batch_size=1024, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) # BUG?\n",
    "pyro.clear_param_store()\n",
    "\n",
    "vae = VariationalAutoEncoder(latent_dim=2)\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    vae = vae.cuda()\n",
    "    \n",
    "svi = pyro.infer.SVI(model=vae.model, \n",
    "                     guide=vae.guide, \n",
    "                     optim=pyro.optim.Adam({\"lr\": 1e-2}), \n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for nepoch in tqdm(range(10)):\n",
    "    # Plot latent space on the fly\n",
    "    Z = torch.tensor([], device='cuda') if use_gpu else torch.tensor([], device='cpu')\n",
    "    for x, label in test_loader:\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        Z = torch.cat((Z, torch.cat((vae.encoder(x.reshape(-1, 28*28))), dim=1)), dim=0)\n",
    "    Z = Z.detach().cpu().numpy()\n",
    "    ax.cla()\n",
    "    for digit in range(10):\n",
    "        mask = mnist_test.targets == digit\n",
    "        ax.errorbar(x=Z[mask, 0], y=Z[mask, 1], \n",
    "                    xerr=Z[mask, 2], yerr=Z[mask, 3],\n",
    "                    fmt='none', alpha=0.5, label=str(digit))\n",
    "    plt.legend()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # Actual training\n",
    "    epoch_loss = 0.0\n",
    "    for x, label in train_loader:\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        epoch_loss += svi.step(x)\n",
    "    print(\"%d %f\" %(nepoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    vae = vae.cpu()\n",
    "    \n",
    "output_activation = torch.nn.Sigmoid()\n",
    "fig, ax = plt.subplots(2, 10, figsize=(8, 2), tight_layout=True)\n",
    "\n",
    "x, label = next(iter(train_loader))\n",
    "z_loc, z_scale = vae.encoder.forward(x.reshape(-1, 28*28))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(x.detach().numpy()[i, 0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[0, i].axis('off')\n",
    "    reconstructions_mean = output_activation(vae.decoder(z_loc)).reshape(-1, 28, 28). detach().numpy()\n",
    "    ax[1, i].imshow(reconstructions_mean[i], cmap=plt.cm.Greys_r)\n",
    "    ax[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 30\n",
    "z_plot = np.linspace(-3, 3, num=M)\n",
    "big_imag = np.zeros(shape=(28*M, 28*M))\n",
    "\n",
    "for i in range(M):\n",
    "    for j in range(M):\n",
    "        z = torch.tensor(np.array([z_plot[j], z_plot[M-1-i]]), dtype=torch.float32)\n",
    "        xhat = output_activation(vae.decoder.forward(z)).reshape(28, 28). detach().numpy()\n",
    "        big_imag[i*28:(i+1)*28, j*28:(j+1)*28] = xhat\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9), tight_layout=True)\n",
    "Z_plot1, Z_plot2 = np.meshgrid(z_plot, z_plot)\n",
    "ax.matshow(big_imag, vmin=0.0, vmax=1.0, cmap=plt.cm.gray, extent=[-4, 4, -4, 4])\n",
    "#H, xedge, yedge = np.histogram2d(Z[:, 0], Z[:, 1], bins=30, range=[[-4, 4], [-4, 4]])\n",
    "#ax.contour(Z_plot1, Z_plot2, H.T, linewidths=3, levels=[1], cmap=plt.cm.Reds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks and irregular sampling other than phased-lstm?\n",
    "\n",
    "- https://arxiv.org/abs/1806.07366v5, https://arxiv.org/abs/1907.03907\n",
    "- http://proceedings.mlr.press/v80/binkowski18a.html\n",
    "- https://openreview.net/forum?id=r1efr3C9Ym, https://papers.nips.cc/paper/6475-a-scalable-end-to-end-gaussian-process-adapter-for-irregularly-sampled-time-series-classification.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
