{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "p = scipy.stats.bernoulli(0.6).rvs(1000)\n",
    "G1 = scipy.stats.norm(loc=5., scale=2.).rvs(1000) # N(5, sqrt(2))\n",
    "G2 = scipy.stats.norm(loc=-2., scale=1.5).rvs(1000) # N(0, sqrt(10))\n",
    "data = np.concatenate((G1[p==1], G2[p==0])) # Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    #mu_prior = pyro.sample(\"mean\", pyro.distributions.Normal(0, 10))\n",
    "    #sd_prior = pyro.sample(\"sigma\", pyro.distributions.HalfNormal(5, 10))\n",
    "    #return pyro.sample(\"obs\", pyro.distributions.Normal(mu_prior, sd_prior))\n",
    "    mu = torch.tensor([[5.], [-2.]])\n",
    "    sd = torch.tensor([[2.], [1.5]])\n",
    "    pi = torch.tensor([np.log(0.6), np.log(0.4)])\n",
    "    return pyro.sample(\"obs\", pyro.distributions.MixtureOfDiagNormals(mu, sd**2, pi))\n",
    "    \n",
    "\n",
    "def guide():\n",
    "    mu_loc = pyro.param(\"mu_loc\", torch.tensor([0.], dtype=torch.float32))\n",
    "    mu_scale = pyro.param(\"mu_scale\", torch.tensor([1.], dtype=torch.float32), \n",
    "                          constraint=torch.distributions.constraints.positive)\n",
    "    sd_loc = pyro.param(\"sd_loc\", torch.tensor([1.], dtype=torch.float32), \n",
    "                        constraint=torch.distributions.constraints.positive)\n",
    "    sd_scale = pyro.param(\"sd_scale\", torch.tensor([1], dtype=torch.float32), \n",
    "                          constraint=torch.distributions.constraints.positive)\n",
    "    \n",
    "    mu = pyro.sample(\"mean\", pyro.distributions.Normal(mu_loc, mu_scale))\n",
    "    sd = pyro.sample(\"sigma\", pyro.distributions.Normal(sd_loc, sd_scale))\n",
    "\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "#guide = AutoDiagonalNormal(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#data = 5*np.random.randn(1000)\n",
    "data_torch = torch.from_numpy(data.astype('float32'))\n",
    "conditioned_model = pyro.condition(model, data={\"obs\": data_torch})\n",
    "\n",
    "svi = pyro.infer.SVI(model=conditioned_model,\n",
    "                     guide=guide,\n",
    "                     optim=pyro.optim.Adam({\"lr\": 1e-3}),\n",
    "                     loss=pyro.infer.Trace_ELBO(),\n",
    "                     num_samples=10)\n",
    "\n",
    "losses, a,b  = [], [], []\n",
    "num_steps = 2000\n",
    "for epoch in range(num_steps):\n",
    "    #a.append(pyro.param(\"mu\").detach().numpy())\n",
    "    #b.append(pyro.param(\"cov\").detach().numpy())\n",
    "    losses.append(svi.step())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses)\n",
    "ax.set_title(\"ELBO\")\n",
    "ax.set_xlabel(\"step\")\n",
    "ax.set_ylabel(\"loss\");\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.hist(data, bins=30, density=True, alpha=0.75);\n",
    "x_plot = torch.linspace(np.amin(data), np.amax(data), steps=1000)\n",
    "#line = ax.plot(x_plot.numpy(), model.pdf(x_plot).detach().numpy(), lw=2)\n",
    "\n",
    "#ax.plot(x_plot, np.exp(-0.5*(x_plot - 2.15)**2/3.7580**2)/np.sqrt(2.0*np.pi*3.75**2))\n",
    "\n",
    "#anim = animation.FuncAnimation(fig, update_plot, frames=100, interval=20, \n",
    "#                               repeat=True, blit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "fig, ax = plt.subplots()\n",
    "for i in [0, 100, 1000, -1]:\n",
    "    ellipse = Ellipse((a[i][0], a[i][1]),\n",
    "            width=b[i][0], height=b[i][1], facecolor='none', ls='--', edgecolor='k')\n",
    "\n",
    "    ax.add_patch(ellipse)\n",
    "    \n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([-5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Variable Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian latent variable model: Probabilistic PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de datos $X = (x_1, x_2, \\ldots, x_N)$ con $x_i \\in \\mathbb{R}^D$\n",
    "\n",
    "Prior Gaussiano estándar\n",
    "$$\n",
    "p(z) = \\mathcal{N}(z|0, I)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x|z)  = \\mathcal{N}(x|Wz + \\mu, I\\sigma^2) \n",
    "$$\n",
    "\n",
    "Verosimilitud marginal (evidencia)\n",
    "$$\n",
    "p(x) = \\int p(x|z) p(z) \\,dz = \\mathcal{N}(x|\\mu, W^T W + I\\sigma^2 )\n",
    "$$\n",
    "\n",
    "y el posterior\n",
    "\n",
    "$$\n",
    "p(z|x) = \\mathcal{N}(z|M^{-1}W^T(x-\\mu), M\\sigma^{-2} )\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "M = W^T W + I\\sigma^2\n",
    "$$\n",
    "\n",
    "Entrenamos maximizando la log verosimilitud\n",
    "$$\n",
    "\\max \\log L(W,\\mu, \\sigma^2) = \\sum_{i=1}^N \\log p(x_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import theano.tensor as tt\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "N = 10000 \n",
    "M = 3  # dimensions of the data\n",
    "D = 2  # dimensions of the projection\n",
    "\n",
    "np.random.seed(10)\n",
    "C = np.random.randn(M, M)\n",
    "C = np.dot(C.T, C)\n",
    "X = np.random.multivariate_normal(np.zeros(shape=(M, )), C, size=N)\n",
    "# In the general case we subtract the mean and divide by std, \n",
    "X = X - np.mean(X, axis=0)\n",
    "X = X/np.std(X, axis=0)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], s=2)\n",
    "\n",
    "pca = PCA(n_components=2, whiten=False)\n",
    "R = pca.fit_transform(X)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(R[:, 0], R[:, 1], s=1)\n",
    "_ = plt.title('PCA projection')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with pm.Model() as PPCA:\n",
    "    s = pm.HalfCauchy('s', beta=5, shape=[1,])\n",
    "    w = pm.Normal('w', mu=tt.zeros([D, M]), sd=tt.ones([D, M]), shape=[D, M])\n",
    "    z = pm.Normal('z', mu=tt.zeros([N, D]), sd=tt.ones([N, D]), shape=[N, D])\n",
    "    x = pm.Normal('x', mu=z.dot(w), sd=s*tt.ones([N, M]), shape=[N, M], observed=X)  \n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=2000, method=inference, obj_optimizer=pm.adam(learning_rate=1e-1))\n",
    "\n",
    "_ = plt.plot(-inference.hist)\n",
    "plt.ylabel('Evidence lower bound (ELBO)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid()\n",
    "\n",
    "with PPCA:\n",
    "    trace = approx.sample(draws=1000)\n",
    "    ppc = pm.sample_ppc(trace=trace, samples=100)\n",
    "_ = pm.traceplot(trace=trace, varnames=['w', 's'])\n",
    "\n",
    "W_avg = np.mean(trace['w'], axis=0)\n",
    "s_avg = np.mean(trace['s'], axis=0)\n",
    "print(\"Average W\")\n",
    "print(W_avg)\n",
    "print(\"Average sigma: %f\" %(s_avg))\n",
    "\n",
    "x_reconstructed = ppc['x'][0, :, :]  # this is one draw from the posterior\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], s=2)\n",
    "ax.set_title('Input data')             \n",
    "bx, by, bz = ax.get_xbound(), ax.get_ybound(), ax.get_zbound()      \n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.set_title(\"Sampled data\")\n",
    "ax.scatter(x_reconstructed[:, 0], x_reconstructed[:, 1], x_reconstructed[:, 2], s=1, alpha=0.5)\n",
    "t = np.linspace(-4, 4, num=100)\n",
    "ax.set_xbound(bx)\n",
    "ax.set_ybound(by)\n",
    "ax.set_zbound(bz)\n",
    "\n",
    "z_trace_avg = np.mean(trace['z'], axis=0)\n",
    "z_trace_std = np.std(trace['z'], axis=0)\n",
    "z_trace_var = np.mean(np.var(trace['z'], axis=1), axis=0)\n",
    "# Sort the new axis in decreasing order of variance\n",
    "axis_order = np.argsort(z_trace_var)[::-1]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 3))\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.errorbar(z_trace_avg[:, axis_order[0]], z_trace_avg[:, axis_order[1]], \n",
    "            z_trace_std[:, axis_order[0]], z_trace_std[:, axis_order[1]], fmt='none', alpha= 0.5)\n",
    "plt.title('Average z from trace')\n",
    "\n",
    "Z_test = np.dot(X, np.dot(np.linalg.inv(np.dot(W_avg.T, W_avg) + np.eye(M)*s_avg**2 ), W_avg.T))\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.scatter(Z_test[:, axis_order[0]], Z_test[:, axis_order[1]], s=1, alpha=0.5)\n",
    "_ = plt.title('Average z by hand')\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.scatter(R[:, 0], R[:, 1], s=1, alpha=0.5)\n",
    "_ = plt.title('z from sklearn PCA')\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "# SKLEARN gives you the new axis already sorted by variance, also axis might appear rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Latent Variable: Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.reddit.com/r/MachineLearning/comments/9g1rxs/d_how_is_the_log_marginal_likelihood_of/\n",
    "\n",
    "https://colinraffel.com/blog/gans-and-divergence-minimization.html\n",
    "\n",
    "https://www.inference.vc/maximum-likelihood-for-representation-learning-2/\n",
    "\n",
    "https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b\n",
    "\n",
    "https://pyro.ai/, https://pyro.ai/examples/intro_part_i.html\n",
    "\n",
    "https://www.tuananhle.co.uk/notes/reverse-forward-kl.html\n",
    "\n",
    "https://blog.evjang.com/2016/08/variational-bayes.html\n",
    "\n",
    "https://dibyaghosh.com/blog/probability/kldivergence.html\n",
    "\n",
    "https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
