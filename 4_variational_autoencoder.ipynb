{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Liebler divergence between two continuous distributions is\n",
    "\n",
    "$$\n",
    "D_\\text{KL}\\left[q (x) || p(x)\\right] = \\mathbb{E}_{x\\sim q(x)} \\left[ \\log \\frac{q(x)}{p(x)}\\right] = \\int q(x) \\log \\frac{q(x)}{p(x)} \\,dx \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our generative model with latent variable $z$ y observed variable $x$ is described by the joint density\n",
    "\n",
    "$$\n",
    "p_\\theta(x, z) = p_\\theta(x|z) p(z)\n",
    "$$\n",
    "\n",
    "Tipically, we find the parameters $\\theta$ of our generator using a maximum likelihood approach over the **evidence** or **marginal likelihood** $p_\\theta (x)$\n",
    "\n",
    "$$\n",
    "\\max_\\theta p_\\theta (x) = \\int p_\\theta(x|z) p(z) \\,dz\n",
    "$$\n",
    "\n",
    "but the integral is in general intractable.\n",
    "\n",
    "Instead we will optimize a lower bound of the evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simpler posterior $q_\\phi(z|x)$ and write the divergence between this and the generator posterior\n",
    "\n",
    "$$\n",
    "p_\\theta(z|x) = \\frac{p_\\theta(x|z) p(z)}{p_\\theta(x)}, \n",
    "$$\n",
    "where $p(z)$ is a prior specified by the user\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_\\text{KL}\\left[q_\\phi(z|x) || p_\\theta(z|x)\\right] &=\n",
    "\\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [ \\log \\frac{q_\\phi(z|x)}{p_\\theta(z|x)}\\right ]\\nonumber \\\\\n",
    "&= \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [ \\log \\frac{p_\\theta(x)}{p_\\theta(x|z)}\\frac{q_\\phi(z|x)}{p(z)} \\right ] \\nonumber \\\\\n",
    "&= \\log p_\\theta(x) + \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [ - \\log p_\\theta(x|z) + \\log \\frac{q_\\phi(z|x)}{p(z)} \\right ] \\nonumber \\\\\n",
    "&= \\log p_\\theta(x) - \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z)\\right ] + D_\\text{KL}\\left[q_\\phi(z|x) || p(z)\\right] \\nonumber \\\\\n",
    "&= \\log p_\\theta(x) - \\mathcal{L}_{\\theta, \\phi} (x) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From here we can do\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_\\theta(x) &= \\mathcal{L}_{\\theta, \\phi} (x) + D_\\text{KL}\\left[q_\\phi(z|x) || p_\\theta(z|x)\\right] \\nonumber \\\\\n",
    "&\\geq \\mathcal{L}_{\\theta, \\phi} (x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "because the KL divergence is non-negative\n",
    "\n",
    "The term\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\theta, \\phi} (x) = \\mathbb{E}_{z\\sim q_\\phi(z|x)} \\left [\\log p_\\theta(x|z)\\right ] + D_\\text{KL}\\left[q_\\phi(z|x) || p(z)\\right],\n",
    "$$\n",
    "\n",
    "is known as the *Evidence Lower Bound* (ELBO)\n",
    "\n",
    "\n",
    "El ELBO se puede obtener de forma alternativa aplicando la desigualdad de Jensen sobre\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(x) = \\log \\mathbb{E}_{z\\sim p(z)}[p_\\theta(x|z)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supuestos\n",
    "\n",
    "\n",
    "logverosimilitud, prior, posterior, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden1=100, n_hidden2=100, n_latent=2):\n",
    "        super(RedEjemplo, self).__init__()\n",
    "        # encoder layers\n",
    "        self.enc_hidden1 = torch.nn.Linear(28*28, n_hidden1)\n",
    "        self.enc_hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.enc_mu = torch.nn.Linear(n_hidden2, n_latent)\n",
    "        self.enc_logsigma = torch.nn.Linear(n_hidden2, n_latent)\n",
    "        # decoder layers\n",
    "        self.dec_hidden1 = torch.nn.Linear(n_latent, n_hidden2)\n",
    "        self.dec_hidden2 = torch.nn.Linear(n_hidden2, n_hidden1)\n",
    "        self.dec_hidden3 = torch.nn.Linear(n_hidden1, 28*28)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = x.reshape(-1, 1*28*28)\n",
    "        #print(x.shape)\n",
    "        z = self.activation(self.enc_hidden1(x))\n",
    "        z = self.activation(self.enc_hidden2(z))\n",
    "        return self.enc_mu(z), self.enc_logsigma(z)\n",
    "    \n",
    "    def sample(self, mu, logsigma):\n",
    "        return mu + torch.exp(logsigma)*torch.randn_like(logsigma)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.activation(self.dec_hidden1(z))\n",
    "        x = self.activation(self.dec_hidden2(x))\n",
    "        return (self.dec_hidden3(x)).reshape(-1, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logsigma = self.encode(x)\n",
    "        z = self.sample(mu, logsigma)\n",
    "        return self.decode(z), mu, logsigma    \n",
    "    \n",
    "Elogpxz = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "def KLqzxpz(mu, logsigma):\n",
    "    return 0.5*torch.sum(1 + 2*logsigma - torch.pow(mu, 2) - torch.exp(2*logsigma))\n",
    "\n",
    "negELBO = loss_function(xhat, x) - KLdiv(mu, logsigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "mnist_train_data = torchvision.datasets.MNIST('dataset', train=True, download=True,\n",
    "                                              transform=torchvision.transforms.ToTensor())\n",
    "mnist_test_data = torchvision.datasets.MNIST('dataset', train=False, download=True,\n",
    "                                             transform=torchvision.transforms.ToTensor())\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "np.random.seed(0)\n",
    "#idx = list(range(len(mnist_train_data)))\n",
    "idx = list(range(10000))\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.7*len(idx))\n",
    "\n",
    "train_loader = DataLoader(mnist_train_data, batch_size=128, \n",
    "                          sampler=SubsetRandomSampler(idx[:split]))\n",
    "\n",
    "valid_loader = DataLoader(mnist_train_data, batch_size=256, \n",
    "                          sampler=SubsetRandomSampler(idx[split:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo de Gradiente descedente con ADAM para entrenar\n",
    "mi_red_neuronal = RedEjemplo()\n",
    "optimizer = torch.optim.Adam(mi_red_neuronal.parameters(), lr=1e-3)\n",
    "\n",
    "history1 = hl.History()\n",
    "canvas1 = hl.Canvas()\n",
    "# GPU: Para entrenar en GPU trasladamos el modelo con\n",
    "#mi_red_neuronal = mi_red_neuronal.cuda()\n",
    "#for epoch in tqdm_notebook(range(10)):\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0.0\n",
    "    KL_loss = 0.0\n",
    "    for image, label in train_loader:\n",
    "        # GPU: También trasladamos las imágenes\n",
    "        #image = image.cuda()\n",
    "        #label = label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        prediction, mu, logsigma = mi_red_neuronal.forward(image)\n",
    "        rec, KLdiv = ELBO(image, prediction, mu, logsigma)\n",
    "        loss = rec + KLdiv\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += rec.item()  \n",
    "        KL_loss += KLdiv.item()  \n",
    "    den = train_loader.__len__()*train_loader.batch_size\n",
    "    history1.log(epoch, loss_train=epoch_loss/den, kl_train=KL_loss/den)\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    KL_loss = 0.0\n",
    "    for image, label in valid_loader:\n",
    "        prediction, mu, logsigma = mi_red_neuronal.forward(image)\n",
    "        rec, KLdiv = ELBO(image, prediction, mu, logsigma)\n",
    "        epoch_loss += rec.item()  \n",
    "        KL_loss += KLdiv.item()  \n",
    "    den = valid_loader.__len__()*valid_loader.batch_size\n",
    "    history1.log(epoch, loss_valid=epoch_loss/den, kl_valid=KL_loss/den)\n",
    "\n",
    "    with canvas1: # So that they render together\n",
    "        canvas1.draw_plot([history1[\"loss_train\"], history1[\"loss_valid\"]],\n",
    "                          labels=[\"Train loss\", \"Validation loss\"])\n",
    "        canvas1.draw_plot([history1[\"kl_train\"], history1[\"kl_valid\"]],\n",
    "                          labels=[\"Train KL\", \"Validation KL\"])\n",
    "        #canvas1.draw_plot([history1[\"loss_valid\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si entrenamos en GPU y queremos inferir en CPU recuperamos nuestra red con:\n",
    "#mi_red_neuronal = mi_red_neuronal.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 10, figsize=(12, 2.5), tight_layout=True)\n",
    "sig = torch.nn.Sigmoid()\n",
    "\n",
    "for i in range(10):\n",
    "    image, label = mnist_train_data[i]\n",
    "    ax[0, i].imshow(image[0, :, :].numpy())\n",
    "    ax[0, i].set_title(label);\n",
    "    ax[0, i].axis('off')\n",
    "    image_hat = sig(mi_red_neuronal.forward(image)[0]).detach().numpy()\n",
    "    ax[1, i].imshow(image_hat[0, 0, :, :])\n",
    "    ax[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = mnist_test_data.__len__()\n",
    "codes = np.zeros(shape=(N, 4))\n",
    "labels = np.zeros(shape=(N, ))\n",
    "for k in range(N):\n",
    "    labels[k] = mnist_test_data[k][1]\n",
    "    codes[k, :] = torch.cat(mi_red_neuronal.encode(mnist_test_data[k][0]), dim=1).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9), tight_layout=True)\n",
    "for k, color in enumerate(plt.cm.tab10.colors):\n",
    "    ax.scatter(codes[labels==k, 0], codes[labels==k, 1], \n",
    "               c=np.array(color).reshape(1, -1), s=10, alpha=0.5, label=str(k))\n",
    "    ax.errorbar(codes[labels==k, 0], codes[labels==k, 1], \n",
    "                np.exp(codes[labels==k, 2]), np.exp(codes[labels==k, 3]), fmt='none', alpha=0.5)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 30\n",
    "z_plot = np.linspace(-4, 4, num=M)\n",
    "big_imag = np.zeros(shape=(28*M, 28*M))\n",
    "sig = torch.nn.Sigmoid()\n",
    "for i in range(M):\n",
    "    for j in range(M):\n",
    "        z = torch.tensor(np.array([z_plot[j], z_plot[M-1-i]]), dtype=torch.float32)\n",
    "        xhat = sig(mi_red_neuronal.decode(z)).detach().numpy()\n",
    "        big_imag[i*28:(i+1)*28, j*28:(j+1)*28] = xhat\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9), tight_layout=True)\n",
    "Z_plot1, Z_plot2 = np.meshgrid(z_plot, z_plot)\n",
    "ax.matshow(big_imag, vmin=0.0, vmax=1.0, cmap=plt.cm.gray, extent=[-4, 4, -4, 4])\n",
    "H, xedge, yedge = np.histogram2d(codes[:, 0], codes[:, 1], bins=M, range=[[-4, 4], [-4, 4]])\n",
    "ax.contour(Z_plot1, Z_plot2, H.T, linewidths=3, levels=[5], cmap=plt.cm.Reds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/RuiShu/vae-experiments/blob/master/modality/README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
