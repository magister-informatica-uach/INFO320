{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most interesting models the evidence is intractable\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\mathcal{M}_i) = \\int p(\\mathcal{D}|\\mathcal{M}_i, \\theta) p(\\theta| \\mathcal{M}_i) d\\theta\n",
    "$$\n",
    "\n",
    "We resort to MCMC or approximate inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace method\n",
    "\n",
    "Propose a function of $\\theta$\n",
    "\n",
    "$$\n",
    "g(\\theta) = \\log p(\\mathcal{D}|\\mathcal{M}_i, \\theta) p(\\theta| \\mathcal{M}_i)\n",
    "$$\n",
    "\n",
    "We can do a second order Taylor expansion on $\\theta= \\hat \\theta_{\\text{map}}$\n",
    "\n",
    "$$\n",
    "g(\\theta) \\approx g(\\hat \\theta_{\\text{map}}) + (\\theta - \\hat \\theta_{\\text{map}})^T \\frac{dg}{d\\theta}(\\hat \\theta_{\\text{map}}) + \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\frac{d^2 g}{d\\theta^2} (\\hat \\theta_{\\text{map}}) (\\theta - \\hat \\theta_{\\text{map}})\n",
    "$$\n",
    "\n",
    "By definition the first derivative is zero, and calling $\\Sigma^{-1} = -\\frac{d^2 g}{d\\theta^2} (\\hat \\theta_{\\text{map}})$\n",
    "\n",
    "$$\n",
    "g(\\theta) \\approx g(\\hat \\theta_{\\text{map}}) -  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Sigma^{-1} (\\theta - \\hat \\theta_{\\text{map}})\n",
    "$$\n",
    "\n",
    "Plugging the approximation on the evidence\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\mathcal{M}_i) \\approx  e^{g(\\hat \\theta_{\\text{map}})} \\int e^{-  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Sigma^{-1} (\\theta - \\hat \\theta_{\\text{map}})} d\\theta \n",
    "$$\n",
    "\n",
    "The solution of the integral is normalizing constant of a Multivariate Gaussian with $K$ parameters\n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}|\\mathcal{M}_i) \\approx \\log p(\\mathcal{D}|\\mathcal{M}_i, \\hat \\theta_{\\text{map}}) + \\log p(\\hat \\theta_{\\text{map}}| \\mathcal{M}_i) + \\frac{K}{2} \\log(2\\pi) + \\frac{1}{2} \\log | \\Sigma |\n",
    "$$\n",
    "\n",
    "The evidence is approximate by the best likelihood fit plus the occam factor\n",
    "\n",
    "The occam factor depends on the\n",
    "- second derivative of the posterior (model uncertainty)\n",
    "- number of parameters (complexity)\n",
    "- and the prior pdf\n",
    "\n",
    "If the prior is very broad and $N$ is very large we recover the **Bayesian Information Criterion** (BIC) Proof?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference\n",
    "\n",
    "We want the posterior\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "We have the likelihood and the prior but the evidence is intractable\n",
    "\n",
    "Let's propose an approximate posterior\n",
    "\n",
    "$$\n",
    "q_\\eta(\\theta)\n",
    "$$\n",
    "\n",
    "This posterior can be tuned by changing the hyperparameter $\\eta$\n",
    "\n",
    "We turn this into an optimization problem: Find $\\eta$ that makes $q$ most similar to $p$\n",
    "\n",
    "We can write this as a KL divergence\n",
    "\n",
    "$$\n",
    "D_{KL}[q_\\eta(\\theta) || p(\\theta|\\mathcal{D})] = \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\theta|\\mathcal{D})} d\\theta\n",
    "$$\n",
    "\n",
    "We can use Bayes Theorem and move the evidence out from the integral\n",
    "\n",
    "$$\n",
    "D_{KL}[q_\\eta(\\theta) || p(\\theta|\\mathcal{D})] = \\log p(\\mathcal{D}) + \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta\n",
    "$$\n",
    "\n",
    "> If we are minimizing with respect to $\\eta$ we can ignore the evidence\n",
    "\n",
    "Also note that because the KL divergence is non-negative then\n",
    "$$\n",
    "\\log p(\\mathcal{D}) \\geq  \\mathcal{L}(\\eta) = - \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta\n",
    "$$\n",
    "\n",
    "hence $\\mathcal{L}(\\eta)$ is called the **Evidence Lower BOund** (ELBO)\n",
    "\n",
    "> Minimizing the KL is equivalent to maximizing the ELBO\n",
    "\n",
    "Let's work the ELBO \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\eta) &= - \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta \\nonumber \\\\\n",
    "&= - \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{ p (\\theta)} d\\theta + \\int q_\\eta(\\theta) \\log p(\\mathcal{D}|\\theta) d\\theta \\nonumber \\\\\n",
    "&= - D_{KL}[q_\\eta(\\theta) || p(\\theta)] + \\mathbb{E}_{\\theta \\sim q_\\eta(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\right]\\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Maximizing the ELBO is equivalent to:\n",
    "- Maximize the log likelihood when sampling from the approximate posterior: maximum likelihood\n",
    "- Minimize KL between approximate posterior and prior: regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
