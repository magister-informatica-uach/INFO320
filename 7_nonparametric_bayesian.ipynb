{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "import pyro\n",
    "print(pyro.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-parametric Bayesian Methods\n",
    "\n",
    "Before training a neural network we have to select the number of layers and the number of neurons per layer\n",
    "\n",
    "> The structure of the neural network is fixed\n",
    "\n",
    "In contrast non-parametric models have no fixed structure\n",
    "\n",
    "> Non-parametric models automatically infer a model size from the complexity of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process (GP)\n",
    "\n",
    "Consider our classical linear regression problem \n",
    "$$\n",
    "\\begin{align}\n",
    "Y &= f_\\theta(x) + \\epsilon \\nonumber \\\\\n",
    "&= \\Phi(x) \\theta + \\epsilon \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "where we want to find $\\theta$ that best fit $Y$\n",
    "\n",
    "We may consider a Gaussian prior for $\\theta$\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(0, I \\sigma_\\theta^2)\n",
    "$$\n",
    "hence the distribution of $f_\\theta$ is\n",
    "$$\n",
    "p(f) = \\mathcal{N}(0, K)\n",
    "$$\n",
    "\n",
    "because the expected value of $f_\\theta$ is zero and the covariance is\n",
    "$$\n",
    "\\text{cov}(f_\\theta) = \\mathbb{E}[ \\Phi(x)\\theta \\theta^T \\Phi(x)^T] = \\sigma_\\theta^2 \\Phi \\Phi^T = K\n",
    "$$\n",
    "\n",
    "> The prior over $f$ is called a Gaussian Process (GP)\n",
    "\n",
    "A GP is a distribution over functions\n",
    "\n",
    "The GP is parametrized by its covariance aka the kernel or gram matrix\n",
    "\n",
    "$$\n",
    "K = \\begin{pmatrix} \n",
    "\\kappa(x_1, x_1)& \\kappa(x_1, x_2)& \\ldots & \\kappa(x_1, x_N) \\\\\n",
    "\\kappa(x_2, x_1)& \\kappa(x_2, x_2)& \\ldots & \\kappa(x_2, x_N) \\\\\n",
    "\\vdots& \\vdots& \\ddots & \\vdots \\\\\n",
    "\\kappa(x_N, x_1)& \\kappa(x_N, x_2)& \\ldots & \\kappa(x_N, x_N) \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "which by definition is symmetric and positive definite\n",
    "\n",
    "> The kernel function $\\kappa$ controls the similarity between points\n",
    "\n",
    "We can encode different properties in the kernel (trends, locality, periodicity)\n",
    "\n",
    "The most broadly used kernel is the Gaussian kernel\n",
    "\n",
    "$$\n",
    "\\kappa(x, z) = \\sigma^2 \\exp \\left ( \\frac{\\|x - z \\|^2}{2\\ell^2} \\right)\n",
    "$$\n",
    "\n",
    "with parameter $\\sigma$ that controls the amplitude and $\\ell$ which controls the length-scale of the interactions\n",
    "\n",
    "> The linear regression has a fixed number of parameters $\\theta$ (and basis functions, the GP is the limit with infinite basis functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with GP\n",
    "\n",
    "If we assume that $\\epsilon$ is white Gaussian noise then we can get the distribution of $y$\n",
    "\n",
    "$$\n",
    "p(y) = \\int p(y|f) p(f) = \\mathcal{N}(0, K + I \\sigma_\\epsilon^2)\n",
    "$$\n",
    "\n",
    "Let's say we have $(x,y)$ training data points and we want to predict the output for $x^*$\n",
    "\n",
    "We can write\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}y\\\\f^*\\end{pmatrix} \\sim \\mathcal{N} \\begin{pmatrix} K_{xx}+I\\sigma_\\epsilon^2 & K_{xx*} \\\\ K_{xx*} & K_{x*x*} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Using a bit of algebra we can write the posterior\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(f^*|y) = \\mathcal{N}(&K_{xx*} (K_{xx}+I\\sigma_\\epsilon^2)^{-1} y, \\nonumber \\\\\n",
    "& K_{x*x*} - K_{xx*} (K_{xx}+I\\sigma_\\epsilon^2)^{-1} K_{xx*}^T ) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Which we can fit by maximizing the marginal log likelihood \n",
    "$$\n",
    "\\log p(y|x) = \\log \\int p(y | f, x) p(f) df\n",
    "$$ \n",
    "using for example gradient descent\n",
    "\n",
    "#### Details\n",
    "- The mean of the GP can be a function \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP with pyro\n",
    "\n",
    "Let's start by creating some synthetic data for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) #100x1\n",
    "x_test = np.linspace(-0.1, 1.1, num=200).astype('float32')\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32'))\n",
    "y_torch = torch.from_numpy(y.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [`pyro.contrib.gp`](http://docs.pyro.ai/en/stable/contrib.gp.html) to implement our first GP\n",
    "\n",
    "Let's start by creating a kernel from `gp.kernels`\n",
    "\n",
    "We will use a Radial Basis Function (RBF) aka Squared Exponential aka Gaussian kernel as our covariance\n",
    "\n",
    "We can specify the initial value of the variance and the lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.contrib.gp as gp\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1), \n",
    "                   lengthscale=torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this model looks before fitting the data? \n",
    "\n",
    "Let's inspect the prior $\\mathcal{N}(0, K)$ on the test data\n",
    "\n",
    "**Activity:** Increase/decrese the lengthscale and repeat, get a notion of its influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum a small value to the diagonal for numerical stability\n",
    "C = K.forward(torch.from_numpy(x_test)) + torch.eye(len(x_test))*1e-4\n",
    "# Then we sample from the a multivariate normal distribution\n",
    "samples = pyro.distributions.MultivariateNormal(torch.zeros(len(x_test)), \n",
    "                                                covariance_matrix=C).sample(sample_shape=(50,))\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(samples.shape[0]):\n",
    "    ax.plot(x_test, samples.detach().numpy()[i, :],\n",
    "            linestyle='-', c='tab:blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a model from `gp.models`\n",
    "\n",
    "> In this case we pick a model for regression `GPRegression`\n",
    "\n",
    "This model expects the train data, the kernel and the initial value of the noise variance\n",
    "\n",
    "Then we select an optimizer and a cost function\n",
    "\n",
    "> We will use Adam and the Trace_ELBO, respectively\n",
    "\n",
    "Training is very similar to how we train neural networks in pytorch\n",
    "\n",
    "> The only remarkable difference is that the cost function expects the `model` and `guide` from our GP\n",
    "\n",
    "To do predictions we use the forward attribute of our `GPRegression` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train GP and plot the results\n",
    "\n",
    "def train_gp_plots(model, x, y, x_test, nepochs=2000):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "    line_loss = ax[1].plot([], [])\n",
    "    ax[0].scatter(x, y)\n",
    "    epoch_loss = np.zeros(shape=(nepochs,))\n",
    "\n",
    "    for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model.model, model.guide)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss[k] = loss.item()\n",
    "        #break    \n",
    "        if k % 100 == 0:\n",
    "            ax[0].cla()\n",
    "            # Predictions at x_test\n",
    "            mu, cov = model.forward(x_test, full_cov=True, noiseless=False)\n",
    "            mu = mu.detach().numpy()\n",
    "            sd = cov.diag().sqrt().detach().numpy()        \n",
    "            ax[0].scatter(x, y, c='k')\n",
    "            ax[0].plot(x_test.detach(), mu)\n",
    "            ax[0].fill_between(x_test.detach(), mu-sd, mu+sd, alpha=0.5)\n",
    "            line_loss[0].set_xdata(range(k))\n",
    "            line_loss[0].set_ydata(epoch_loss[:k])\n",
    "            ax[1].relim()\n",
    "            ax[1].autoscale_view()\n",
    "            fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, # Training data\n",
    "                                   mean_function=None, # Mean\n",
    "                                   kernel=K, # Covarianze\n",
    "                                   jitter=1e-6, # Increase this if you have numerical problems \n",
    "                                   noise=torch.tensor(2.) # The variance of the white noise\n",
    "                                   )\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"RBF variance:\", gpr_model.kernel.variance.item())\n",
    "display(\"RBF length scale:\", gpr_model.kernel.lengthscale.item())\n",
    "display(\"Noise variance:\", gpr_model.noise.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum a small value to the diagonal for numerical stability\n",
    "mu, Sigma = gpr_model.forward(torch.from_numpy(x_test), full_cov=True, noiseless=True)\n",
    "Sigma += torch.eye(len(x_test))*1e-5\n",
    "# Then we sample from the a multivariate normal distribution\n",
    "samples = pyro.distributions.MultivariateNormal(mu, covariance_matrix=Sigma).sample(sample_shape=(50,))\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(samples.shape[0]):\n",
    "    ax.plot(x_test, samples.detach().numpy()[i, :], \n",
    "            linestyle='-', c='tab:blue', alpha=0.25)\n",
    "ax.scatter(x, y, c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different kernels\n",
    "\n",
    "Kernel are implemented in [pyro.contrib.gp.kernels](http://docs.pyro.ai/en/stable/contrib.gp.html#module-pyro.contrib.gp.kernels)\n",
    "\n",
    "Compare the RBF and Matern52 kernels. What differences do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.1))\n",
    "# K = gp.kernels.Matern52(input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "# Train and plot\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting priors for the parameters \n",
    "\n",
    "Before we did an MLE like estimation to find point estimates of the kernel parameters and the noise variance\n",
    "\n",
    "We can treat these parameters as random variables and set priors for them\n",
    "\n",
    "Training with these priors is equivalent to the MAP solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "from pyro.distributions import LogNormal\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model_prior = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "\n",
    "# Setting priors\n",
    "gpr_model_prior.kernel.lengthscale = pyro.nn.PyroSample(LogNormal(0.0, 1.0))\n",
    "gpr_model_prior.kernel.variance = pyro.nn.PyroSample(LogNormal(0.0, 1.0))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model_prior.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "# Train and plot    \n",
    "train_gp_plots(gpr_model_prior, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining kernels\n",
    "\n",
    "Summing or multiplying valid kernels yield a valid kernel\n",
    "\n",
    "> We can easily create new kernels from other kernels\n",
    "\n",
    "Take advantage of the different properties of kernels\n",
    "\n",
    "#### Example\n",
    "\n",
    "The following data has a periodic oscilation on a rising trend\n",
    "\n",
    "Try to fit it using `K1`, `K2`, `Ksum12` and `Kprod12`\n",
    "\n",
    "Can you explain in simple words your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100).astype('float32')*100\n",
    "y = (0.03*x + np.sin(0.1*x) + 0.1*np.random.randn(100)).astype('float32')\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "K1 = gp.kernels.Periodic(input_dim=1, variance=torch.tensor(1.), \n",
    "                        lengthscale=torch.tensor(10),\n",
    "                        period=torch.tensor(50))\n",
    "K2 = gp.kernels.Linear(input_dim=1, variance=torch.tensor(1.))\n",
    "Ksum12 = gp.kernels.Sum(K1, K2)\n",
    "Kprod12 = gp.kernels.Product(K1, K2)\n",
    "\n",
    "\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(torch.from_numpy(x), torch.from_numpy(y), \n",
    "                                   kernel=K1, noise=torch.tensor(2.))\n",
    "\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "train_gp_plots(gpr_model, x, y, torch.linspace(-50, 150, 100), nepochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Gaussian Processes\n",
    "\n",
    "Fitting a Gaussian process has cubic complexity\n",
    "\n",
    "Sparse Gaussian processes use a much smaller set of \"inducing points\" to compute the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=1000) #100x1\n",
    "x_test = np.linspace(-0.1, 1.1, num=200).astype('float32')\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32'))\n",
    "y_torch = torch.from_numpy(y.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model = gp.models.SparseGPRegression(x_torch, y_torch, approx='VFE',\n",
    "                                         kernel=K, Xu=torch.linspace(0, 1, 20),\n",
    "                                         noise=torch.tensor(2.), jitter=1e-5)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "train_gp_plots(gpr_model, x, y, torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task GP and Coregionalization\n",
    "\n",
    "**PENDING**\n",
    "\n",
    "In some cases we might need to perform regression on vector valued data (more than one feature)\n",
    "\n",
    "We might also expect some degree of correlation between the features\n",
    "\n",
    "We train a multi-task GP and learn the correlation between features with a [coregionalized kernel](http://docs.pyro.ai/en/stable/contrib.gp.html#pyro.contrib.gp.kernels.Coregionalize)\n",
    "\n",
    "A coregionalization kernel is defined as a matrix\n",
    "$$\n",
    "B = W W^T + I \\delta\n",
    "$$\n",
    "where $W$ models correlations between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_output1 = lambda x: 4. * np.cos(x/5.) - .4*x - 35. + np.random.rand(x.size) * 2.\n",
    "f_output2 = lambda x: 6. * np.cos(x/5.) + .2*x + 35. + np.random.rand(x.size) * 8.\n",
    "\n",
    "#{X,Y} training set for each output\n",
    "X1 = np.random.rand(100).astype('float32'); X1=X1*75\n",
    "X2 = np.random.rand(100).astype('float32'); X2=X2*70 + 30\n",
    "Y1 = f_output1(X1).astype('float32')\n",
    "Y2 = f_output2(X2).astype('float32')\n",
    "#{X,Y} test set for each output\n",
    "Xt1 = np.random.rand(100).astype('float32')*100\n",
    "Xt2 = np.random.rand(100).astype('float32')*100\n",
    "Yt1 = f_output1(Xt1).astype('float32')\n",
    "Yt2 = f_output2(Xt2).astype('float32')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(7, 4))\n",
    "ax[0].scatter(X1, Y1)\n",
    "ax[0].scatter(Xt1, Yt1)\n",
    "ax[1].scatter(X2, Y2)\n",
    "ax[1].scatter(Xt2, Yt2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "K1 = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.1))\n",
    "gpr_model1 = gp.models.GPRegression(torch.from_numpy(X1), torch.from_numpy(Y1), \n",
    "                                   kernel=K1, \n",
    "                                   noise=torch.tensor(2.))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(gpr_model1.parameters(), lr=1e-2)\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "train_gp_plots(gpr_model1, X1, Y1, torch.from_numpy(np.sort(Xt1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "K2 = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.1))\n",
    "gpr_model2 = gp.models.GPRegression(torch.from_numpy(X2), torch.from_numpy(Y2), \n",
    "                                   kernel=K2, \n",
    "                                   noise=torch.tensor(2.))\n",
    "\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "optimizer = torch.optim.Adam(gpr_model2.parameters(), lr=1e-2)\n",
    "train_gp_plots(gpr_model2, X2, Y2, torch.from_numpy(np.sort(Xt2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Gaussian Processes\n",
    "\n",
    "Stacks of Gaussian Processes\n",
    "\n",
    "Example in Pyro: https://fehiepsi.github.io/blog/deep-gaussian-process/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPyTorch\n",
    "\n",
    "https://gpytorch.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-study\n",
    "\n",
    "- Mackay chapter 45\n",
    "- Barber chapter 19\n",
    "- [Rasmussen & Willams, \"Gaussian Process for Machine Learning\"](http://gaussianprocess.org/gpml/?)\n",
    "- [Zhoubin Ghahramadi tutorial](http://mlg.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
