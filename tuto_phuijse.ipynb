{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "from tqdm.notebook import tqdm\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='int') # class labels\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0, 0.5, N) # radius\n",
    "    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2 # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = sklearn.datasets.make_moons(200, noise=0.2)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral_r, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network model in pytorch\n",
    "- class that inherits from `torch.nn.Module`\n",
    "- `__init__(self, args):` Define layers\n",
    "- `forward(self, x):` Define how layers are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet_classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=10):\n",
    "        super(NNet_classifier, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, num_hidden)\n",
    "        self.layer2 = torch.nn.Linear(num_hidden, num_hidden)\n",
    "        self.layer3 = torch.nn.Linear(num_hidden, 3)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        z = self.activation(self.layer1(x))\n",
    "        z = self.activation(self.layer2(z))\n",
    "        return self.layer3(z) #Neural net output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network training\n",
    "\n",
    "- `criterion`: Cost function to be minimized, *.e.g.* BCE for binary classification \n",
    "- `optimizer`: Optimization algorithm, typically based on stochastic gradient descent ($\\eta$ is the learning rate)\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "\n",
    "\n",
    "Training is performed by\n",
    "1. Evaluating the network using `forward`\n",
    "1. Calculating the error/loss selected in `criterion`\n",
    "1. Computing the derivatives of the error using the `backward` attribute of the error\n",
    "1. Updating parameters according to `optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, model):\n",
    "    ax[0].cla()\n",
    "    Z = model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "    Z = torch.nn.Softmax(dim=1)(Z).argmax(dim=1).detach().numpy().reshape(xx.shape[0], xx.shape[1])\n",
    "    #ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75, vmin=0, vmax=1)\n",
    "    ax[0].pcolormesh(xx, yy, Z, cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "    \n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    for ax_ in ax:\n",
    "        ax_.relim()\n",
    "        ax_.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNet_classifier(num_hidden=20)\n",
    "display(model)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(x, y, phase='train'):\n",
    "    haty = model.forward(x) # Evaluate the model\n",
    "    loss = criterion(haty, y) # Calculate errors\n",
    "    if phase == 'train':\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Compute derivaties\n",
    "        optimizer.step() # Update parameters \n",
    "    return loss.item()\n",
    "\n",
    "x_train = torch.from_numpy(X.astype('float32'))#.reshape(-1, 1)\n",
    "y_train = torch.from_numpy(y)#.reshape(-1, 1)\n",
    "epoch_loss = np.zeros(shape=(6000,)) \n",
    "\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = train_one_epoch(x_train, y_train)\n",
    "    if k % 100 == 0: \n",
    "        update_plot(k, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "class BayesianNNet_classifier(pyro.nn.PyroModule):\n",
    "    def __init__(self, num_hidden=10, prior_std=10.):\n",
    "        super().__init__()\n",
    "        prior = dist.Normal(0, prior_std)\n",
    "        self.layer1 = pyro.nn.PyroModule[torch.nn.Linear](2, num_hidden)\n",
    "        self.layer1.weight = pyro.nn.PyroSample(prior.expand([num_hidden, 2]).to_event(2))\n",
    "        self.layer1.bias = pyro.nn.PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer2 = pyro.nn.PyroModule[torch.nn.Linear](num_hidden, num_hidden)\n",
    "        self.layer2.weight = pyro.nn.PyroSample(prior.expand([num_hidden, num_hidden]).to_event(2))\n",
    "        self.layer2.bias = pyro.nn.PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer3 = pyro.nn.PyroModule[torch.nn.Linear](num_hidden, 3)\n",
    "        self.layer3.weight = pyro.nn.PyroSample(prior.expand([3, num_hidden]).to_event(2))\n",
    "        self.layer3.bias = pyro.nn.PyroSample(prior.expand([3]).to_event(1))        \n",
    "        \n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.activation(self.layer1(x))\n",
    "        h = self.activation(self.layer2(h))\n",
    "        p = self.layer3(h).squeeze(1)\n",
    "        with pyro.plate(\"data\", size=x.shape[0], dim=-1):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(logits=p), obs=y)\n",
    "            #obs = pyro.sample(\"obs\", dist.Bernoulli(logits=p), obs=y)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, samples):\n",
    "    ax[0].cla()\n",
    "    p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)\n",
    "    #Z = samples[\"obs\"].mean(0).reshape(xx.shape).detach().numpy()\n",
    "    Z = p.argmax(dim=1).reshape(xx.shape).detach().numpy()\n",
    "    #ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75, vmin=0, vmax=1)\n",
    "    ax[0].pcolormesh(xx, yy, Z, cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    \n",
    "\n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    for ax_ in ax:\n",
    "        ax_.relim()\n",
    "        ax_.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "model = BayesianNNet_classifier(num_hidden=20, prior_std=10.)\n",
    "print(pyro.poutine.trace(model).get_trace(x_train, y_train).format_shapes())\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-3}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = svi.step(x_train, y_train)\n",
    "    if k % 100 == 0:\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=10)\n",
    "        samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "        update_plot(k, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=500)\n",
    "samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(9, 2), tight_layout=True)\n",
    "\n",
    "for k in range(4):\n",
    "    zz = samples[\"obs\"][k].reshape(xx.shape).detach().numpy()\n",
    "    ax[k].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1)\n",
    "    #ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75, vmin=0, vmax=1)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[k].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "#zz = samples[\"obs\"].mean(0).reshape(xx.shape).detach().numpy()\n",
    "p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)\n",
    "zz = p.argmax(dim=1).reshape(xx.shape).detach().numpy()\n",
    "#ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75, vmin=0, vmax=1)\n",
    "ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "\n",
    "zz = -(p/500.*(p/500.+1e-32).log()).sum(dim=1).reshape(xx.shape).detach().numpy()\n",
    "#zz = samples[\"obs\"].std(0).reshape(xx.shape).detach().numpy()\n",
    "ax[1].contourf(xx, yy, zz, cmap=plt.cm.Blues, alpha=0.75, vmin=0.)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[1].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "mnist_test = torchvision.datasets.MNIST(root='~/datasets', train=False, transform=torchvision.transforms.ToTensor())\n",
    "mnist_loader = torch.utils.data.DataLoader(mnist_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "class BayesianNNet_classifier(pyro.nn.PyroModule):\n",
    "    def __init__(self, ninput=28*28, num_hidden=10):\n",
    "        super().__init__()\n",
    "        self.layer1 = pyro.nn.PyroModule[torch.nn.Linear](ninput, num_hidden)\n",
    "        self.layer1.weight = pyro.nn.PyroSample(dist.Normal(0., 1.).expand([num_hidden, ninput]).to_event(2))\n",
    "        self.layer1.bias = pyro.nn.PyroSample(dist.Normal(0., 1.).expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer2 = pyro.nn.PyroModule[torch.nn.Linear](num_hidden, 10)\n",
    "        self.layer2.weight = pyro.nn.PyroSample(dist.Normal(0., 1.).expand([10, num_hidden]).to_event(2))\n",
    "        self.layer2.bias = pyro.nn.PyroSample(dist.Normal(0., 1.).expand([10]).to_event(1))\n",
    "        \n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        p = self.layer2(self.activation(self.layer1(x))).squeeze(1)\n",
    "        with pyro.plate(\"data\", size=x.shape[0], dim=-1):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(logits=p), obs=y)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "model = BayesianNNet_classifier(num_hidden=100)\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "epoch_loss = np.zeros(shape=(100,))\n",
    "for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "    for images, labels in mnist_loader:\n",
    "        # calculate the loss and take a gradient step\n",
    "        epoch_loss[k] += svi.step(images.reshape(-1, 28*28), labels)\n",
    "    #break    \n",
    "    if k % 1 == 0:\n",
    "        ax[0].cla()\n",
    "        line2[0].set_xdata(range(k))\n",
    "        line2[0].set_ydata(epoch_loss[:k])\n",
    "        for ax_ in ax:\n",
    "            ax_.relim()\n",
    "            ax_.autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=100)\n",
    "samples = predictive(mnist_test.data.reshape(-1, 28*28)/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(5, 3))\n",
    "idx = 0\n",
    "def update(x):\n",
    "    global idx\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    ax[0].imshow(mnist_test.data[idx], cmap=plt.cm.Greys_r)\n",
    "    res = ax[1].hist(samples['obs'][:, idx], range=(0, 10))\n",
    "    H = np.sum(-res[0]*np.log(res[0]/100+1e-10)/100)\n",
    "    ax[1].set_title(\"%0.4f\" %(H))\n",
    "    ax[1].set_xticks(range(10));\n",
    "    idx+=1\n",
    "\n",
    "bnext = widgets.Button(description='next')\n",
    "bnext.on_click(update)\n",
    "bnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
