{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In linear regression we have a \n",
    "- continuous one-dimensional target $y$ \n",
    "- continuous D-dimensional input $x$ \n",
    "\n",
    "related by a linear mapping\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^D w_k x_k + b = f_\\theta(x)  \\rightarrow y\n",
    "$$\n",
    "\n",
    "> The model is specified by $\\theta=(b, w_1, w_2, \\ldots, w_D)$\n",
    "\n",
    "Typically, we fit this model by \n",
    "$$\n",
    "\\min_\\theta\\sum_i \\left(y_i - f_\\theta(x_i) \\right)^2\n",
    "$$\n",
    "\n",
    "whose solution is\n",
    "\n",
    "$$\n",
    "\\theta = (\\Phi^T \\Phi)^{-1} \\Phi^T Y,\n",
    "$$\n",
    "\n",
    "where $\\Phi  = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\ldots & x_{1D} \\\\ \n",
    "1 & x_{21} & x_{22} & \\ldots & x_{2D} \\\\\n",
    "1 & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N1} & x_{N2} & \\ldots & x_{ND} \\end{pmatrix}$,  $Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}$ and  $\\theta =  \\begin{pmatrix} b \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{pmatrix}$\n",
    "\n",
    "> This is known as the ordinary least squares (OLS) solution\n",
    "\n",
    "#### Note: \n",
    "\n",
    "Linear regression is **linear on the parameters**\n",
    "\n",
    "If we apply transformations we obtain the same solution. The only difference is in $\\Phi$\n",
    "\n",
    "For example\n",
    "- Polynomial basis regression $f_\\theta(x) = \\sum_j w_j x^j + b$ \n",
    "- Sine-wave basis regression $f_\\theta(x) = \\sum_j a_j \\cos(2\\pi j x)  + \\sum_j b_j \\sin(2\\pi j x) + c$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic linear regression\n",
    "\n",
    "We can assume that observations are noisy and write\n",
    "\n",
    "$$\n",
    "y = \\sum_j w_j x_j  + b + \\epsilon,\n",
    "$$\n",
    "\n",
    "If the noise is independent and Gaussian distributed then\n",
    "\n",
    "$$\n",
    "p(y|\\theta) = \\mathcal{N}\\left(\\sum_j w_j x_j + b, I\\sigma_\\epsilon^2 \\right)\n",
    "$$\n",
    "\n",
    "Additionally, we may want to discourage large values of $\\theta$ by placing a prior\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(0, \\sigma^2_\\theta)\n",
    "$$\n",
    "\n",
    "The priors gives us the space of possible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=100)\n",
    "\n",
    "sw, sb = 5., 5.\n",
    "plt.figure(figsize=(7, 3))\n",
    "for i in range(10):\n",
    "    W = sw*np.random.randn(1)\n",
    "    b = sb*np.random.randn(1)\n",
    "    plt.plot(x, W*x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constraint the space of solutions by presenting data\n",
    "\n",
    "### Point-estimate solution\n",
    "\n",
    "The Maximum a posteriori estimator of $\\theta$ is given by\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p(\\mathcal{D}| \\theta, \\sigma_\\epsilon^2) ~ p (\\theta) \\nonumber  \\\\\n",
    "&= \\text{arg}\\min_\\theta  \\frac{1}{2\\sigma_\\epsilon^2} (Y-\\Phi\\theta)^T (Y - \\Phi\\theta) + \\frac{1}{2\\sigma_\\theta^2} \\|\\theta\\|^2 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "and the result is\n",
    "$$\n",
    "\\hat \\theta = (\\Phi^T \\Phi + \\lambda I)^{-1} \\Phi^T Y\n",
    "$$\n",
    "where $\\lambda = \\sigma_\\epsilon^2 / \\sigma_\\theta^2$\n",
    "\n",
    "> This is the ridge regression or regularized least squares solution\n",
    "\n",
    "What happens if my prior totally uninformative ($\\sigma_\\theta \\to \\infty$) ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIAS VARIANCE TRADE OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural networks\n",
    "\n",
    "[Artificial neural networks](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit?usp=sharing) (ANN) are non-linear parametric function approximators built by connecting simple units\n",
    "\n",
    "These units are simplified models of biological neurons: \n",
    "\n",
    "> linear regressor followed by a non-linear activation function\n",
    "\n",
    "Feed-forward ANN are organized in layers. Each layer has a certain amount of neurons (user-defined)\n",
    "\n",
    "> **Multilayer perceptron (MLP) architecture:** Every unit is connected to all units of its previous and next layers\n",
    "\n",
    "Different ways of connecting neurons yields different ANN architectures (convolutional, recurrent, etc)\n",
    "\n",
    "The parameter vector $\\theta$ includes the weights and biases of all the neurons\n",
    "\n",
    "- Let's consider a Gaussian prior for $\\theta$ and study the space of possible models\n",
    "- How does it compare to the linear regressor? \n",
    "    - What happens when you add more neurons? \n",
    "    - What happens if you remove the nonlinearity?\n",
    "    - What happens when you add more layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=100)[:, None]\n",
    "\n",
    "def activation(z):\n",
    "    #return np.maximum(0., z) # ReLU\n",
    "    return 1.0/(1.0 + np.exp(-z)) # Logistic/Sigmoid\n",
    "\n",
    "sw, sb = 5., 5.\n",
    "Nh = 10\n",
    "plt.figure(figsize=(7, 3))\n",
    "for i in range(10):\n",
    "    W = sw*np.random.randn(1, Nh)\n",
    "    b = sb*np.random.randn(Nh)\n",
    "    z = np.dot(x, W) + b\n",
    "    z = activation(z)\n",
    "    #W = sw*np.random.randn(Nh, Nh)\n",
    "    #b = sb*np.random.randn(Nh)\n",
    "    #z = activation(np.dot(z, W) + b)\n",
    "    \n",
    "    W = sw*np.random.randn(Nh, 1)\n",
    "    b = sb*np.random.randn(1)\n",
    "    \n",
    "    plt.plot(x, np.dot(z, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch tutorial: https://github.com/magister-informatica-uach/INFO267/blob/master/unidad1/3_redes_neuronales.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic interpretation of ANN\n",
    "\n",
    "MLP for regression\n",
    "- No activation in output layer\n",
    "- Trained by minimizing the **Mean Square Error** as in Linear Regression\n",
    "\n",
    "MLP for classification\n",
    "- Sigmoid or softmax activation in output layer\n",
    "- Trained by minimizing the **Cross Entropy Error** as in Logistic Regression\n",
    "\n",
    "These cost functions arise from assuming a certain likelihood on the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
