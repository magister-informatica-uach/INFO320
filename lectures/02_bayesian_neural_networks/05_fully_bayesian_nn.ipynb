{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Networks\n",
    "\n",
    "> Deep Neural Networks are non-linear function approximators which represent the state of the art in pattern recognition\n",
    "\n",
    "But they do have limitations\n",
    "\n",
    "- Very deep models require lots of data and can be hard to train\n",
    "- Selecting an architecture requires a lot of experimentation\n",
    "- [Not too robust](https://openai.com/blog/adversarial-example-research/)\n",
    "- Poor at representing uncertainty\n",
    "\n",
    "> We can leverage some of these by going Bayesian\n",
    "\n",
    "- A Bayesian neural network (BNN) places a prior distribution on its parameters \n",
    "- Training the BNN $\\equiv$ Learning the posterior distribution of the parameters given the data\n",
    "- The **uncertainty on the data and the parameters** can be propagated to estimate the **uncertainty on our predictions**\n",
    "    - Uncertainty on the data is called **aleatoric uncertainty** and it is related to irreducible noise\n",
    "    - Uncertainty on the model (parameters and structure) is called **epistemic uncertainty**\n",
    "\n",
    "> We know what we don't know\n",
    "\n",
    "We can use this \"new knowledge\" to\n",
    "- Decide when to use a more simple/complex model (complexity-control)\n",
    "- Decide when to take a critical decisions\n",
    "    - Autonomouse cars\n",
    "    - Cancer diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of history\n",
    "\n",
    "> Bayesian neural networks is an active area of research \n",
    "\n",
    "- 1980's: Bayes theorem applied to Neural Networks (John Hopfield and Naftali Tishby)\n",
    "- 1990's: Monte-Carlo and VI for bayesian neural networks was studied extensively by [David Mackay](http://www.inference.org.uk/mackay/BayesNets.html) and [Radford Neal](https://www.cs.toronto.edu/~radford/res-neural.html) (Also Bishop, Barber, Hinton, Gharamani and many others). Neal shows that Gaussian process are bayesian neural networks with infinite neurons\n",
    "\n",
    "> The models remain quite difficult to train for some time\n",
    "\n",
    "- 2010's: Deep learning arrives \n",
    "- 2011: [Alex Graves' VI for neural networks](https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks)\n",
    "- Explosion of practical deep bayesian networks \n",
    "    - [Charles Blundell's Bayes by backprop](https://arxiv.org/abs/1505.05424)\n",
    "    - [Yarin Gal's many work](http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf)\n",
    "    - Durk Kingma, Danilo Jimenez Rezende, Shakir Mohamed, José Miguel Hernandez-Lobato\n",
    "- [Hot topic now a days](http://bayesiandeeplearning.org/)\n",
    "\n",
    "History in video by [Zoubin Gharamani](http://mlg.eng.cam.ac.uk/zoubin/) at [NIPS 2016](https://www.youtube.com/watch?v=FD8l2vPU5FY) and [interesting panel discussion](https://www.youtube.com/watch?v=HumFmLu3CJ8) on the same conference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More formally\n",
    "\n",
    "\n",
    "Assuming\n",
    "- $N$ *iid* samples $\\mathcal{D} =\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}$ \n",
    "- $x$ is a $D$ dimensional vector\n",
    "- Fully-connected neural network with one hidden layer ($H$ neurons) for regression\n",
    "- $\\text{sgm}(\\cdot)$ is a non-linear activation function\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\theta(x) &=   b_i + \\sum_{j=1}^H w_{ij} h_j  \\nonumber \\\\\n",
    "&=  b_i + \\sum_{j=1}^H w_{ij} \\text{sgm} \\left( b_j + \\sum_{d=1}^D w_{jd} x_d  \\right) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The parameter vector $\\theta$ contains all the weights and biases of the model\n",
    "\n",
    "**Prior:** We propose a prior for $\\theta$, typically\n",
    "\n",
    "$$\n",
    "\\theta \\sim \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "**Likelihood:** We propose a likelihood depending on our task, typically Gaussian for regression and Bernoulli/Categorical for binary/multiclass classification \n",
    "\n",
    "**Posterior:** We use Bayes theorem to write the posterior\n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})} = \\frac{1}{{p(\\mathcal{D})}} \\prod_n \\mathcal{N}(y^{(n)} | f(x^{(n)}), \\sigma^2) \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "Even though the likelihood and prior are normal **the posterior in this case is not normal** because of the nested nonlinearity (Can you show this?)\n",
    "\n",
    "In general:\n",
    "\n",
    "> We cannot obtain an analytical posterior for a bayesian neural network\n",
    "\n",
    "We resort to sampling-based (MCMC) or deterministic (VI) approximate inference (previous class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My first Bayesian Neural Network using Pyro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same synthetic data from our previous class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) \n",
    "x_test = np.linspace(-0.05, 1.05, num=200)\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32')).unsqueeze(1)\n",
    "x_test = torch.from_numpy(x_test.astype('float32')).unsqueeze(1)\n",
    "y_torch = torch.from_numpy(y.astype('float32')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding the bayesian neural net\n",
    "\n",
    "Neural nets in pyro are classes that inherit from `PyroModule`\n",
    "\n",
    "This is done using `PyroModule` and `PyroSample` from [`pyro.nn`](https://docs.pyro.ai/en/stable/nn.html)\n",
    "\n",
    "- `PyroSample` is used to declare a neural net parameter as a random variable \n",
    "- `PyroModule` is used to declare torch modules which accept random parameters\n",
    "\n",
    "In the following example we lift `torch.nn.Linear` using `PyroModule`, and add priors its parameters using `PyroSample`\n",
    "\n",
    "In this regression problem we assume that the output is Gaussian distributed\n",
    "\n",
    "The likelihood is declared with its corresponding plate in the `forward` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network is coded we can use `pyro.poutine.trace` with pyro validation activated to make sure that the shapes are correct\n",
    "\n",
    "- Batch dimension is 15 (number of samples)\n",
    "- Event dimension is equal to the number of neurons for each layer\n",
    "\n",
    "Independent RV (likelihood) should be in the left while dependent (weights and biases) should be on the right\n",
    "\n",
    "This is controlled using plates and the `to_event()` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.distributions import Uniform, Normal, MixtureOfDiagNormals\n",
    "\n",
    "class BayesianMLPRegression(PyroModule):\n",
    "    def __init__(self, n_hidden=10, prior_scale=1.):\n",
    "        super().__init__()\n",
    "        prior = Normal(0, prior_scale)\n",
    "        # Hidden layer\n",
    "        self.hidden = PyroModule[torch.nn.Linear](1, n_hidden)\n",
    "        self.hidden.weight = PyroSample(prior.expand([n_hidden, 1]).to_event(2))\n",
    "        self.hidden.bias = PyroSample(prior.expand([n_hidden]).to_event(1))\n",
    "        # Output layer\n",
    "        self.output = PyroModule[torch.nn.Linear](n_hidden, 1)\n",
    "        self.output.weight = PyroSample(prior.expand([1, n_hidden]).to_event(2))\n",
    "        self.output.bias = PyroSample(prior.expand([1]).to_event(1))\n",
    "        # activation function\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        z = self.activation(self.hidden(x))\n",
    "        mean = self.output(z).squeeze(-1)\n",
    "        sigma = pyro.sample(\"sigma\", Uniform(0.0, 0.1))\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", Normal(mean, sigma), obs=y) #likelihood\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "\n",
    "model = BayesianMLPRegression()\n",
    "\n",
    "print(pyro.poutine.trace(model).get_trace(x_torch, y_torch).format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the BNN: MCMC \n",
    "\n",
    "Even for a extremely simple NN and using the most advanced samplers MCMC can be inpractical\n",
    "\n",
    "(Don't try to wait for this to converge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "pyro.clear_param_store() \n",
    "model = BayesianMLPRegression(n_hidden=10, prior_scale=1.) # Declare the neural network\n",
    "\n",
    "nuts_kernel = NUTS(model, adapt_step_size=True)\n",
    "sampler = MCMC(nuts_kernel, num_chains=1, num_samples=10000, warmup_steps=1000)\n",
    "sampler.run(x_torch, y_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of VI\n",
    "\n",
    "Propose an approximate (simple) posterior $q_\\nu(\\theta)$ and optimize so that it looks similar to the actual posterior\n",
    "\n",
    "We do this by maximizing a lower bound on the evidence\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\nu) = \\mathbb{E}_{q_\\nu(\\theta)}[ \\log p(\\mathcal{D}|\\theta)] - \\text{KL}[q_\\nu(\\theta)|p(\\theta)]\n",
    "$$\n",
    "\n",
    "An we use $q_\\nu(\\theta)$ as our replacement for $p(\\theta|\\mathcal{D})$ to calculate the **posterior predictive distribution**\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{x}, \\mathcal{D}) = \\int p(\\mathbf{y}|\\mathbf{x}, \\theta) p(\\theta| \\mathcal{D}) \\,d\\theta\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the BNN: VI\n",
    "\n",
    "Once the model is specified we need to write a guide (approximate posterior)\n",
    "\n",
    "This can be done manually as before or using the automatic guides in `pyro.infer.autoguide`. Typically we would start we the simplest diagonal normal guide (assumes no correlation between the parameters of the BNN)\n",
    "\n",
    "Then we create an SVI object and call the `step` attribute of this object iteratively\n",
    "\n",
    "We can evaluate the posteriors of the parameters and the predictive posterior using `pyro.infer.Predictive`\n",
    "\n",
    "In what follows the neural network is trained for 10000 epochs and every 100 epochs the predictive posterior is plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3), tight_layout=True, dpi=80)\n",
    "    #ax[0].set_yscale('log')\n",
    "\n",
    "    def update_plot(k, epoch_loss, samples):\n",
    "        ax[0].cla()\n",
    "        ax[0].plot(range(k), epoch_loss[:k])\n",
    "        #ax[0].autoscale_view()\n",
    "        ax[1].cla()\n",
    "        ax[1].plot(x, y, 'k.');\n",
    "        med = np.median(samples, axis=[0])\n",
    "        qua = np.quantile(samples, (0.05, 0.95), axis=0)\n",
    "        ax[1].plot(x_test.numpy()[:, 0], med)\n",
    "        ax[1].fill_between(x_test.numpy()[:, 0], qua[0], qua[1], alpha=0.5)\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) # Turn this on for additional debugging\n",
    "pyro.clear_param_store() \n",
    "model = BayesianMLPRegression(n_hidden=10, prior_scale=1.) # Declare the neural network\n",
    "\n",
    "# Create a guide\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-2)\n",
    "\n",
    "# Create SVI object\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2, 'clip_norm': 10.0}), # Optimizer\n",
    "                     loss=pyro.infer.Trace_ELBO()) # Loss function \n",
    "\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    loss = svi.step(x=x_torch, y=y_torch.squeeze(-1)) # Actual training step\n",
    "    epoch_loss[k] = loss / len(x_torch)\n",
    "        \n",
    "    if k % 100 == 0:\n",
    "        # Compute predictive posterior\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=100)\n",
    "        samples = predictive(x_test, None)['obs'].detach().numpy()\n",
    "        # Plot it\n",
    "        update_plot(k, epoch_loss, samples)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is complete we can use the guide as our replacement to the posterior\n",
    "\n",
    "The trained pararemeters of the guide are stored in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can use `pyro.infer.Predictive` to get samples from our bayesian neural network when evaluated on new inputs \n",
    "\n",
    "Here we sample \"100 neural networks\" and evaluate them on `x_test` \n",
    "\n",
    "This returns the sampled parameters (weights and biases) and outputs (obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, guide=guide, num_samples=100)\n",
    "for k, v in predictive(x_test, None).items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian network for multi-class classification with Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create synthetic 2D data with 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='int') # class labels\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0, 0.5, N) # radius\n",
    "    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2 # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j\n",
    "\n",
    "#X, y = sklearn.datasets.make_moons(200, noise=0.2)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code a Bayesian Neural Network with two hidden layers and normal prior in all activations\n",
    "\n",
    "For the likelihood we use the Categorical (Multinomial with $n=1$). The categorical distribution expects unnormalized probabilities (logits) as input, in this case the un-activated output of the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.distributions import Normal, Categorical\n",
    "\n",
    "class BayesianMLPClassifier(PyroModule):\n",
    "    def __init__(self, num_hidden=10, prior_std=1.):\n",
    "        super().__init__()\n",
    "        prior = Normal(0, prior_std)\n",
    "        self.layer1 = PyroModule[torch.nn.Linear](2, num_hidden)\n",
    "        self.layer1.weight = PyroSample(prior.expand([num_hidden, 2]).to_event(2))\n",
    "        self.layer1.bias = PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer2 = PyroModule[torch.nn.Linear](num_hidden, num_hidden)\n",
    "        self.layer2.weight = PyroSample(prior.expand([num_hidden, num_hidden]).to_event(2))\n",
    "        self.layer2.bias = PyroSample(prior.expand([num_hidden]).to_event(1))\n",
    "        \n",
    "        self.layer3 = PyroModule[torch.nn.Linear](num_hidden, 3)\n",
    "        self.layer3.weight = PyroSample(prior.expand([3, num_hidden]).to_event(2))\n",
    "        self.layer3.bias = PyroSample(prior.expand([3]).to_event(1))        \n",
    "        \n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.activation(self.layer1(x))\n",
    "        h = self.activation(self.layer2(h))\n",
    "        p = self.layer3(h).squeeze(1)\n",
    "        with pyro.plate(\"data\", size=x.shape[0], dim=-1):\n",
    "            obs = pyro.sample(\"obs\", Categorical(logits=p), obs=y) # Multiclass\n",
    "            #obs = pyro.sample(\"obs\", dist.Bernoulli(logits=p), obs=y) # Binary\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use an automatic diagonal normal guide (no covariance) and train using `Trace_ELBO`\n",
    "\n",
    "We plot the mean of the predictive posterior every 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, samples):\n",
    "    ax[0].cla()\n",
    "    p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)\n",
    "    zz = p.argmax(dim=1).reshape(xx.shape).detach().numpy()\n",
    "    ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    \n",
    "\n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    for ax_ in ax:\n",
    "        ax_.relim()\n",
    "        ax_.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(X.astype('float32'))\n",
    "y_train = torch.from_numpy(y)\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "model = BayesianMLPClassifier(num_hidden=100, prior_std=10.)\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-1)\n",
    "\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = svi.step(x_train, y_train)\n",
    "    if k % 100 == 0:\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=10)\n",
    "        samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "        update_plot(k, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample 100 neural networks and plot four individual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=100)\n",
    "samples = predictive(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(9, 2), tight_layout=True)\n",
    "\n",
    "for k in range(4):\n",
    "    zz = samples[\"obs\"][k].reshape(xx.shape).detach().numpy()\n",
    "    ax[k].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[k].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these categorical samples we can compute statistics\n",
    "\n",
    "In the left we plot the mode (more repeated class) and in the right the entropy. \n",
    "\n",
    "The higher then entropy the more different the output of the neural networks (high uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "zz = torch.mode(samples[\"obs\"], dim=0)[0].reshape(xx.shape).detach().numpy()\n",
    "ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "\n",
    "p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=3).sum(dim=0)/100.\n",
    "entropy = lambda p: -(p*(p+1e-32).log()).sum(dim=1)\n",
    "\n",
    "zz = entropy(p).reshape(xx.shape).detach().numpy()\n",
    "cf = ax[1].contourf(xx, yy, zz, cmap=plt.cm.Blues, alpha=0.75)\n",
    "fig.colorbar(cf, ax=ax[1])\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[1].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result using a non-bayesian neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(torch.nn.Module):    \n",
    "    def __init__(self, num_hidden=10):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, num_hidden) \n",
    "        self.layer2 = torch.nn.Linear(num_hidden, num_hidden)\n",
    "        self.layer3 = torch.nn.Linear(num_hidden, 3)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        z = self.activation(self.layer1(x))\n",
    "        z = self.activation(self.layer2(z))\n",
    "        return self.layer3(z)     \n",
    "    \n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line2 = ax[1].plot([], [])\n",
    "\n",
    "def update_plot(k, model):\n",
    "    ax[0].cla()\n",
    "    Z = model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "    zz = torch.nn.Softmax(dim=1)(Z).argmax(dim=1).detach().numpy().reshape(xx.shape[0], xx.shape[1])\n",
    "    ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "    for i, m in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "    \n",
    "    line2[0].set_xdata(range(k))\n",
    "    line2[0].set_ydata(epoch_loss[:k])\n",
    "    for ax_ in ax:\n",
    "        ax_.relim()\n",
    "        ax_.autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(num_hidden=100)\n",
    "display(model)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(x, y, phase='train'):\n",
    "    haty = model.forward(x) # Evaluate the model\n",
    "    loss = criterion(haty, y) # Calculate errors\n",
    "    if phase == 'train':\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Compute derivatives\n",
    "        optimizer.step() # Update parameters \n",
    "    return loss.item()\n",
    "\n",
    "x_train = torch.from_numpy(X.astype('float32'))#.reshape(-1, 1)\n",
    "y_train = torch.from_numpy(y)#.reshape(-1, 1)\n",
    "epoch_loss = np.zeros(shape=(3000,)) \n",
    "\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    epoch_loss[k] = train_one_epoch(x_train, y_train)\n",
    "    if k % 100 == 0: \n",
    "        update_plot(k, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the softmax output as probabilities we can also compute its entropy\n",
    "\n",
    "Is it the same as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "Z = torch.nn.Softmax(dim=1)(model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32'))))\n",
    "zz = Z.argmax(dim=1).detach().numpy().reshape(xx.shape[0], xx.shape[1])\n",
    "ax[0].pcolormesh(xx, yy, zz, cmap=plt.cm.Set1, alpha=0.75)\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[0].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)\n",
    "    \n",
    "zz = -(Z*(Z+1e-32).log()).sum(dim=1).reshape(xx.shape).detach().numpy()\n",
    "cf = ax[1].contourf(xx, yy, zz, cmap=plt.cm.Blues, alpha=0.75, vmin=0., vmax=np.log(3))\n",
    "fig.colorbar(cf, ax=ax[1])\n",
    "for i, m in enumerate(['o', 'x', 'd']):\n",
    "    ax[1].scatter(X[y==i, 0], X[y==i, 1], c='k', marker=m, s=20, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a known phenomenon in deep neural networks: **Miscalibration**\n",
    "\n",
    "The uncertainty of the predictions is very low even when far from the data\n",
    "\n",
    "> The network is being **over-confident**\n",
    "\n",
    "\"after (almost) all training samples are correctly classified, crossentropy (neg log likelihood) can be further minimized by increasing the confidence of the predictions (reducing entropy of softmax output)\"\n",
    "\n",
    "The uncertainty obtained from model averaging (bayesian) and the one derived from the softmax output should not be confused\n",
    "\n",
    "\n",
    "References:\n",
    "- [On Calibration of Modern Neural Networks](https://arxiv.org/pdf/1706.04599.pdf)\n",
    "- [Being Bayesian, Even Just a Bit,Fixes Overconfidence in ReLU Networks](https://arxiv.org/pdf/2002.10118v1.pdf)\n",
    "- [Evidential Deep Learning to Quantify Classification Uncertainty](https://arxiv.org/pdf/1806.01768.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional bayesian network for MNIST with Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST handwritten digits dataset\n",
    "\n",
    "In this example we will use only the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "mnist_test = torchvision.datasets.MNIST(root='~/datasets', train=False, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple fully-connected neural network \n",
    "\n",
    "Can you extend this to a convolutional neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.distributions import Normal, Categorical\n",
    "\n",
    "class CBNN(PyroModule):\n",
    "    def __init__(self, prior_std=1.):\n",
    "        super().__init__()\n",
    "        prior = Normal(0, prior_std)\n",
    "        \n",
    "        self.conv1 = PyroModule[torch.nn.Conv2d](1, 16, 3, 2) # input filters, output filters, kernel size, stride\n",
    "        self.conv1.weight = PyroSample(prior.expand([16, 1, 3, 3]).to_event(4))\n",
    "        self.conv1.bias = PyroSample(prior.expand([16]).to_event(1))\n",
    "        \n",
    "        self.conv2 = PyroModule[torch.nn.Conv2d](16, 16, 3, 2)\n",
    "        self.conv2.weight = PyroSample(prior.expand([16, 16, 3, 3]).to_event(4))\n",
    "        self.conv2.bias = PyroSample(prior.expand([16]).to_event(1))\n",
    "        \n",
    "        self.fc1 = PyroModule[torch.nn.Linear](16*6*6, 100)\n",
    "        self.fc1.weight = PyroSample(prior.expand([100, 16*6*6]).to_event(2))\n",
    "        self.fc1.bias = PyroSample(prior.expand([100]).to_event(1))\n",
    "        \n",
    "        self.fc2 = PyroModule[torch.nn.Linear](100, 10)\n",
    "        self.fc2.weight = PyroSample(prior.expand([10, 100]).to_event(2))\n",
    "        self.fc2.bias = PyroSample(prior.expand([10]).to_event(1))\n",
    "        \n",
    "        self.activation = torch.nn.Softplus()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.activation(self.conv1(x))\n",
    "        h = self.activation(self.conv2(h))\n",
    "        h = h.reshape(-1, 16*6*6)\n",
    "        h = self.activation(self.fc1(h))\n",
    "        p = self.fc2(h).squeeze(1)\n",
    "        with pyro.plate(\"data\", size=x.shape[0], dim=-1):\n",
    "            obs = pyro.sample(\"obs\", Categorical(logits=p), obs=y)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with diagonal normal guide and `Trace_ELBO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(7, 3), tight_layout=True)\n",
    "line = ax.plot([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "model = CBNN(prior_std=10.)\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-1)\n",
    "\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "mnist_loader = torch.utils.data.DataLoader(mnist_test, batch_size=128, shuffle=True)\n",
    "epoch_loss = np.zeros(shape=(100,))\n",
    "\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    for images, labels in mnist_loader:\n",
    "        # calculate the loss and take a gradient step\n",
    "        epoch_loss[k] += svi.step(images, labels)\n",
    "    #break    \n",
    "    if k % 1 == 0:\n",
    "        line[0].set_xdata(range(k))\n",
    "        line[0].set_ydata(epoch_loss[:k])\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mode and entropy from the predictive samples\n",
    "\n",
    "With this we can explore the digits for which the model is most uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=100)\n",
    "samples = predictive(mnist_test.data.unsqueeze(1)/255.)\n",
    "p = torch.nn.functional.one_hot(samples[\"obs\"], num_classes=10).sum(dim=0)/100.\n",
    "mode = p.argmax(dim=1)\n",
    "entropy = -(p*(p+1e-32).log()).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=mnist_test.targets.detach().numpy(),\n",
    "                            y_pred=mode.detach().numpy(),\n",
    "                            digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 2.5), tight_layout=True)\n",
    "\n",
    "digit = 4\n",
    "mask = mnist_test.targets == digit\n",
    "idx = np.argsort(entropy[mask].numpy())[::-1]\n",
    "k = 0\n",
    "def update(x):\n",
    "    global k\n",
    "    for ax_ in ax:\n",
    "        ax_.cla()\n",
    "    ax[0].imshow(mnist_test.data[mask][idx[k]], cmap=plt.cm.Greys_r)\n",
    "    res = ax[1].hist(samples['obs'][:, mask][:, idx[k]], range=(0, 10))\n",
    "    ax[1].set_title(\"%d %0.4f\" %(mode[mask][idx[k]], entropy[mask][idx[k]]))\n",
    "    ax[1].set_xticks(range(10));\n",
    "    k+=1\n",
    "\n",
    "bnext = widgets.Button(description='next')\n",
    "bnext.on_click(update)\n",
    "bnext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on Bayesian Neural Networks Training\n",
    "\n",
    "- State of the art!\n",
    "- Very delicate: bad initializations and local minima \n",
    "- Set appropriate priors\n",
    "- Variance control and reparameterization (more on this next class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
