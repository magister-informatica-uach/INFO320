{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "import pyro\n",
    "print(pyro.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-parametric Bayesian Methods\n",
    "\n",
    "Before training a neural network we select the number of layers and the number of neurons per layer\n",
    "\n",
    "> The structure of the neural network is fixed\n",
    "\n",
    "In contrast non-parametric models have no fixed structure\n",
    "\n",
    "> Non-parametric models automatically infer a model size from the complexity of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process (GP)\n",
    "\n",
    "A GP is a distribution over functions $f: \\mathcal{X} \\rightarrow \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP with pyro\n",
    "\n",
    "Let's start by creating some synthetic data for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) #100x1\n",
    "x_test = np.linspace(-0.1, 1.1, num=200)\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32'))\n",
    "x_test = torch.from_numpy(x_test.astype('float32'))\n",
    "y_torch = torch.from_numpy(y.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `pyro.contrib.gp` to implement our first GP\n",
    "\n",
    "Let's start by creating a kernel from `gp.kernels`\n",
    "\n",
    "We will use a Radial Basis Function (RBF) aka Squared Exponential aka Gaussian kernel as our covariance\n",
    "\n",
    "We can specify the initial value of the variance and the lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.contrib.gp as gp\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1), \n",
    "                   lengthscale=torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this model looks before fitting the data? \n",
    "\n",
    "Let's inspect the prior $\\mathcal{N}(0, K)$ on the test data\n",
    "\n",
    "**Activity:** Increase/decrese the lengthscale and repeat, get a notion of its influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum a small value to the diagonal for numerical stability\n",
    "C = K.forward(x_test) + torch.eye(len(x_test))*1e-4\n",
    "# Then we sample from the a multivariate normal distribution\n",
    "samples = pyro.distributions.MultivariateNormal(torch.zeros(len(x_test)), \n",
    "                                                covariance_matrix=C).sample(sample_shape=(50,))\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(samples.shape[0]):\n",
    "    ax.plot(x_test.detach().numpy(), samples.detach().numpy()[i, :],\n",
    "            linestyle='-', c='tab:blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a model from `gp.models`\n",
    "\n",
    "> In this case we pick a model for regression `GPRegression`\n",
    "\n",
    "This model expects the train data, the kernel and the initial value of the noise variance\n",
    "\n",
    "Then we select an optimizer and a cost function\n",
    "\n",
    "> We will use Adam and the Trace_ELBO, respectively\n",
    "\n",
    "The training is very similar to how we train neural networks in pytorch\n",
    "\n",
    "> The only remarkable difference is that the cost function expects the `model` and `guide` from our GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#Kernel\n",
    "K = gp.kernels.RBF(input_dim=1, \n",
    "                   variance=torch.tensor(1.0), \n",
    "                   lengthscale=torch.tensor(0.1))\n",
    "# Model\n",
    "gpr_model = gp.models.GPRegression(x_torch, y_torch, \n",
    "                                   kernel=K, \n",
    "                                   noise=torch.tensor(2.))\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gpr_model.parameters(), lr=1e-2)\n",
    "# Criterion\n",
    "criterion = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "# We train the model and visualize the results on the fly\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True)\n",
    "line_loss = ax[1].plot([], [])\n",
    "ax[0].scatter(x, y)\n",
    "epoch_loss = np.zeros(shape=(2000,))\n",
    "\n",
    "for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(gpr_model.model, gpr_model.guide)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss[k] = loss.item()\n",
    "    #break    \n",
    "    if k % 100 == 0:\n",
    "        ax[0].cla()\n",
    "        mu, cov = gpr_model.forward(x_test, full_cov=True, noiseless=False)\n",
    "        mu = mu.detach().numpy()\n",
    "        sd = cov.diag().sqrt().detach().numpy()        \n",
    "        ax[0].scatter(x, y, c='k')\n",
    "        ax[0].plot(x_test.detach(), mu)\n",
    "        ax[0].fill_between(x_test.detach(), mu-sd, mu+sd, alpha=0.5)\n",
    "        line_loss[0].set_xdata(range(k))\n",
    "        line_loss[0].set_ydata(epoch_loss[:k])\n",
    "        ax[1].relim()\n",
    "        ax[1].autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"RBF variance:\", gpr_model.kernel.variance.item())\n",
    "display(\"RBF length scale:\", gpr_model.kernel.lengthscale.item())\n",
    "display(\"Noise variance:\", gpr_model.noise.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum a small value to the diagonal for numerical stability\n",
    "mu, Sigma = gpr_model.forward(x_test, full_cov=True, noiseless=True)\n",
    "Sigma += torch.eye(len(x_test))*1e-5\n",
    "# Then we sample from the a multivariate normal distribution\n",
    "samples = pyro.distributions.MultivariateNormal(mu, covariance_matrix=Sigma).sample(sample_shape=(50,))\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "for i in range(samples.shape[0]):\n",
    "    ax.plot(x_test.detach().numpy(), samples.detach().numpy()[i, :], \n",
    "            linestyle='-', c='tab:blue', alpha=0.25)\n",
    "ax.scatter(x, y, c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-study\n",
    "\n",
    "- Mackay chapter 45\n",
    "- Barber chapter 19\n",
    "- [Zhoubin Ghahramadi tutorial](http://mlg.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pyro.ai/examples/gp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link dump \n",
    "\n",
    "https://www.inference.vc/maximum-likelihood-for-representation-learning-2/\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/9g1rxs/d_how_is_the_log_marginal_likelihood_of/\n",
    "\n",
    "https://colinraffel.com/blog/gans-and-divergence-minimization.html\n",
    "\n",
    "https://www.inference.vc/maximum-likelihood-for-representation-learning-2/\n",
    "\n",
    "https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b\n",
    "\n",
    "https://pyro.ai/, https://pyro.ai/examples/intro_part_i.html\n",
    "\n",
    "https://www.tuananhle.co.uk/notes/reverse-forward-kl.html\n",
    "\n",
    "https://blog.evjang.com/2016/08/variational-bayes.html\n",
    "\n",
    "https://dibyaghosh.com/blog/probability/kldivergence.html\n",
    "\n",
    "https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/\n",
    "\n",
    "http://willwolf.io/2018/03/31/gaussian-algebra-to-gaussian-processes-part-1/\n",
    "\n",
    "http://andymiller.github.io/2016/11/23/vb.html\n",
    "\n",
    "http://mlg.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
