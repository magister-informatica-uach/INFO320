{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap from class 3\n",
    "\n",
    "We are interested in a posterior \n",
    "$$\n",
    "p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})}\n",
    "$$\n",
    "which may be intractable\n",
    "\n",
    "In that case we do approximate inference either through sampling (MCMC) or optimization (VI). \n",
    "\n",
    "In the latter we select a (simple) approximate posterior $q_\\nu(\\theta)$ and we optimize the parameters $\\nu$ by maximizing the evidence lower bound (ELBO)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}) \\geq  \\mathcal{L}(\\nu) &= - \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta  \\\\\n",
    "&= \\mathbb{E}_{\\theta \\sim q_\\nu(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\right]- D_{KL}[q_\\nu(\\theta) || p(\\theta)]  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which makes $q_\\nu(\\theta)$ close to $p(\\theta|\\mathcal{D})$\n",
    "\n",
    "What can we do to improve this?\n",
    "\n",
    "1. Using more flexible approximate posteriors\n",
    "1. Making the bound tigher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More flexible approximate posteriors for VI\n",
    "\n",
    "One way to obtain a more flexible posterior that is still tractable is to start with a simple distribution and apply a sequence of invertible transformations\n",
    "\n",
    "This is the key idea behind [normalizing flows](https://arxiv.org/abs/1505.05770)\n",
    "\n",
    "Let's say that $z\\sim q(z)$ where $q$ is simple, *e.g.* standard gaussian\n",
    "\n",
    "and that there is a smooth and invertible transformation $f$ such that $f^{-1}(f(z)) = z$\n",
    "\n",
    "Then $z' = f(z)$ is a random variable too but its distribution is\n",
    "\n",
    "$$\n",
    "q_{z'}(z') = q(z) \\left| \\frac{\\partial f^{-1}}{\\partial z'} \\right| = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1}\n",
    "$$\n",
    "\n",
    "which is the original distribution times the inverse of jacobian of the transformation\n",
    "\n",
    "And we can apply a chain of transformations $f_1, f_2, \\ldots, f_K$ obtaining\n",
    "\n",
    "$$\n",
    "q_K(z_K) = q_0(z_0) \\prod_{k=1}^K \\left| \\frac{\\partial f_k}{\\partial z_{k-1}} \\right|^{-1}\n",
    "$$\n",
    "\n",
    "With this we can go from a simple Gaussian to more expressive/complex/multi-modal distributions \n",
    "\n",
    "Nowadays several types of flows exist in the literature, *e.g.* planar, radial, autoregresive\n",
    "\n",
    "[Normalizing flows have been used to make the approximate posterior in VAE more expressive](https://arxiv.org/abs/1809.05861)\n",
    "\n",
    "Three excellent blog posts covering normalizing flows:\n",
    "- https://blog.evjang.com/2018/01/nf1.html\n",
    "- http://akosiorek.github.io/ml/2018/04/03/norm_flows.html\n",
    "- https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html\n",
    "\n",
    "[Normalizing flows in Pyro](https://bmazoure.github.io/posts/nf-in-pyro/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tigher bounds for VI\n",
    "\n",
    "- [Auxiliary Deep Generative Models](https://arxiv.org/abs/1602.05473)\n",
    "- [Importance Weighted Autoencoders](https://arxiv.org/abs/1509.00519)\n",
    "- [Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference](https://openreview.net/forum?id=HyZoi-WRb)\n",
    "- [Tighter Variational Bounds are Not Necessarily Better](https://arxiv.org/abs/1802.04537)\n",
    "- http://artem.sobolev.name/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other perspectives\n",
    "\n",
    "- [Stein Variational gradient descent](https://arxiv.org/abs/1608.04471)\n",
    "- [Approximate MCMC](https://arxiv.org/abs/1908.03491)\n",
    "- [Adversarially learned inference (ALI)]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://papers.nips.cc/paper/8681-practical-deep-learning-with-bayesian-principles.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Comprehensive guide to Bayesian Convolutional Neural Network with Variational Inference\n",
    "\n",
    "https://arxiv.org/pdf/1901.02731.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GECO: https://arxiv.org/pdf/1810.00597.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blundell weight uncertainty (bayes by backprop): https://arxiv.org/pdf/1505.05424.pdf, https://github.com/ThirstyScholar/bayes-by-backprop, https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html, http://krasserm.github.io/2019/03/14/bayesian-neural-networks/\n",
    "\n",
    "- Graves, practical VI for NN: https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf\n",
    "- https://csc2541-f17.github.io/slides/lec04.pdf\n",
    "\n",
    "\n",
    "FLIPOUT: https://arxiv.org/abs/1803.04386"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- advances in VI: https://arxiv.org/pdf/1711.05597.pdf\n",
    "- Variational dropout and local reparameterization trick: http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick, https://alsibahi.xyz/snippets/2019/06/15/pyro_mnist_bnn_kl.html, \n",
    "- Uncertainty Estimations by Softplus normalization inBayesian Convolutional Neural Networks withVariational Inference: https://arxiv.org/pdf/1806.05978.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning \n",
    "\n",
    "https://arxiv.org/abs/1506.02142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kalman VAE\n",
    "\n",
    "https://arxiv.org/pdf/1710.057416.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimization\n",
    "\n",
    "http://pyro.ai/examples/bo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frameworks\n",
    "- [Automatic Differentiation Variational Inference (ADVI)](https://arxiv.org/abs/1603.00788)\n",
    "- [Operator Variational Inference (OPVI)](https://papers.nips.cc/paper/6091-operator-variational-inference.pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DODEEPGENERATIVEMODELSKNOWWHATTHEYDONâ€™TKNOW? \n",
    "https://arxiv.org/pdf/1810.09136.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
