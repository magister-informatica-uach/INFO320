{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%autosave 0\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#import pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Variable Models\n",
    "\n",
    "\n",
    "Let's say we want to model a dataset $X = (x_1, x_2, \\ldots, x_N)$ with $x_i \\in \\mathbb{R}^D$ \n",
    "\n",
    "> We are looking for $p(x)$\n",
    "\n",
    "Each sample has D attributes\n",
    "\n",
    "> These are the **observed variables** (visible space)\n",
    "\n",
    "To model the data we have to propose dependency relationships between variables\n",
    "\n",
    "> Modeling correlation is difficult\n",
    "\n",
    "One alternative is to assume that what we observe is correlated due to *hidden causes*\n",
    "\n",
    "> These are the **latent variables** (hidden space)\n",
    "\n",
    "Models with latent variables are called Latent Variable Models (LVM)\n",
    "\n",
    "Then we get the marginal using\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) &= \\int_z p(x, z) \\,dz \\nonumber \\\\\n",
    "&= \\int_z p(x|z) p(z) \\,dz \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Did we gain anything? \n",
    "\n",
    "> The integral can be hard to solve (in some cases it is tractable)\n",
    "\n",
    "The answer is YES\n",
    "\n",
    "> We can propose simple $p(x|z)$ and $p(z)$ and get complex $p(x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Principal Component Analysis (PCA)\n",
    "\n",
    "\n",
    "## Classical PCA\n",
    "\n",
    "PCA is an algorithm to reduce the dimensionality of continous data\n",
    "\n",
    "Let's say we have $X = (x_1, x_2, \\ldots, x_N)$ con $x_i \\in \\mathbb{R}^D$\n",
    "\n",
    "In classical PCA we \n",
    "\n",
    "1. Compute covariance matrix $C = \\frac{1}{N} X^T X$\n",
    "1. Solve the eigen value problem $(C - \\lambda I)W = 0$\n",
    "\n",
    "This comes from \n",
    "\n",
    "$$\n",
    "\\min_W W^T C W, \\text{s.t.} ~ W^T W = I\n",
    "$$\n",
    "\n",
    "> PCA finds an **orthogonal transformation** $W$ that **minimizes the variance** of the projected data $XW$\n",
    "\n",
    "Then we can reduce the amount of columns of $W$ to reduce the dimensionality of $XW$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic interpretation\n",
    "\n",
    "We can give a probabilistic interpretation to PCA as an LVM\n",
    "\n",
    "An observed sample $x_i \\in \\mathbb{R}^D$ is modeled as \n",
    "\n",
    "$$\n",
    "x_i = W z_i + \\mu + \\epsilon\n",
    "$$\n",
    "\n",
    "> Observed variable is related to the latent variable via a **linear mapping**\n",
    "\n",
    "where \n",
    "- $\\mu \\in \\mathbb{R}^D$ is the mean of $X$\n",
    "- $W \\in \\mathbb{R}^{D\\times K}$ is a linear transformation matrix\n",
    "- $\\epsilon$ is noise\n",
    "\n",
    "> $z_i \\in  \\mathbb{R}^K$ is a continuous latent variable with $K<D$\n",
    "\n",
    "#### Assumption: The noise is independent and gaussian distributed with variance $\\sigma^2$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "p(x_i | z_i) = \\mathcal{N}(\\mu + W z_i, I \\sigma^2)\n",
    "$$\n",
    "\n",
    "Note: In general factor analysis the noise has a diagonal covariance\n",
    "\n",
    "#### Assumption: The latent variable has a standard gaussian prior\n",
    "\n",
    "$$\n",
    "p(z_i) = \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Marginal likelihood\n",
    "\n",
    "The Gaussian is conjugated to itself (convolution of Gaussians is Gaussian)\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) &= \\int p(x|z) p(z) \\,dz \\nonumber \\\\\n",
    "&= \\mathcal{N}(x|\\mu, W^T W + I\\sigma^2 ) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> We have parametrized a normal with full covariance from to normals with diagonal covariance\"\n",
    "\n",
    "The parameters are calculated from \n",
    "- $\\mathbb{E}[x] = W\\mathbb{E}[z] + \\mu + \\mathbb{E}[\\epsilon]$\n",
    "- $\\mathbb{E}[(Wz + \\epsilon)(Wz + \\epsilon)^T] = W \\mathbb{E}[zz^T] W^T + \\mathbb{E}[\\epsilon \\epsilon^T]$\n",
    "\n",
    "#### Posterior\n",
    "\n",
    "Using Bayes we can obtain the posterior to go from observed to latent\n",
    "\n",
    "$$\n",
    "p(z|x) = \\mathcal{N}(z|M^{-1}W^T(x-\\mu), M\\sigma^{-2} )\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "M = W^T W + I\\sigma^2\n",
    "$$\n",
    "\n",
    "#### Training\n",
    "\n",
    "We fit the model to find $W$, $\\mu$ and $\\sigma$ by maximizing the marginal likelihood\n",
    "\n",
    "$$\n",
    "\\max \\log L(W,\\mu, \\sigma^2) = \\sum_{i=1}^N \\log p(x_i)\n",
    "$$\n",
    "\n",
    "From here we can do derivates and obtain closed form solutions of the parameters\n",
    "\n",
    "> Solution for $W$ is equivalent to conventional PCA ($\\sigma^2 \\to 0$)\n",
    "\n",
    "> Now we have estimated $\\sigma$, we have errorbars for $z$ and the model is generative\n",
    "\n",
    "\n",
    "## Self-study\n",
    "- Barber, Chapter 21 and Murphy, Chapter 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import theano.tensor as tt\n",
    "from sklearn.decomposition import PCA\n",
    "N = 1000 \n",
    "M = 3  # dimensions of the data\n",
    "D = 2  # dimensions of the projection\n",
    "\n",
    "np.random.seed(10)\n",
    "C = np.random.randn(M, M)\n",
    "C = np.dot(C.T, C)\n",
    "X = np.random.multivariate_normal(np.zeros(shape=(M, )), C, size=N)\n",
    "X = X - np.mean(X, axis=0)\n",
    "X = X/np.std(X, axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], s=2)\n",
    "pca = PCA(n_components=2, whiten=False)\n",
    "R = pca.fit_transform(X)\n",
    "ax = fig.add_subplot(122)\n",
    "plt.scatter(R[:, 0], R[:, 1], s=1)\n",
    "_ = plt.title('PCA projection')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as PPCA:\n",
    "    s = pm.HalfCauchy('s', beta=5, shape=[1,])\n",
    "    w = pm.Normal('w', mu=tt.zeros([D, M]), sd=tt.ones([D, M]), shape=[D, M])\n",
    "    z = pm.Normal('z', mu=tt.zeros([N, D]), sd=tt.ones([N, D]), shape=[N, D])\n",
    "    x = pm.Normal('x', mu=z.dot(w), sd=s*tt.ones([N, M]), shape=[N, M], observed=X)  \n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=2000, method=inference, obj_optimizer=pm.adam(learning_rate=1e-1))\n",
    "\"\"\"\n",
    "_ = plt.plot(-inference.hist)\n",
    "plt.ylabel('Evidence lower bound (ELBO)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid()\n",
    "\"\"\"\n",
    "with PPCA:\n",
    "    trace = approx.sample(draws=1000)\n",
    "    ppc = pm.sample_ppc(trace=trace, samples=100)\n",
    "_ = pm.traceplot(trace=trace, varnames=['w', 's'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_avg = np.mean(trace['w'], axis=0)\n",
    "s_avg = np.mean(trace['s'], axis=0)\n",
    "print(\"Average W\")\n",
    "print(W_avg)\n",
    "print(\"Average sigma: %f\" %(s_avg))\n",
    "\n",
    "x_reconstructed = ppc['x'][0, :, :] \n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], s=2)\n",
    "ax.set_title('Input data')             \n",
    "bx, by, bz = ax.get_xbound(), ax.get_ybound(), ax.get_zbound()      \n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.set_title(\"Sampled data\")\n",
    "ax.scatter(x_reconstructed[:, 0], x_reconstructed[:, 1], x_reconstructed[:, 2], s=1, alpha=0.5)\n",
    "t = np.linspace(-4, 4, num=100)\n",
    "ax.set_xbound(bx)\n",
    "ax.set_ybound(by)\n",
    "ax.set_zbound(bz)\n",
    "\n",
    "z_trace_avg = np.mean(trace['z'], axis=0)\n",
    "z_trace_std = np.std(trace['z'], axis=0)\n",
    "z_trace_var = np.mean(np.var(trace['z'], axis=1), axis=0)\n",
    "# Sort the new axis in decreasing order of variance\n",
    "axis_order = np.argsort(z_trace_var)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3), tight_layout=True)\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.errorbar(z_trace_avg[:, axis_order[0]], z_trace_avg[:, axis_order[1]], \n",
    "            z_trace_std[:, axis_order[0]], z_trace_std[:, axis_order[1]], fmt='none', alpha= 0.5)\n",
    "plt.title('Average z from trace')\n",
    "\n",
    "Z_test = np.dot(X, np.dot(np.linalg.inv(np.dot(W_avg.T, W_avg) + np.eye(M)*s_avg**2 ), W_avg.T))\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.scatter(Z_test[:, axis_order[0]], Z_test[:, axis_order[1]], s=1, alpha=0.5)\n",
    "_ = plt.title('Average z by hand')\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.scatter(R[:, 0], R[:, 1], s=1, alpha=0.5)\n",
    "_ = plt.title('z from sklearn PCA')\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "# SKLEARN gives you the new axis already sorted by variance, also axis might appear rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model\n",
    "\n",
    "Model with categorical latent variables\n",
    "\n",
    "\n",
    "FUTURE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "p = scipy.stats.bernoulli(0.6).rvs(1000)\n",
    "G1 = scipy.stats.norm(loc=5., scale=2.).rvs(1000) # N(5, sqrt(2))\n",
    "G2 = scipy.stats.norm(loc=-2., scale=1.5).rvs(1000) # N(0, sqrt(10))\n",
    "data = np.concatenate((G1[p==1], G2[p==0])) # Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    #mu_prior = pyro.sample(\"mean\", pyro.distributions.Normal(0, 10))\n",
    "    #sd_prior = pyro.sample(\"sigma\", pyro.distributions.HalfNormal(5, 10))\n",
    "    #return pyro.sample(\"obs\", pyro.distributions.Normal(mu_prior, sd_prior))\n",
    "    mu = torch.tensor([[5.], [-2.]])\n",
    "    sd = torch.tensor([[2.], [1.5]])\n",
    "    pi = torch.tensor([np.log(0.6), np.log(0.4)])\n",
    "    return pyro.sample(\"obs\", pyro.distributions.MixtureOfDiagNormals(mu, sd**2, pi))\n",
    "    \n",
    "\n",
    "def guide():\n",
    "    mu_loc = pyro.param(\"mu_loc\", torch.tensor([0.], dtype=torch.float32))\n",
    "    mu_scale = pyro.param(\"mu_scale\", torch.tensor([1.], dtype=torch.float32), \n",
    "                          constraint=torch.distributions.constraints.positive)\n",
    "    sd_loc = pyro.param(\"sd_loc\", torch.tensor([1.], dtype=torch.float32), \n",
    "                        constraint=torch.distributions.constraints.positive)\n",
    "    sd_scale = pyro.param(\"sd_scale\", torch.tensor([1], dtype=torch.float32), \n",
    "                          constraint=torch.distributions.constraints.positive)\n",
    "    \n",
    "    mu = pyro.sample(\"mean\", pyro.distributions.Normal(mu_loc, mu_scale))\n",
    "    sd = pyro.sample(\"sigma\", pyro.distributions.Normal(sd_loc, sd_scale))\n",
    "\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "#guide = AutoDiagonalNormal(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#data = 5*np.random.randn(1000)\n",
    "data_torch = torch.from_numpy(data.astype('float32'))\n",
    "conditioned_model = pyro.condition(model, data={\"obs\": data_torch})\n",
    "\n",
    "svi = pyro.infer.SVI(model=conditioned_model,\n",
    "                     guide=guide,\n",
    "                     optim=pyro.optim.Adam({\"lr\": 1e-3}),\n",
    "                     loss=pyro.infer.Trace_ELBO(),\n",
    "                     num_samples=10)\n",
    "\n",
    "losses, a,b  = [], [], []\n",
    "num_steps = 2000\n",
    "for epoch in range(num_steps):\n",
    "    #a.append(pyro.param(\"mu\").detach().numpy())\n",
    "    #b.append(pyro.param(\"cov\").detach().numpy())\n",
    "    losses.append(svi.step())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses)\n",
    "ax.set_title(\"ELBO\")\n",
    "ax.set_xlabel(\"step\")\n",
    "ax.set_ylabel(\"loss\");\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.hist(data, bins=30, density=True, alpha=0.75);\n",
    "x_plot = torch.linspace(np.amin(data), np.amax(data), steps=1000)\n",
    "#line = ax.plot(x_plot.numpy(), model.pdf(x_plot).detach().numpy(), lw=2)\n",
    "\n",
    "#ax.plot(x_plot, np.exp(-0.5*(x_plot - 2.15)**2/3.7580**2)/np.sqrt(2.0*np.pi*3.75**2))\n",
    "\n",
    "#anim = animation.FuncAnimation(fig, update_plot, frames=100, interval=20, \n",
    "#                               repeat=True, blit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "fig, ax = plt.subplots()\n",
    "for i in [0, 100, 1000, -1]:\n",
    "    ellipse = Ellipse((a[i][0], a[i][1]),\n",
    "            width=b[i][0], height=b[i][1], facecolor='none', ls='--', edgecolor='k')\n",
    "\n",
    "    ax.add_patch(ellipse)\n",
    "    \n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([-5, 5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
