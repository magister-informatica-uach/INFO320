{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many interesting models the evidence  \n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\mathcal{M}_i) = \\int p(\\mathcal{D}|\\mathcal{M}_i, \\theta) p(\\theta| \\mathcal{M}_i) d\\theta\n",
    "$$\n",
    "\n",
    "and hence the posterior are intractable:\n",
    "\n",
    "- The integral has no closed-form\n",
    "- The dimensionality is so big that numerical integration is not feasible\n",
    "\n",
    "We resort to stochastic or deterministic approximations\n",
    "\n",
    "- MCMC is computationally demanding but can be exact\n",
    "- VI scales better but is not exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Approximation\n",
    "\n",
    "\n",
    "Propose a function of $\\theta \\in \\mathbb{R}^K$\n",
    "\n",
    "$$\n",
    "g(\\theta) = \\log p(\\mathcal{D}| \\theta) p(\\theta)\n",
    "$$\n",
    "\n",
    "Do a second order Taylor expansion around $\\theta= \\hat \\theta_{\\text{map}}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(\\theta) \\approx g(\\hat \\theta_{\\text{map}}) &+ (\\theta - \\hat \\theta_{\\text{map}})^T \\frac{dg}{d\\theta}\\bigg \\rvert_{\\theta=\\hat \\theta_{\\text{map}}} \\nonumber \\\\\n",
    "&+ \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\frac{d^2 g}{d\\theta^2} \\bigg \\rvert_{\\theta=\\hat \\theta_{\\text{map}}} (\\theta - \\hat \\theta_{\\text{map}})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- By definition the first derivative evaluated at $\\hat \\theta_{\\text{map}}$ is zero \n",
    "- We call the negative Hessian evaluated at $\\hat \\theta_{\\text{map}}$: $\\Sigma^{-1} = -\\frac{d^2 g}{d\\theta^2} (\\hat \\theta_{\\text{map}})$ \n",
    "\n",
    "If we plug the approximation in the evidence we can solve the integral\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}) \\approx  e^{g(\\hat \\theta_{\\text{map}})} \\int e^{-  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Sigma^{-1} (\\theta - \\hat \\theta_{\\text{map}})} d\\theta = e^{g(\\hat \\theta_{\\text{map}})} (2\\pi)^{K/2} |\\Sigma|^{1/2}\n",
    "$$\n",
    "\n",
    "And the posterior\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta| \\mathcal{D}) &= \\frac{p(\\mathcal{D}|\\theta) p(\\theta) }{p(\\mathcal{D})} \\nonumber \\\\\n",
    "&\\approx \\frac{1}{(2\\pi)^{K/2} |\\Sigma|^{1/2}} e^{-  \\frac{1}{2} (\\theta - \\hat \\theta_{\\text{map}})^T \\Sigma^{-1} (\\theta - \\hat \\theta_{\\text{map}})} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Laplace method approximates the posterior by a **Multivariate Gaussian** centered in the MAP\n",
    "\n",
    "Two steps\n",
    "1. Find the mode (MAP)\n",
    "1. Evaluate the Hessian at the mode\n",
    "\n",
    "Note that\n",
    "- We didn't assume any distribution for the prior or likelihood\n",
    "- We require that $g$ is continuous and differentiable on $\\theta$ \n",
    "- We also require that the negative Hessian of $g$ on the MAP is a proper covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evidence decomposition \n",
    "\n",
    "Using Laplace approximation the log evidence can be decomposed as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}|\\mathcal{M}_i) &\\approx g(\\hat \\theta_{\\text{map}}) + \\log (2\\pi)^{K/2} |\\Sigma|^{1/2} \\nonumber \\\\\n",
    "&=\\log p(\\mathcal{D}|\\mathcal{M}_i, \\hat \\theta_{\\text{map}}) + \\log p(\\hat \\theta_{\\text{map}}| \\mathcal{M}_i) + \\frac{K}{2} \\log(2\\pi) + \\frac{1}{2} \\log | \\Sigma | \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> The log evidence is approximated by the best likelihood fit plus the Occam's factor\n",
    "\n",
    "The Occam's factor depends on the\n",
    "- log pdf of $\\theta$\n",
    "- number of parameters $K$\n",
    "- second derivative of the posterior (model uncertainty)\n",
    "\n",
    "\n",
    "> If the prior is very broad and $N$ is very large we recover the **Bayesian Information Criterion** (BIC) (Proof?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference (VI)\n",
    "\n",
    "We want the posterior\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "but it may be intractable\n",
    "\n",
    "In VI a simpler (tractable) posterior distribution is proposed\n",
    "\n",
    "$$\n",
    "q_\\eta(\\theta)\n",
    "$$\n",
    "\n",
    "> We approximate $p(\\theta|\\mathcal{D})$ with $q_\\eta(\\theta)$\n",
    "\n",
    "$q_\\eta(\\theta)$ represents a family of distributions parametrized by $\\eta$\n",
    "\n",
    "> **Optimization problem:** Find $\\eta$ that makes $q$ most similar to $p$\n",
    "\n",
    "We can write this as a KL divergence\n",
    "\n",
    "$$\n",
    "\\min_\\eta D_{\\text{KL}}[q_\\eta(\\theta) || p(\\theta|\\mathcal{D})] = \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\theta|\\mathcal{D})} d\\theta\n",
    "$$\n",
    "\n",
    "This is still intractable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue we use Bayes Theorem on the posterior and move the evidence out from the integral\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}[q_\\eta(\\theta) || p(\\theta|\\mathcal{D})] = \\log p(\\mathcal{D}) + \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta\n",
    "$$\n",
    "\n",
    "> If we are minimizing with respect to $\\eta$ we can ignore the evidence\n",
    "\n",
    "The KL divergence is \n",
    "- non-negative\n",
    "- zero only if $q_\\eta(\\theta) \\equiv p(\\theta|\\mathcal{D})$\n",
    "\n",
    "Using the non-negativity we find a lower bound for the evidence\n",
    "\n",
    "$$\n",
    "\\log p(\\mathcal{D}) \\geq  \\mathcal{L}(\\eta) = - \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta\n",
    "$$\n",
    "\n",
    "> $\\mathcal{L}(\\eta)$ is called the **Evidence Lower BOund** (ELBO)\n",
    "\n",
    "- Minimizing the KL between $q$ and $p$ is equivalent to maximizing the ELBO wrt $q$\n",
    "$$\n",
    "\\hat \\eta = \\text{arg}\\max_\\eta \\mathcal{L}(\\eta)\n",
    "$$\n",
    "- We can use $q_{\\hat \\eta}(\\theta)$ as a drop-in replacement for $p(\\theta|\\mathcal{D})$\n",
    "- The ELBO is tractable for simple, parametric $q$\n",
    "- The ELBO can only be tight if $p$ is within the family of $q$\n",
    "\n",
    "\n",
    "\n",
    "¿Why variational?\n",
    "\n",
    "- Functional: Function of functions. [Calculus of variations](https://en.wikipedia.org/wiki/Calculus_of_variations): Derivatives of functionals\n",
    "- [Variational Free Energy](https://en.wikipedia.org/wiki/Thermodynamic_free_energy): $-\\mathcal{L}(\\eta)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to \"obtain\" the ELBO\n",
    "\n",
    "Using Jensen's inequality on the log evidence\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\mathcal{D}) &=  \\log \\mathbb{E}_{\\theta\\sim p(\\theta)} \\left[p(\\mathcal{D}|\\theta)\\right]\\nonumber \\\\\n",
    "&=  \\log \\mathbb{E}_{\\theta\\sim q_\\eta(\\theta)} \\left[p(\\mathcal{D}|\\theta)\\frac{p(\\theta)}{q_\\eta(\\theta)}\\right]\\nonumber \\\\\n",
    "&\\geq  \\mathbb{E}_{\\theta\\sim q_\\eta(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\frac{p(\\theta)}{q_\\eta(\\theta)}\\right] =- \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta \\nonumber \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-factorized posterior\n",
    "\n",
    "A simple (tractable) posterior\n",
    "\n",
    "$$\n",
    "q_\\eta(\\theta) = \\prod_{i=1}^K q_{\\eta_i}(\\theta_i)\n",
    "$$\n",
    "\n",
    "- no correlation between factors\n",
    "- this is known as the Mean-field VI or Mean-field Theory (physics)\n",
    "\n",
    "Using this factorized posterior the ELBO\n",
    "$$\n",
    "\\mathcal{L}(\\eta) =  \\int q_{\\eta_i}(\\theta_i) \\log \\left [ p(\\mathcal{D}|\\theta)p(\\theta) \\prod_{j\\neq i} q_{\\eta_j}(\\theta_j) d\\theta_j \\right ] d\\theta_i - \\sum_i \\int q_{\\eta_i}(\\theta_i) \\log q_{\\eta_i}(\\theta_i)  d\\theta_i\n",
    "$$\n",
    "\n",
    "We can  keep all $\\theta$ but $i$ fixed and update $i$ iteratively\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\eta_i) &=  \\int q_{\\eta_i}(\\theta_i) \\mathbb{E}_{\\prod q_{i\\neq j}} \\left[ p(\\mathcal{D}| \\theta)p(\\theta) \\right ] d\\theta_i - \\int q_{\\eta_i}(\\theta_i) \\log q_{\\eta_i}(\\theta_i)  d\\theta_i + \\text{Constant}  \\nonumber \\\\\n",
    "&  - \\int q_{\\eta_i}(\\theta_i) \\log \\frac{ q_{\\eta_i}(\\theta_i)}{\\mathbb{E}_{\\prod q_{i\\neq j}} \\left[ p(\\mathcal{D}| \\theta)p(\\theta) \\right ]} d\\theta_i + \\text{Constant} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Maximizing the ELBO es equivalent to minimizing the KL between $q_{\\eta_i}(\\theta_i)$ and \n",
    "\n",
    "The solution is\n",
    "\n",
    "$$\n",
    "\\log q_i(\\eta_i) =  \\mathbb{E}_{\\prod q_{i\\neq j}} \\left[ p(\\mathcal{D}| \\theta)p(\\theta) \\right ] + \\text{Constant}\n",
    "$$\n",
    "> Guaranteed convergence (convex on factors $q_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Approximating a Gaussian\n",
    "\n",
    "A full univariate Gaussian model would be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x, \\theta) &= p(x|\\theta) p(\\theta) \\nonumber \\\\\n",
    "& = p(x|\\mu, \\sigma^2) p(\\mu, \\sigma^2) \\nonumber \\\\\n",
    "& = p(x|\\mu, \\sigma^2) p(\\mu|\\sigma^2) p(\\sigma^2) \\nonumber \\\\\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "A full conjugate model would have Normal $p(\\mu|\\sigma^2) $ and Gamma $p(\\sigma^2)$\n",
    "\n",
    "Approximating with a fully-factorized assumes\n",
    "\n",
    "$$\n",
    "q(\\theta) = q(\\mu)q(\\sigma^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "from pyro.distributions import Normal, InverseGamma\n",
    "from torch.distributions import constraints\n",
    "from pyro.infer import EmpiricalMarginal\n",
    "\n",
    "\n",
    "from pyro.infer.mcmc.api import MCMC\n",
    "from pyro.infer.mcmc import NUTS\n",
    "\n",
    "data = 5. + 2.*torch.from_numpy(np.random.randn(10000).astype('float32'))\n",
    "\n",
    "def model(x):\n",
    "    s2 = pyro.sample(\"s2\", InverseGamma(torch.tensor(0.1), torch.tensor(0.1)))    \n",
    "    mu = pyro.sample(\"mu\", Normal(torch.tensor(0.), s2))  \n",
    "    return pyro.sample(\"likelihood\", Normal(mu, s2), obs=x)\n",
    "\n",
    "nuts_kernel = NUTS(model, adapt_step_size=True)\n",
    "sampler = MCMC(nuts_kernel, num_chains=2, num_samples=1000, warmup_steps=100)\n",
    "sampler.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.diagnostics()\n",
    "samples = sampler.get_samples(10000)\n",
    "mu_plot = samples['mu'].detach().numpy()\n",
    "s2_plot = samples['s2'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "#ax.hist2d(samples['mu'].detach().numpy(), samples['s2'].detach().numpy(), \n",
    "#          bins=20, cmap=plt.cm.Blues, range=[[4.5, 5.5], [1.5, 2.5]])\n",
    "#ax.set_xlabel('mu')\n",
    "#ax.set_ylabel('s2')\n",
    "figure = corner.corner(np.stack((mu_plot, s2_plot)).T, \n",
    "                       labels=[r\"$\\mu$\", r\"$\\sigma^2$\"],\n",
    "                       quantiles=[0.16, 0.5, 0.84],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(x):\n",
    "    mu_loc = pyro.param(\"mu_loc\", torch.tensor(0.))\n",
    "    mu_scale = pyro.param(\"mu_scale\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    mu = pyro.sample(\"mu\", Normal(mu_loc, mu_scale))  \n",
    "    a = pyro.param(\"s_a\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    b = pyro.param(\"s_b\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    s2 = pyro.sample(\"s2\", InverseGamma(a, b))    \n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model=model, guide=guide,\n",
    "                     optim=pyro.optim.Adam({\"lr\": 0.05}),\n",
    "                     loss=pyro.infer.Trace_ELBO(), num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(7, 3), tight_layout=True)\n",
    "lines = [ax_.plot([], [])[0] for ax_ in ax.ravel()[:-1]]\n",
    "param_names = [\"mu_loc\", \"mu_scale\", \"s_a\", \"s_b\"]\n",
    "losses, param_evolution = [], {}\n",
    "for name in param_names:\n",
    "    param_evolution[name] = []\n",
    "    \n",
    "ax[0, 0].set_title('ELBO')\n",
    "for ax_, name in zip(ax.ravel()[1:], param_names):\n",
    "    ax_.set_title(name)\n",
    "\n",
    "for k in tqdm_notebook(range(7500)):\n",
    "    losses.append(svi.step(data))\n",
    "    #display(pyro.get_param_store().keys())\n",
    "\n",
    "    for name in param_names:\n",
    "        param_evolution[name].append(pyro.param(name).item())\n",
    "    \n",
    "    \n",
    "\n",
    "    if np.mod(k, 100) == 0:\n",
    "        lines[0].set_ydata(losses[:k])\n",
    "        for i, name in enumerate(param_names):\n",
    "            lines[i+1].set_ydata(param_evolution[name][:k])\n",
    "        for line in lines:\n",
    "            line.set_xdata(range(k))\n",
    "        for ax_ in ax.ravel():\n",
    "            ax_.relim()\n",
    "            ax_.autoscale_view()\n",
    "        ax[1, 2].cla() \n",
    "        posterior = svi.run(data)\n",
    "        mu_plot = EmpiricalMarginal(posterior, \"mu\").sample(sample_shape=torch.Size([10000]))\n",
    "        s2_plot = EmpiricalMarginal(posterior, \"s2\").sample(sample_shape=torch.Size([10000]))\n",
    "        ax[1, 2].hist2d(mu_plot.detach().numpy(), s2_plot.detach().numpy(), \n",
    "                        bins=20, cmap=plt.cm.Blues, range=[[3.5, 6.5], [0., 5.]])\n",
    "        ax[1, 2].set_ylim([0, 5.])\n",
    "        ax[1, 2].set_xlim([3.5, 6.5])\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import EmpiricalMarginal\n",
    "posterior = svi.run(data)\n",
    "mu_plot = EmpiricalMarginal(posterior, \"mu\").sample(sample_shape=torch.Size([10000]))\n",
    "s2_plot = EmpiricalMarginal(posterior, \"s2\").sample(sample_shape=torch.Size([10000]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist2d(mu_plot.detach().numpy(), \n",
    "          s2_plot.detach().numpy(), bins=20, cmap=plt.cm.Blues)\n",
    "ax.set_xlabel('mu')\n",
    "ax.set_ylabel('s2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More attention on the ELBO \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\eta) &= - \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta \\nonumber \\\\\n",
    "&= - \\int q_\\eta(\\theta) \\log \\frac{q_\\eta(\\theta)}{ p (\\theta)} d\\theta + \\int q_\\eta(\\theta) \\log p(\\mathcal{D}|\\theta) d\\theta \\nonumber \\\\\n",
    "&= - D_{KL}[q_\\eta(\\theta) || p(\\theta)] + \\mathbb{E}_{\\theta \\sim q_\\eta(\\theta)} \\left[\\log p(\\mathcal{D}|\\theta)\\right]\\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Maximizing the ELBO is equivalent to:\n",
    "- Maximize the log likelihood when sampling from the approximate posterior\n",
    "- Minimize the KL between the approximate posterior and prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-study\n",
    "\n",
    "- [Chapter 28 of D. Barber's book](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)\n",
    "    - 28.2 Laplaca Approximation\n",
    "    - 28.4 Variational Bounding\n",
    "- David Blei, [\"Variational Inference: A review for statisticians\"](https://arxiv.org/abs/1601.00670), [\"Foundations and innovations\"](https://www.youtube.com/watch?v=DaqNNLidswA)\n",
    "- https://cedar.buffalo.edu/~srihari/CSE574/Chap10/10.2VariationalInference.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
