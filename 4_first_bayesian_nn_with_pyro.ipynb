{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualizaciÃ³n en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm_notebook\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (brief) tutorial on [Pyro](https://pyro.ai/)\n",
    "\n",
    "Pyro can be used to perform MCMC and/or approximate inference for intractable posteriors\n",
    "\n",
    "We can use Pyro to move from point estimates to posteriors in our **torch-based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "display(pyro.__version__)\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a normal distribution\n",
    "\n",
    "Distributions in Pyro are implemented in [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)\n",
    "\n",
    "The `Normal` object expects location $\\mu$ and scale $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal\n",
    "\n",
    "w_prior = Normal(loc=torch.tensor(0.), \n",
    "                 scale=torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample from this distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1000 samples \n",
    "samples = w_prior.rsample(sample_shape=(1000, ))\n",
    "display(samples.shape)\n",
    "# Build an histogram\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.hist(samples.detach().numpy(), bins=20, density=True)\n",
    "# Plot the pdf\n",
    "w_plot = np.linspace(-3, 3, num=100)\n",
    "w_pdf = torch.exp(w_prior.log_prob(torch.from_numpy(w_plot))).detach().numpy()\n",
    "plt.plot(w_plot, w_pdf, 'k-', lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean\n",
    "display(w_prior.mean)\n",
    "#standard deviation\n",
    "display(w_prior.stddev)\n",
    "#entropy\n",
    "display(w_prior.entropy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of pyro tensors\n",
    "\n",
    "A distribution has two shapes\n",
    "\n",
    "`event_shape` refers to the dimensionality of the distribution, e.g. normal (number), multivariate normal (vector), Cholesky (matrix), etc\n",
    "\n",
    "> `event_shape` denotes dependent random variables\n",
    "\n",
    "`batch_shape` refers to a batch of distributions\n",
    "\n",
    "> `batch_shape` denotes conditionally independent random variables (typically our data dimension)\n",
    "\n",
    "We can create a batched distribution by batching the parameters\n",
    "\n",
    "The shape of a sampled tensor will be the sum of event and batch shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two conditionally independent normal RVs\n",
    "w_prior = Normal(torch.tensor([[0., 2.]]), torch.tensor([[1., 1.]]))\n",
    "# A multivariate normal with diagonal covariance\n",
    "#w_prior = Normal(torch.tensor([[0., 2.]]), torch.tensor([[1., 1.]])).to_event(1)\n",
    "\n",
    "display(w_prior.batch_shape)\n",
    "display(w_prior.event_shape)\n",
    "display(w_prior.rsample().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables in Pyro\n",
    "\n",
    "To create random variables that we can track within a model we use [`pyro.sample`](http://pyro.ai/examples/intro_part_i.html#The-pyro.sample-Primitive)\n",
    "\n",
    "`sample` expects a name and an object from [`pyro.distributions`](http://docs.pyro.ai/en/stable/distributions.html)\n",
    "\n",
    "For example, to create a variable named \"w\" with the previously defined distribution\n",
    "$$\n",
    "\\begin{align}\n",
    "w \\sim &\\mathcal{N}(\\mu, \\sigma^2) \\nonumber \\\\\n",
    "&\\mu = \\begin{pmatrix}0 \\\\ 2 \\end{pmatrix}, \\sigma = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\nonumber\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    return pyro.sample(name='w', \n",
    "                       fn=w_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we run a model a random sample is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and conditioning \n",
    "\n",
    "Let's consider a linear regression model\n",
    "\n",
    "$$\n",
    "y = w x + b\n",
    "$$\n",
    "\n",
    "We will write this model in Pyro \n",
    "\n",
    "For this we consider\n",
    "- $w$ and $b$ to be random variables with normal distributions (priors)\n",
    "- $y$ to be a random variable with normal distribution (likelihood)\n",
    "- $x$ to be a deterministic variable\n",
    "- $y$ is continioned to the observed data $\\{y_i\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    w = pyro.sample(\"w\", Normal(0.0, 10.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 10.0))\n",
    "    with pyro.plate('dataset', size=len(x)):\n",
    "        return pyro.sample(\"y\", Normal(x*w + b, 1.0))\n",
    "\n",
    "def conditioned_model(x, y):\n",
    "    return pyro.condition(model, data={\"y\": y})(x)\n",
    "\n",
    "# or equivalently\n",
    "\n",
    "def model_obs(x, y):  # equivalent to conditioned_scale above\n",
    "    w = pyro.sample(\"w\", Normal(0.0, 10.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 10.0))\n",
    "    with pyro.plate('dataset', size=len(x)):\n",
    "        return pyro.sample(\"y\", Normal(x*w + b, 1.0), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical details\n",
    "\n",
    "To create conditions we can use `pyro.condition` or the `obs` keyword of `pyro.sample`\n",
    "\n",
    "To create conditions on the whole dataset (assuming iid) we use [`pyro.plate`](http://docs.pyro.ai/en/stable/primitives.html#pyro.plate), which expects a name and the size of the dataset\n",
    "\n",
    "In this case we use `pyro.plate` as a context (vectorized plate), it can also be used as an iterator\n"
   ]
  },
  {
   "attachments": {
    "graphical_model.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorical details\n",
    "\n",
    "In summary the model has to define the generative process \n",
    "\n",
    "In this case\n",
    "- Choose hyperparameters: $\\mu_w, \\sigma_w, \\mu_b, \\sigma_b, \\sigma_\\epsilon$\n",
    "- Sample: $w \\sim \\mathcal{N}(\\mu_w, \\sigma_w^2)$\n",
    "- Sample: $b \\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)$\n",
    "- For each $i=1,2,\\ldots, N$\n",
    "    - Sample: $y_i \\sim \\mathcal{N}(w x_i + b, \\sigma_\\epsilon^2)$\n",
    "\n",
    "This is often summarized using plate notation diagrams\n",
    "\n",
    "![graphical_model.png](attachment:graphical_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Now that we have specified the model we would like to obtain the posterior of the parameters given the data and do predictions\n",
    "\n",
    "> If we can not solve it analytically or via enumeration we can resort to approximate inference\n",
    "\n",
    "Pyro offers different ways to perform approximate inference in the module [`pyro.infer`](https://docs.pyro.ai/en/stable/inference.html)\n",
    "\n",
    "For now we will focus on **Stochastic Variational Inference** \n",
    "\n",
    "\n",
    "The unified Variational Inference interface in Pyro is located in [`pyro.infer.SVI`](http://docs.pyro.ai/en/stable/inference_algos.html) \n",
    "\n",
    "To use SVI we need to specify\n",
    "1. A model function that defines our generative model\n",
    "1. A guide function that defines our approximate posterior\n",
    "1. A cost function \n",
    "1. An optimizer\n",
    "1. Number of samples to compute Monte-Carlo estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide function\n",
    "\n",
    "> The guide represents our approximate posterior $q_\\nu(\\theta)$ \n",
    "\n",
    "The guide has to define the distribution of the posterior of the parameters \n",
    "\n",
    "We use [`pyro.param`]() to register the hyperparameters of the approximate posterior $\\eta$\n",
    "\n",
    "> These parameters are the ones that we learn through optimization\n",
    "\n",
    "**Technical detail:** The guide function has the same inputs as the model function\n",
    "\n",
    "In this example we set a normal posterior $\\theta=(w, b)$ and create the corresponding hyperparameters $\\eta = (\\mu_w, \\sigma_w, \\mu_b, \\sigma_b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "\n",
    "def guide(x, y):\n",
    "    w_loc = pyro.param(\"w_loc\", torch.tensor(0.))\n",
    "    w_scale = pyro.param(\"w_scale\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    w = pyro.sample(\"w\", Normal(w_loc, w_scale))\n",
    "    b_loc = pyro.param(\"b_loc\", torch.tensor(0.))\n",
    "    b_scale = pyro.param(\"b_scale\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    b = pyro.sample(\"b\", Normal(b_loc, b_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "In the previous class we studied the Evidence Lower Bound (ELBO)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\nu &= \\text{arg}\\max_\\nu \\mathcal{L}(\\nu) \\nonumber \\\\\n",
    "&= \\text{arg}\\max_\\nu - \\int q_\\nu(\\theta) \\log \\frac{q_\\nu(\\theta)}{p(\\mathcal{D}|\\theta) p (\\theta)} d\\theta\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "- The model function defines $p(\\mathcal{D}|\\theta) p (\\theta)$ \n",
    "- The guide function defines $q_\\nu(\\theta)$ \n",
    "\n",
    "Pyro offers several versions of the [ELBO](https://docs.pyro.ai/en/stable/inference_algos.html#module-pyro.infer.elbo)\n",
    "\n",
    "- `Trace_ELBO`: Default ELBO. Reduces variance of the gradients using \"Rao-Blackwellization\"\n",
    "- `TraceEnum_ELBO`: Performs exhaustive enumeration for discrete variables\n",
    "- `TraceMeanField_ELBO`: Assumes Mean-field structure. Reduce variance of gradients using analytical KL when possible\n",
    "\n",
    "> We will study the importance of gradient variance later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Once we have defined the model and guide we create an SVI object\n",
    "\n",
    "In this example we select the default ELBO and SGD with adaptive learning rate optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model=model_obs, guide=guide,\n",
    "                     loss=pyro.infer.Trace_ELBO(),\n",
    "                     optim=pyro.optim.ClippedAdam({\"lr\": 0.01}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method of SVI is \n",
    "\n",
    "- `svi.step(*args)`: Performs a gradient step, similar to `backward` plus `step` in pytorch\n",
    "\n",
    "`step` receives the inputs for guide and model as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed data\n",
    "x = torch.tensor([-2., 2.])\n",
    "y = torch.tensor([-2., 0.])\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(8, 2), dpi=120, tight_layout=True)\n",
    "lines = [ax_.plot([], [])[0] for ax_ in ax]\n",
    "param_names = [\"ELBO\", \"w_loc\", \"w_scale\", \"b_loc\", \"b_scale\"]\n",
    "param_evolution = {}\n",
    "for name in param_names:\n",
    "    param_evolution[name] = []\n",
    "    \n",
    "for ax_, name in zip(ax, param_names):\n",
    "    ax_.set_title(name)\n",
    "    \n",
    "for k in tqdm_notebook(range(3000)):\n",
    "    param_evolution[\"ELBO\"].append(svi.step(x, y))\n",
    "    for name in param_names[1:]:\n",
    "        param_evolution[name].append(pyro.param(name).item()) \n",
    "    \n",
    "    if np.mod(k, 100) == 0:\n",
    "        for i, name in enumerate(param_names):\n",
    "            lines[i].set_ydata(param_evolution[name][:k])\n",
    "        for line in lines:\n",
    "            line.set_xdata(range(k))\n",
    "        for ax_ in ax.ravel():\n",
    "            ax_.relim()\n",
    "            ax_.autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ELBO and the parameters have converged\n",
    "\n",
    "We can evaluate our results by observing the posteriors that we learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_plot = Normal(pyro.param('w_loc'), pyro.param('w_scale')).rsample((10000,)).detach().numpy()\n",
    "b_plot = Normal(pyro.param('b_loc'), pyro.param('b_scale')).rsample((10000,)).detach().numpy()\n",
    "\n",
    "figure = corner.corner(np.stack((b_plot, w_plot)).T, smooth=1.,\n",
    "                       labels=[\"bias\", \"weight\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-4, 4), (-4, 4)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "\n",
    "line_x = np.linspace(-5, 5, num=100).astype('float32') \n",
    "\n",
    "for i in range(100):    \n",
    "    line_y = line_x*w_plot[i] + b_plot[i]\n",
    "    ax.plot(line_x, line_y, c='tab:blue', alpha=0.2)\n",
    "\n",
    "ax.errorbar(2, 0, xerr=0, yerr=2, fmt='none', c='k', zorder=100);\n",
    "ax.errorbar(-2, -2., xerr=0, yerr=2, fmt='none', c='k', zorder=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the predictive posterior\n",
    "\n",
    "For the moment the pyro interface to do this is a bit clunky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Delta\n",
    "from pyro.infer import EmpiricalMarginal, TracePredictive\n",
    "\n",
    "posterior = svi.run(x, y)\n",
    "\n",
    "def wrapped_model(x, y):\n",
    "    pyro.sample(\"prediction\", Delta(model_obs(x, y)))\n",
    "\n",
    "trace_predictive = TracePredictive(wrapped_model, posterior, num_samples=10000)\n",
    "posterior_predictive = trace_predictive.run(torch.from_numpy(line_x), None)\n",
    "posterior_trace = EmpiricalMarginal(posterior_predictive, \"prediction\").enumerate_support().detach().cpu().numpy()\n",
    "\n",
    "med = np.median(posterior_trace, axis=0)\n",
    "qua = np.quantile(posterior_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.plot(line_x, med)\n",
    "ax.fill_between(line_x, qua[0], qua[1], alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC with Pyro\n",
    "\n",
    "[On the MCMC side Pyro](https://docs.pyro.ai/en/stable/mcmc.html) offers Hamiltonian Monte-Carlo and the more recent No-U turn sampler (NUTS)\n",
    "\n",
    "For theoretical details see Barber Chapter 27 or [here](https://github.com/magister-informatica-uach/INFO337/tree/master/MCMC)\n",
    "\n",
    "Here we run MCMC as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.mcmc import NUTS\n",
    "from pyro.infer.mcmc.api import MCMC\n",
    "\n",
    "nuts_kernel = NUTS(model_obs, adapt_step_size=True)\n",
    "sampler = MCMC(nuts_kernel, num_chains=1, num_samples=10000, warmup_steps=1000)\n",
    "sampler.run(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_samples()\n",
    "w_plot = samples['w'].detach().numpy()\n",
    "b_plot = samples['b'].detach().numpy()\n",
    "\n",
    "figure = corner.corner(np.stack((b_plot, w_plot)).T, smooth=1.,\n",
    "                       labels=[\"b\", \"w\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], range=[(-4, 4), (-4, 4)],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too different from the VI solution\n",
    "\n",
    "But remember, in this case we used to actual posterior (normal) in the guide\n",
    "\n",
    "Most of the time we won't be so lucky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro summary\n",
    "\n",
    "- We create (deterministic) parameters with `pyro.param`\n",
    "- We create latent random variables using `pyro.sample`\n",
    "- We create observed random variables using `pyro.sample` with the `obs` keyword\n",
    "\n",
    "> The model represents our graphical model\n",
    "\n",
    "> The guide represents our assumptions on the latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Networks\n",
    "\n",
    "> Deep Neural Networks are non-linear function approximators which represent the state of the art in pattern recognition\n",
    "\n",
    "But they do have limitations\n",
    "\n",
    "- Very deep models require lots of data and can be hard to train\n",
    "- Selecting an architecture requires a lot of experimentation\n",
    "- [Not too robust](https://openai.com/blog/adversarial-example-research/)\n",
    "- Poor at representing uncertainty\n",
    "\n",
    "> We can leverage some of these by going Bayesian\n",
    "\n",
    "- A Bayesian neural network (BNN) places a prior distribution on its parameters \n",
    "- Training the BNN $\\equiv$ Learning the posterior distribution of the parameters given the data\n",
    "- The **uncertainty on the data and the parameters** can be propagated to estimate the **uncertainty on our predictions**\n",
    "    - Uncertainty on the data is called **aleatoric uncertainty** and it is related to irreducible noise\n",
    "    - Uncertainty on the model (parameters and structure) is called **epistemic uncertainty**\n",
    "\n",
    "> We know what we don't know\n",
    "\n",
    "We can use this \"new knowledge\" to\n",
    "- Decide when to use a more simple/complex model (complexity-control)\n",
    "- Decide when to take a critical decisions\n",
    "    - Autonomouse cars\n",
    "    - Cancer diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of history\n",
    "\n",
    "> Bayesian neural networks is an active area of research \n",
    "\n",
    "- 1980's: Bayes theorem applied to Neural Networks (John Hopfield and Naftali Tishby)\n",
    "- 1990's: Monte-Carlo and VI for bayesian neural networks was studied extensively by [David Mackay](http://www.inference.org.uk/mackay/BayesNets.html) and [Radford Neal](https://www.cs.toronto.edu/~radford/res-neural.html) (Also Bishop, Barber, Hinton, Gharamani and many others). Neal shows that Gaussian process are bayesian neural networks with infinite neurons\n",
    "\n",
    "> The models remain quite difficult to train for some time\n",
    "\n",
    "- 2010's: Deep learning arrives \n",
    "- 2011: [Alex Graves' VI for neural networks](https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks)\n",
    "- Explosion of practical deep bayesian networks \n",
    "    - [Charles Blundell's Bayes by backprop](https://arxiv.org/abs/1505.05424)\n",
    "    - [Yarin Gal's many work](http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf)\n",
    "    - Durk Kingma, Danilo Jimenez Rezende, Shakir Mohamed, JosÃ© Miguel Hernandez-Lobato\n",
    "- [Hot topic now a days](http://bayesiandeeplearning.org/)\n",
    "\n",
    "History in video by [Zoubin Gharamani](http://mlg.eng.cam.ac.uk/zoubin/) at [NIPS 2016](https://www.youtube.com/watch?v=FD8l2vPU5FY) and [interesting panel discussion](https://www.youtube.com/watch?v=HumFmLu3CJ8) on the same conference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More formally\n",
    "\n",
    "\n",
    "Assuming we have *iid* samples $\\mathcal{D} =\\{(x^{(1)}, y^{(1)}), \\ldots \\}$ and a one-layer fully-connected neural networks for regression\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) &=   b_i + \\sum_{j=1}^H w_{ij} h_j  \\nonumber \\\\\n",
    "&=  b_i + \\sum_{j=1}^H w_{ij} g \\left( b_j + \\sum_{d=1}^D w_{jd} x_d  \\right) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The parameter vector $\\theta$ contains all the weights and biases\n",
    "\n",
    "First we propose a prior for $\\theta$, typically\n",
    "\n",
    "$$\n",
    "\\theta \\sim \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "Then we propose a likelihood depending on our task (Gaussian for regression, Bernoulli for classification)\n",
    "\n",
    "Finally we use Bayes theorem to write the posterior\n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})} = \\frac{1}{{p(\\mathcal{D})}} \\prod_n \\mathcal{N}(y^{(n)} | f(x^{(n)}), \\sigma^2) \\mathcal{N}(\\theta | 0, \\Sigma_\\theta)\n",
    "$$\n",
    "\n",
    "But\n",
    "\n",
    "> Because of the nested nonlinearities this posterior is not Gaussian\n",
    "\n",
    "In general\n",
    "\n",
    "> We cannot obtain an analytical posterior for a bayesian neural network\n",
    "\n",
    "We resort to sampling-based (MCMC) or deterministic (VI) approximate inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My first Bayesian Neural Network using Pyro\n",
    "\n",
    "\n",
    "First let's write a pytorch implementation of our simple MLP network for regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_hidden=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(1, n_hidden, bias=True)\n",
    "        self.output = torch.nn.Linear(n_hidden, 1, bias=True)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.hidden(x))\n",
    "        return self.output(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same synthetic from our previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) \n",
    "x_test = np.linspace(-0.05, 1.05, num=200)\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32')).unsqueeze(1)\n",
    "x_test = torch.from_numpy(x_test.astype('float32')).unsqueeze(1)\n",
    "y_torch = torch.from_numpy(y.astype('float32')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(n_hidden=10)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), \n",
    "                             lr=1e-2, \n",
    "                             weight_decay=0.0) # Change the weight decay\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True, dpi=120)\n",
    "f = mlp_model.forward(x_test).detach().numpy()\n",
    "line1 = ax[0].plot(x_test.detach().numpy(), f, 'k-')\n",
    "line2 = ax[1].plot([], [])\n",
    "ax[0].scatter(x, y)\n",
    "\n",
    "epoch_loss = np.zeros(shape=(2000,))\n",
    "for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "    optimizer.zero_grad()\n",
    "    f = mlp_model.forward(x_torch)\n",
    "    loss = criterion(y_torch, f)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss[k] = loss.item()\n",
    "    #break    \n",
    "    if k % 100 == 0:\n",
    "        f = mlp_model.forward(x_test).detach().numpy()\n",
    "        line1[0].set_ydata(f)\n",
    "        line2[0].set_xdata(range(k))\n",
    "        line2[0].set_ydata(epoch_loss[:k])\n",
    "        for ax_ in ax:\n",
    "            ax_.relim()\n",
    "            ax_.autoscale_view()\n",
    "        fig.canvas.draw()\n",
    "torch.save(mlp_model.state_dict(), \"/home/phuijse/models/best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifting a torch model\n",
    "\n",
    "> Pyro offers [`pyro.random_module()`](http://docs.pyro.ai/en/stable/primitives.html?highlight=random_module#pyro.random_module) to \"lift\" a torch model into a pyro model\n",
    "\n",
    "This function grabs an object that inherites from `torch.nn.Module` and adds priors to its parameters\n",
    "\n",
    "In this case the parameters of the model are `hidden.weight`, `hidden.bias`, `output.weight` and `output.bias`\n",
    "\n",
    "> We will add a Normal prior to these parameters\n",
    "\n",
    "We will use conditioning for the likelihood \n",
    "\n",
    "> The likelihood is set to normal with  $f_\\theta(x^{(n)})$ as its mean and $\\sigma_\\epsilon$ as its scale\n",
    "\n",
    "We condition on the whole dataset (assuming independence) using [`pyro.plate`](http://docs.pyro.ai/en/stable/primitives.html#pyro.plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal, LogNormal\n",
    "\n",
    "def model(x, y):\n",
    "    # We add normal priors to w and b\n",
    "    \n",
    "    w_hidden_prior = Normal(torch.zeros_like(mlp_model.hidden.weight), \n",
    "                            10*torch.ones_like(mlp_model.hidden.weight)).to_event(2)\n",
    "    b_hidden_prior = Normal(torch.zeros_like(mlp_model.hidden.bias), \n",
    "                            10*torch.ones_like(mlp_model.hidden.bias)).to_event(1)\n",
    "    w_output_prior = Normal(torch.zeros_like(mlp_model.output.weight), \n",
    "                            10*torch.ones_like(mlp_model.output.weight)).to_event(2)\n",
    "    b_output_prior = Normal(torch.zeros_like(mlp_model.output.bias), \n",
    "                            10*torch.ones_like(mlp_model.output.bias)).to_event(1)\n",
    "    priors = {'hidden.weight': w_hidden_prior, 'hidden.bias': b_hidden_prior,\n",
    "              'output.weight': w_output_prior, 'output.bias': b_output_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", mlp_model, priors)    \n",
    "    #with pyro.plate(\"num_particles\", size=2, dim=-1):\n",
    "    lifted_mlp_model = lifted_module()\n",
    "    # We create a random variable for the scale\n",
    "    #scale = pyro.sample(\"sigma\", LogNormal(-2., 1.))\n",
    "    # Condition on the dataset assuming iid using a vectorized plate\n",
    "    with pyro.plate(\"observed_data\", size=len(x), dim=-1):\n",
    "        prediction_mean = lifted_mlp_model(x).squeeze(-1)\n",
    "        pyro.sample(\"likelihood\", Normal(prediction_mean, 0.05), obs=y)\n",
    "    return prediction_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very useful:** debugging the [shapes](#http://pyro.ai/examples/tensor_shapes.html) of the tensors in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyro.poutine.trace(model).get_trace(x_torch, y_torch.squeeze(-1)).format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch dimension is 15 (number of samples)\n",
    "- Event dimension is equal to the number of neurons for each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC for the BNN\n",
    "\n",
    "Even for a extremely simple NN and using the most advanced samplers MCMC can be inpractical\n",
    "\n",
    "(Don't try to wait for this to converge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.mcmc import NUTS\n",
    "from pyro.infer.mcmc.api import MCMC\n",
    "\n",
    "nuts_kernel = NUTS(model, adapt_step_size=True)\n",
    "sampler = MCMC(nuts_kernel, num_chains=1, num_samples=10000, warmup_steps=1000)\n",
    "sampler.run(x_torch, y_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI for the BNN\n",
    "\n",
    "To proceed with VI instead we need to define our approximate posterior (guide) and loss function (ELBO)\n",
    "\n",
    "> We will consider a factorized Gaussian for the posterior\n",
    "\n",
    "We will use the solution of the \"regular\" neural network to initialize the guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "mlp_model = MLP(n_hidden=10)\n",
    "mlp_model.load_state_dict(torch.load(\"/home/phuijse/models/best_model.pt\"))\n",
    "\n",
    "def guide(x, y):\n",
    "    # Hidden weight\n",
    "    w_hidden_loc = pyro.param(\"w_hidden_mu\", mlp_model.hidden.weight)\n",
    "    w_hidden_scale = pyro.param(\"w_hidden_sigma\", \n",
    "                                0.1*torch.ones_like(mlp_model.hidden.weight),\n",
    "                                constraint=constraints.greater_than(1e-2))\n",
    "    w_hidden_prior = Normal(loc=w_hidden_loc, scale=w_hidden_scale).to_event(2)\n",
    "    # Hidden bias\n",
    "    b_hidden_loc = pyro.param(\"b_hidden_mu\", mlp_model.hidden.bias)\n",
    "    b_hidden_scale = pyro.param(\"b_hidden_sigma\", \n",
    "                                0.1*torch.ones_like(mlp_model.hidden.bias),\n",
    "                                constraint=constraints.greater_than(1e-2))\n",
    "    b_hidden_prior = Normal(loc=b_hidden_loc, scale=b_hidden_scale).to_event(1)\n",
    "    # Output weight\n",
    "    w_output_loc = pyro.param(\"w_output_mu\", mlp_model.output.weight)\n",
    "    w_output_scale = pyro.param(\"w_output_sigma\", \n",
    "                                0.1*torch.ones_like(mlp_model.output.weight), \n",
    "                                constraint=constraints.greater_than(1e-2))\n",
    "    w_output_prior = Normal(loc=w_output_loc, scale=w_output_scale).to_event(2)\n",
    "    # Output bias\n",
    "    b_output_loc = pyro.param(\"b_output_mu\", mlp_model.output.bias)\n",
    "    b_output_scale = pyro.param(\"b_output_sigma\", \n",
    "                                0.1*torch.ones_like(mlp_model.output.bias), \n",
    "                                constraint=constraints.greater_than(1e-2))\n",
    "    b_output_prior = Normal(loc=b_output_loc, scale=b_output_scale).to_event(1)\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    priors = {'hidden.weight': w_hidden_prior, 'hidden.bias': b_hidden_prior,\n",
    "              'output.weight': w_output_prior, 'output.bias': b_output_prior}\n",
    "    lifted_module = pyro.random_module(\"module\", mlp_model, priors)\n",
    "    lifted_mlp_model = lifted_module()\n",
    "    #s_loc = pyro.param(\"s_loc\", torch.tensor(-2.))\n",
    "    #s_scale = pyro.param(\"s_scale\", torch.tensor(1.0), \n",
    "    #                     constraint=constraints.greater_than(1e-2))\n",
    "    #pyro.sample('sigma', LogNormal(s_loc, s_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approximate posterior assumes that there is no correlation between weights: **Mean Field**\n",
    "\n",
    "We can use the `TraceMeanField_ELBO` as cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model, guide, pyro.optim.Adam({\"lr\": 1e-2}), \n",
    "                     loss=pyro.infer.TraceMeanField_ELBO(num_particles=1, \n",
    "                                                         vectorize_particles=False), \n",
    "                     num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train and plot the traces as we go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Delta\n",
    "from pyro.infer import EmpiricalMarginal, TracePredictive\n",
    "\n",
    "epoch_loss = np.zeros(shape=(20000,))\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3), tight_layout=True, dpi=120)\n",
    "line1 = ax[0].plot([], [])\n",
    "\n",
    "def wrapped_model(x_data, y_data):\n",
    "    pyro.sample(\"prediction\", Delta(model(x_data, y_data)))    \n",
    "\n",
    "for k in tqdm_notebook(range(len(epoch_loss))):\n",
    "    loss = svi.step(x=x_torch, y=y_torch.squeeze(-1))\n",
    "    epoch_loss[k] = loss / len(x_torch)\n",
    "        \n",
    "    if k % 10 == 0:\n",
    "        # Update loss\n",
    "        line1[0].set_xdata(range(k))\n",
    "        line1[0].set_ydata(epoch_loss[:k])\n",
    "        ax[0].relim()\n",
    "        ax[0].autoscale_view()\n",
    "        fig.canvas.draw()\n",
    "        # Update predictive posterior\n",
    "        posterior = svi.run(x_torch, y_torch.squeeze(-1))\n",
    "        trace_predictive = TracePredictive(wrapped_model, posterior, num_samples=100)\n",
    "        posterior_predictive = trace_predictive.run(x_test, None)\n",
    "        ax[1].cla()\n",
    "        ax[1].plot(x, y, 'k.');\n",
    "        posterior_trace = EmpiricalMarginal(posterior_predictive, \"prediction\").enumerate_support().detach().cpu().numpy()\n",
    "        med = np.median(posterior_trace, axis=[0])\n",
    "        qua = np.quantile(posterior_trace, (0.05, 0.95), axis=0)\n",
    "        ax[1].plot(x_test.numpy()[:, 0], med)\n",
    "        ax[1].fill_between(x_test.numpy()[:, 0], qua[0], qua[1], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute traces and plot the predictive posterior as we did during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_predictive = TracePredictive(wrapped_model, posterior, num_samples=500)\n",
    "x_plot = np.linspace(-0.2, 1.2, num=200).astype('float32')\n",
    "posterior_predictive = trace_predictive.run(torch.from_numpy(x_plot[:, None]), None)\n",
    "\n",
    "posterior_trace = EmpiricalMarginal(posterior_predictive, \"prediction\").enumerate_support().detach().cpu().numpy()\n",
    "med = np.median(posterior_trace, axis=[0])\n",
    "qua = np.quantile(posterior_trace, (0.01, 0.99), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3), sharex=True, sharey=True, tight_layout=True)\n",
    "for i in range(posterior_trace.shape[0]):\n",
    "    ax[0].plot(x_plot, posterior_trace[i, :], '-', c='tab:blue', alpha=0.01)\n",
    "ax[1].plot(x_plot, med)\n",
    "ax[1].fill_between(x_plot, qua[0], qua[1], alpha=0.5)\n",
    "ax[1].plot(x, y, 'k.');\n",
    "ax[1].set_ylim([-2, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the posteriors of our parameters\n",
    "\n",
    "From the samples of the empirical marginals we can create histograms and compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = svi.run(x_torch, y_torch.squeeze(-1))\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(9, 3), tight_layout=True)\n",
    "for row, param in enumerate([\"module$$$hidden.weight\", \"module$$$output.weight\"]):\n",
    "    #marginal_trace = EmpiricalMarginal(posterior, param).sample(sample_shape=torch.Size([1000]))\n",
    "    marginal_trace = EmpiricalMarginal(posterior, param).enumerate_support()\n",
    "    samples = marginal_trace.reshape(100, -1).detach().numpy()\n",
    "    for k in range(5): #samples.shape[1]\n",
    "        print(\"%s\\t%d\\t%0.4f\\t%0.4f\\t%0.4f\" %(param, k, *np.quantile(samples[:, k], (0.05, 0.5, 0.95))))\n",
    "        ax[row, k].hist(samples[:, k], density=True, label=param+' '+str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain the learned guide hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyro's automatic guides\n",
    "\n",
    "For the guide we could have used the [\"auto guide\"](https://docs.pyro.ai/en/0.3.0-release/contrib.autoguide.html) functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer.autoguide import init_to_feasible, init_to_sample, init_to_mean\n",
    "\n",
    "guide = AutoDiagonalNormal(model, init_loc_fn=init_to_feasible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we train with this guide?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on Bayesian Neural Networks Training\n",
    "\n",
    "- State of the art!\n",
    "- Very delicate: bad initializations and local minima \n",
    "- Set appropriate priors\n",
    "- Variance control and reparameterization (more on this next class)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
