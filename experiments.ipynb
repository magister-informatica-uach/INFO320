{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) \n",
    "x_test = np.linspace(-0.05, 1.05, num=200)\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "x_torch = torch.from_numpy(x.astype('float32')).unsqueeze(1)\n",
    "x_test = torch.from_numpy(x_test.astype('float32')).unsqueeze(1)\n",
    "y_torch = torch.from_numpy(y.astype('float32')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[0., 0.]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = MixtureOfDiagNormals(locs=torch.tensor([[0.], [0.]]),\n",
    "                                     coord_scale=torch.tensor([[1.], [0.01]]),\n",
    "                                     component_logits=torch.tensor([0.5, 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.batch_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.event_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.expand_by([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.expand([10]).to_event(2).shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.expand_by([10]).to_event(1).rsample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.expand_by?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyroSample(prior.expand_by((10, 1)).to_event(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MixtureOfDiagNormals??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.distributions import Uniform, Normal, MixtureOfDiagNormals,Categorical\n",
    "\n",
    "class BayesianMLPRegression(PyroModule):\n",
    "    def __init__(self, n_hidden=10, prior_scale=1.):\n",
    "        super().__init__()\n",
    "        #prior = Normal(0, prior_scale)\n",
    "        prior = MixtureOfDiagNormals(locs=torch.tensor([[0.], [0.]]),\n",
    "                                     coord_scale=torch.tensor([[prior_scale], [0.01]]),\n",
    "                                     component_logits=torch.tensor([0.5, 0.5]))\n",
    "        # Hidden layer\n",
    "        #display(prior.expand([n_hidden, 1]).to_event(2).shape())\n",
    "        self.hidden = PyroModule[torch.nn.Linear](1, n_hidden)\n",
    "        self.hidden.weight = PyroSample(prior.expand_by([n_hidden, 1]).to_event(2))\n",
    "        self.hidden.bias = PyroSample(prior.expand_by([n_hidden]).to_event(1))\n",
    "        # Output layer\n",
    "        self.output = PyroModule[torch.nn.Linear](n_hidden, 1)\n",
    "        self.output.weight = PyroSample(prior.expand([1, n_hidden]).to_event(2))\n",
    "        self.output.bias = PyroSample(prior.expand([1]).to_event(1))\n",
    "        # activation function\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        z = self.activation(self.hidden(x))\n",
    "        mean = self.output(z).squeeze(-1)\n",
    "        sigma = pyro.sample(\"sigma\", Uniform(0.0, 0.1))\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", Normal(mean, sigma), obs=y) #likelihood\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "\n",
    "model = BayesianMLPRegression()\n",
    "\n",
    "print(pyro.poutine.trace(model).get_trace(x_torch, y_torch).format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3), tight_layout=True, dpi=80)\n",
    "#ax[0].set_yscale('log')\n",
    "\n",
    "def update_plot(k, epoch_loss, samples):\n",
    "    ax[0].cla()\n",
    "    ax[0].plot(range(k), epoch_loss[:k])\n",
    "    #ax[0].autoscale_view()\n",
    "    ax[1].cla()\n",
    "    ax[1].plot(x, y, 'k.');\n",
    "    med = np.median(samples, axis=[0])\n",
    "    qua = np.quantile(samples, (0.05, 0.95), axis=0)\n",
    "    ax[1].plot(x_test.numpy()[:, 0], med)\n",
    "    ax[1].fill_between(x_test.numpy()[:, 0], qua[0], qua[1], alpha=0.5)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) # Turn this on for additional debugging\n",
    "pyro.clear_param_store() \n",
    "model = BayesianMLPRegression(n_hidden=10, prior_scale=1.) # Declare the neural network\n",
    "\n",
    "# Create a guide\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model, init_scale=1e-2)\n",
    "\n",
    "# Create SVI object\n",
    "svi = pyro.infer.SVI(model, \n",
    "                     guide, \n",
    "                     optim=pyro.optim.ClippedAdam({'lr':1e-2, 'clip_norm': 10.0}), # Optimizer\n",
    "                     loss=pyro.infer.Trace_ELBO()) # Loss function \n",
    "\n",
    "epoch_loss = np.zeros(shape=(10000,))\n",
    "for k in tqdm(range(len(epoch_loss))):\n",
    "    loss = svi.step(x=x_torch, y=y_torch.squeeze(-1)) # Actual training step\n",
    "    epoch_loss[k] = loss / len(x_torch)\n",
    "        \n",
    "    if k % 100 == 0:\n",
    "        # Compute predictive posterior\n",
    "        predictive = pyro.infer.Predictive(model, guide=guide, num_samples=100)\n",
    "        samples = predictive(x_test, None)['obs'].detach().numpy()\n",
    "        # Plot it\n",
    "        update_plot(k, epoch_loss, samples)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVI, minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blundell weight uncertainty: https://arxiv.org/pdf/1505.05424.pdf\n",
    "- Graves, practical VI for NN: https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf\n",
    "- https://csc2541-f17.github.io/slides/lec04.pdf\n",
    "- Thesis Y Gal: http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- advances in VI: https://arxiv.org/pdf/1711.05597.pdf\n",
    "- Variational dropout and local reparameterization trick: http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick, https://alsibahi.xyz/snippets/2019/06/15/pyro_mnist_bnn_kl.html\n",
    "- Uncertainty Estimations by Softplus normalization inBayesian Convolutional Neural Networks withVariational Inference: https://arxiv.org/pdf/1806.05978.pdf\n",
    "- VAE structured residuals: https://arxiv.org/pdf/1804.01050.pdf\n",
    "- Importance sampling: https://arxiv.org/abs/1509.00519\n",
    "- Stein\n",
    "- generative flow: https://d4mucfpksywv.cloudfront.net/research-covers/glow/paper/glow.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://edwardlib.org/tutorials/klqp\n",
    "    \n",
    "    http://artem.sobolev.name/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html\n",
    "    \n",
    "    https://www.cs.tufts.edu/comp/150BDL/2018f/assignments/hw4.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temas pendientes:\n",
    "VAE condicional\n",
    "VAE semisupervisado\n",
    "Cómo hacer más expresivo el posterior? Variables auxiliares (jerarquías), Inverse autoregresive flow, Stein Gradient descent, Adversarial VAE\n",
    "IWAE: https://arxiv.org/abs/1509.00519, https://openreview.net/forum?id=HyZoi-WRb\n",
    "Cómo hacer más expresivo el prior? Mixture of Gaussians\n",
    "Boosted Variational Mixtures: ftp://ftp.cs.princeton.edu/reports/2017/006.pdf\n",
    "Anchor VAE: https://arxiv.org/pdf/1802.05822.pdf\n",
    "\n",
    "\n",
    "Normalizing flows: https://arxiv.org/pdf/1505.05770.pdf\n",
    "Auxiliary deep generative models: https://arxiv.org/abs/1602.05473\n",
    "Hierarchical features from generative models: https://arxiv.org/abs/1702.08396\n",
    "Adversarial VAE: https://arxiv.org/pdf/1701.04722.pdf\n",
    "Neural discrete representation: https://arxiv.org/pdf/1711.00937.pdf\n",
    "Alternative priors: https://www.ics.uci.edu/~enalisni/nalisnick_openAI_talk.pdf\n",
    "Kalman VAE: https://arxiv.org/pdf/1710.057416.pdf\n",
    "Stick Breaking Autoencoder: https://arxiv.org/pdf/1605.06197.pdf\n",
    "Generative flow: https://d4mucfpksywv.cloudfront.net/research-covers/glow/paper/glow.pdf\n",
    "\n",
    "SS VAE+GRAPH: https://arxiv.org/pdf/1806.02679.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://akosiorek.github.io/ml/2018/04/03/norm_flows.html\n",
    "- https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
    "-https://nbviewer.jupyter.org/github/bmazoure/code_snippets/blob/master/normalizing_flows/Normalizing%20flows%20in%20Pyro.ipynb\n",
    "\n",
    "Recheck bayesian regression and bayesian neural net pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://pyro.ai/examples/custom_objectives.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BO: http://pyro.ai/examples/bo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://petuum.com/2019/01/15/intro-to-modern-bayesian-learning-and-probabilistic-programming/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/pyro-ppl/numpyro/blob/master/examples/bnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "se = 0.1\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 1, num=20) \n",
    "x_test = np.linspace(-0.05, 1.05, num=200)\n",
    "f = lambda x : x*np.sin(10*x)\n",
    "\n",
    "x = np.delete(x, slice(9, 14))\n",
    "y = f(x) + se*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.scatter(x, y);\n",
    "\n",
    "#x_torch = torch.from_numpy(x.astype('float32')).unsqueeze(1)\n",
    "#x_test = torch.from_numpy(x_test.astype('float32')).unsqueeze(1)\n",
    "#y_torch = torch.from_numpy(y.astype('float32')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y, n_hidden=10):\n",
    "\n",
    "    \n",
    "    w1 = numpyro.sample(\"w1\", dist.Normal(jnp.zeros((1, n_hidden)), \n",
    "                                          jnp.ones((1, n_hidden))))  # D_X D_H\n",
    "    \n",
    "    b1 = numpyro.sample(\"b1\", dist.Normal(jnp.zeros((n_hidden,)), \n",
    "                                          jnp.ones((n_hidden,))))  # D_X D_H\n",
    "    \n",
    "    z1 = jnp.tanh(jnp.matmul(x, w1) + b1)   \n",
    "    \n",
    "    w2 = numpyro.sample(\"w2\", dist.Normal(jnp.zeros((n_hidden, n_hidden)), \n",
    "                                          jnp.ones((n_hidden, n_hidden))))  # D_H D_H\n",
    "    b2 = numpyro.sample(\"b2\", dist.Normal(jnp.zeros((n_hidden,)), \n",
    "                                          jnp.ones((n_hidden,))))  # D_X D_H\n",
    "    z2 = jnp.tanh(jnp.matmul(z1, w2) + b2)  # N D_H  <= second layer of activations\n",
    "\n",
    "    # sample final layer of weights and neural network output\n",
    "    w3 = numpyro.sample(\"w3\", dist.Normal(jnp.zeros((n_hidden, 1)), \n",
    "                                          jnp.ones((n_hidden, 1)))) \n",
    "    b3 = numpyro.sample(\"b3\", dist.Normal(jnp.zeros((1,)), \n",
    "                                          jnp.ones((1,))))  # D_H D_Y\n",
    "    z3 = jnp.matmul(z2, w3) + b3  # N D_Y  <= output of the neural network\n",
    "\n",
    "    # we put a prior on the observation noise\n",
    "    prec_obs = numpyro.sample(\"prec_obs\", dist.Gamma(3.0, 1.0))\n",
    "    sigma_obs = 1.0 / jnp.sqrt(prec_obs)\n",
    "\n",
    "    # observe data\n",
    "    numpyro.sample(\"obs\", dist.Normal(z3, sigma_obs), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, rng_key_predict = random.split(random.PRNGKey(0))\n",
    "\n",
    "numpyro.set_host_device_count(2)\n",
    "kernel = NUTS(model)\n",
    "mcmc = MCMC(kernel, 100, 1000, num_chains=2, progress_bar=True)\n",
    "mcmc.run(rng_key, x.reshape(-1,1), y.reshape(-1,1), 10)\n",
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mcmc.get_samples()\n",
    "\n",
    "def predict(model, rng_key, samples, X, D_H):\n",
    "    model = numpyro.handlers.substitute(numpyro.handlers.seed(model, rng_key), samples)\n",
    "    # note that Y will be sampled in the model because we pass Y=None here\n",
    "    model_trace = numpyro.handlers.trace(model).get_trace(x=X, y=None, n_hidden=D_H)\n",
    "    return model_trace['obs']['value']\n",
    "\n",
    "from jax import vmap\n",
    "vmap_args = (samples, random.split(rng_key, 1000 * 2))\n",
    "predictions = vmap(lambda samples, rng_key: predict(model, rng_key, samples, x_test.reshape(-1,1), 10))(*vmap_args)\n",
    "predictions = predictions[..., 0]\n",
    "\n",
    "# compute mean prediction and confidence interval around median\n",
    "mean_prediction = jnp.mean(predictions, axis=0)\n",
    "percentiles = np.percentile(predictions, [5.0, 95.0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_test, mean_prediction)\n",
    "plt.fill_between(x_test, percentiles[0, :], percentiles[1, :], alpha=0.5)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
